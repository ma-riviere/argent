[{"path":"https://ma-riviere.github.io/argent/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 Marc-Aurèle Rivière Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"https://ma-riviere.github.io/argent/articles/advanced-argentic-workflow.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Advanced Agentic Workflows","text":"article demonstrates build agentic workflows, sub-agents. Instead giving one agent tools tasks, create specialized sub-agents excel specific responsibilities main orchestrator coordinates work. avoids clogging main agent’s context window, reduce cost delegating specific sub-tasks less expensive models.","code":""},{"path":"https://ma-riviere.github.io/argent/articles/advanced-argentic-workflow.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Advanced Agentic Workflows","text":"","code":"mirai::daemons(4) # Enable parallel tool calling"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-argentic-workflow.html","id":"building-our-tools","dir":"Articles","previous_headings":"","what":"Building our Tools","title":"Advanced Agentic Workflows","text":"First, let’s build tools agents need: Search: Fetch: Let’s build TODO-list tool provides agents way track multi-step work. Note See Using MCP Servers argent article details use MCP servers argent.","code":"web_search <- function(query) {     #' @description Search the web for information using Tavily API. Returns a JSON array of search results with titles, URLs, and content snippets. Use this when you need current information, facts, news, or any data not in your training data.     #' @param query:string* The search query string. Be specific and use keywords that will yield the most relevant results.          return(web_search_tavily(query)) }  web_search_tavily <- function(query) {     res <- httr2::request(\"https://api.tavily.com/search\") |>          httr2::req_body_json(list(             query = query,             search_depth = \"basic\",             include_answer = FALSE,             max_results = 10,             api_key = Sys.getenv(\"TAVILY_API_KEY\")         )) |>          httr2::req_error(is_error = \\(resp) FALSE) |>          httr2::req_throttle(rate = 20/60, realm = \"tavily\") |>          httr2::req_perform() |>          httr2::resp_body_json() |>          purrr::discard_at(c(\"response_time\", \"follow_up_questions\", \"images\"))      results <- purrr::map(res$results, \\(x) purrr::discard_at(x, \"raw_content\"))      return(jsonlite::toJSON(results, pretty = FALSE, auto_unbox = TRUE)) } web_fetch <- function(url) {     #' @description Fetch and extract the main text content from a web page as clean markdown. Returns the page content with formatting preserved, stripped of navigation, ads, and boilerplate. Use this to read articles, documentation, blog posts, or any web page content.     #' @param url:string* The complete URL of the web page to fetch (e.g., \"https://example.com/article\"). Must be a valid HTTP/HTTPS URL.          trafilatura_installed <- tryCatch({         system(\"which trafilatura\", intern = TRUE, ignore.stderr = TRUE)         return(TRUE)     },     warning = function(e) {         cli::cli_alert_warning(\"trafilatura is not installed. Install with: {.code pip install trafilatura}\")         return(FALSE)     })      if (trafilatura_installed) {         res <- web_fetch_trafilatura(url)          could_not_fetch <- c(             \"Impossible to fetch the contents of this web page\",             \"Please reload this page\",             \"There was an error while loading\",             \"404\"         )         if (is.null(res) || is.na(res) || nchar(res) == 0 ||             any(stringr::str_detect(res, stringr::fixed(could_not_fetch, ignore_case = TRUE)))) {             return(web_fetch_rvest(url))         }         return(res)     }      return(web_fetch_rvest(url)) }  web_fetch_trafilatura <- function(url) {     # pip install trafilatura     tryCatch({         res <- paste0(\"trafilatura -u \", url, \" --markdown --no-comments --links \") |>              system(intern = TRUE) |>              purrr::keep(nzchar) |>             paste0(collapse = \"\\n\")                  return(res)     },     error = function(e) {         return(\"Impossible to fetch the contents of this web page. It might not allow scraping\")     }) }  web_fetch_rvest <- function(url) {     tags_to_ignore <- c(         \"a\", \"script\", \"code\", \"img\", \"svg\", \"footer\", \"g\", \"path\", \"polygon\", \"label\", \"button\", \"form\", \"input\", \"select\",          \"style\", \"link\", \"meta\", \"noscript\", \"iframe\", \"embed\", \"object\", \"param\", \"video\", \"audio\", \"track\", \"source\",          \"canvas\", \"map\", \"area\", \"math\", \"col\", \"colgroup\", \"dl\", \"dt\", \"dd\", \"hr\", \"pre\", \"address\", \"figure\", \"figcaption\",         \"dfn\", \"em\", \"kbd\", \"samp\", \"var\", \"del\", \"ins\", \"mark\", \"circle\"     )      remove_tags <- function(xml, tags) {         purrr::walk(tags, \\(tag) purrr::walk(xml2::xml_find_all(xml, paste0(\".//\", tag)), \\(node) xml2::xml_remove(node)))         return(xml)     }      cleaned_contents <- tryCatch(         rvest::read_html(url)         |> rvest::html_element(\"body\")         |> remove_tags(tags_to_ignore)         |> rvest::html_children()         |> rvest::html_text2()         |> purrr::discard(\\(x) x == \"\")         |> paste0(collapse = \"\\n\\n\"),         error = \\(e) return(\"\")     )     return(cleaned_contents) } # Create a TODO-list create_todo_list <- function() {     env <- new.env(parent = emptyenv())     env$todos <- list()     return(env) }  # Get the current TODOs from a given TODO-list get_todos <- function(todo_mgr) {     todo_mgr$todos }  # Set the TODOs of a given TODO-list set_todos <- function(todo_mgr, todos) {     # Validate input structure     if (!is.list(todos)) {         return(\"Error: todos must be a list\")     }      errors <- character()     for (i in seq_along(todos)) {         todo <- todos[[i]]          # Check required fields         if (!all(c(\"content\", \"status\") %in% names(todo))) {             errors <- c(errors, sprintf(\"Todo %d missing required fields (content, status)\", i))         }          # Validate status         valid_statuses <- c(\"pending\", \"in_progress\", \"completed\")         if (!todo$status %in% valid_statuses) {             errors <- c(errors, sprintf(                 \"Todo %d has invalid status '%s'. Must be one of: %s\",                 i, todo$status, paste(valid_statuses, collapse = \", \")             ))         }     }      if (length(errors) > 0) {         return(paste(c(\"Errors found:\", errors), collapse = \"\\\\n- \"))     }      todo_mgr$todos <- todos     return(sprintf(\"Successfully updated %d TODO(s)\", length(todos))) } github_mcp_server <- mcp_server(     name = \"github\",     type = \"http\",     url = \"https://api.githubcopilot.com/mcp\",     headers = list(Authorization = paste(\"Bearer\", Sys.getenv(\"GITHUB_PAT\"))) )  github_mcp_tools <- mcp_tools(github_mcp_server, tools = c(\"search_code\", \"get_file_contents\"))"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-argentic-workflow.html","id":"web-tools","dir":"Articles","previous_headings":"","what":"Web Tools","title":"Advanced Agentic Workflows","text":"Search: Fetch:","code":"web_search <- function(query) {     #' @description Search the web for information using Tavily API. Returns a JSON array of search results with titles, URLs, and content snippets. Use this when you need current information, facts, news, or any data not in your training data.     #' @param query:string* The search query string. Be specific and use keywords that will yield the most relevant results.          return(web_search_tavily(query)) }  web_search_tavily <- function(query) {     res <- httr2::request(\"https://api.tavily.com/search\") |>          httr2::req_body_json(list(             query = query,             search_depth = \"basic\",             include_answer = FALSE,             max_results = 10,             api_key = Sys.getenv(\"TAVILY_API_KEY\")         )) |>          httr2::req_error(is_error = \\(resp) FALSE) |>          httr2::req_throttle(rate = 20/60, realm = \"tavily\") |>          httr2::req_perform() |>          httr2::resp_body_json() |>          purrr::discard_at(c(\"response_time\", \"follow_up_questions\", \"images\"))      results <- purrr::map(res$results, \\(x) purrr::discard_at(x, \"raw_content\"))      return(jsonlite::toJSON(results, pretty = FALSE, auto_unbox = TRUE)) } web_fetch <- function(url) {     #' @description Fetch and extract the main text content from a web page as clean markdown. Returns the page content with formatting preserved, stripped of navigation, ads, and boilerplate. Use this to read articles, documentation, blog posts, or any web page content.     #' @param url:string* The complete URL of the web page to fetch (e.g., \"https://example.com/article\"). Must be a valid HTTP/HTTPS URL.          trafilatura_installed <- tryCatch({         system(\"which trafilatura\", intern = TRUE, ignore.stderr = TRUE)         return(TRUE)     },     warning = function(e) {         cli::cli_alert_warning(\"trafilatura is not installed. Install with: {.code pip install trafilatura}\")         return(FALSE)     })      if (trafilatura_installed) {         res <- web_fetch_trafilatura(url)          could_not_fetch <- c(             \"Impossible to fetch the contents of this web page\",             \"Please reload this page\",             \"There was an error while loading\",             \"404\"         )         if (is.null(res) || is.na(res) || nchar(res) == 0 ||             any(stringr::str_detect(res, stringr::fixed(could_not_fetch, ignore_case = TRUE)))) {             return(web_fetch_rvest(url))         }         return(res)     }      return(web_fetch_rvest(url)) }  web_fetch_trafilatura <- function(url) {     # pip install trafilatura     tryCatch({         res <- paste0(\"trafilatura -u \", url, \" --markdown --no-comments --links \") |>              system(intern = TRUE) |>              purrr::keep(nzchar) |>             paste0(collapse = \"\\n\")                  return(res)     },     error = function(e) {         return(\"Impossible to fetch the contents of this web page. It might not allow scraping\")     }) }  web_fetch_rvest <- function(url) {     tags_to_ignore <- c(         \"a\", \"script\", \"code\", \"img\", \"svg\", \"footer\", \"g\", \"path\", \"polygon\", \"label\", \"button\", \"form\", \"input\", \"select\",          \"style\", \"link\", \"meta\", \"noscript\", \"iframe\", \"embed\", \"object\", \"param\", \"video\", \"audio\", \"track\", \"source\",          \"canvas\", \"map\", \"area\", \"math\", \"col\", \"colgroup\", \"dl\", \"dt\", \"dd\", \"hr\", \"pre\", \"address\", \"figure\", \"figcaption\",         \"dfn\", \"em\", \"kbd\", \"samp\", \"var\", \"del\", \"ins\", \"mark\", \"circle\"     )      remove_tags <- function(xml, tags) {         purrr::walk(tags, \\(tag) purrr::walk(xml2::xml_find_all(xml, paste0(\".//\", tag)), \\(node) xml2::xml_remove(node)))         return(xml)     }      cleaned_contents <- tryCatch(         rvest::read_html(url)         |> rvest::html_element(\"body\")         |> remove_tags(tags_to_ignore)         |> rvest::html_children()         |> rvest::html_text2()         |> purrr::discard(\\(x) x == \"\")         |> paste0(collapse = \"\\n\\n\"),         error = \\(e) return(\"\")     )     return(cleaned_contents) }"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-argentic-workflow.html","id":"todo-list-tools","dir":"Articles","previous_headings":"","what":"TODO-list Tools","title":"Advanced Agentic Workflows","text":"Let’s build TODO-list tool provides agents way track multi-step work.","code":"# Create a TODO-list create_todo_list <- function() {     env <- new.env(parent = emptyenv())     env$todos <- list()     return(env) }  # Get the current TODOs from a given TODO-list get_todos <- function(todo_mgr) {     todo_mgr$todos }  # Set the TODOs of a given TODO-list set_todos <- function(todo_mgr, todos) {     # Validate input structure     if (!is.list(todos)) {         return(\"Error: todos must be a list\")     }      errors <- character()     for (i in seq_along(todos)) {         todo <- todos[[i]]          # Check required fields         if (!all(c(\"content\", \"status\") %in% names(todo))) {             errors <- c(errors, sprintf(\"Todo %d missing required fields (content, status)\", i))         }          # Validate status         valid_statuses <- c(\"pending\", \"in_progress\", \"completed\")         if (!todo$status %in% valid_statuses) {             errors <- c(errors, sprintf(                 \"Todo %d has invalid status '%s'. Must be one of: %s\",                 i, todo$status, paste(valid_statuses, collapse = \", \")             ))         }     }      if (length(errors) > 0) {         return(paste(c(\"Errors found:\", errors), collapse = \"\\\\n- \"))     }      todo_mgr$todos <- todos     return(sprintf(\"Successfully updated %d TODO(s)\", length(todos))) }"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-argentic-workflow.html","id":"github-mcp-tools","dir":"Articles","previous_headings":"","what":"GitHub MCP Tools","title":"Advanced Agentic Workflows","text":"Note See Using MCP Servers argent article details use MCP servers argent.","code":"github_mcp_server <- mcp_server(     name = \"github\",     type = \"http\",     url = \"https://api.githubcopilot.com/mcp\",     headers = list(Authorization = paste(\"Bearer\", Sys.getenv(\"GITHUB_PAT\"))) )  github_mcp_tools <- mcp_tools(github_mcp_server, tools = c(\"search_code\", \"get_file_contents\"))"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-argentic-workflow.html","id":"creating-our-agents","dir":"Articles","previous_headings":"","what":"Creating our Agents","title":"Advanced Agentic Workflows","text":"example, one main/orchestrator agent, two sub-agents: planning sub-agent (web/github) research sub-agent. planning sub-agent’s role us analyze complex requests breaks set actionable tasks. uses web search tool understand domain creates structured plan using TODO-list tool. Warning Don’t forget add namespaces functions use defining tools, otherwise, parallel tool calling fail. research sub-agent executes specific research tasks using web search, web fetch, GitHub MCP tools. main agent coordinates workflow: calls planner, delegates tasks researchers, synthesizes results. Let’s define main agent’s system prompt prompt: , let’s define main agent’s output schema tools: Now, let’s run main agent: Finally, can print main agent’s output:","code":"planning_subagent <- function(task) {     #' @description Creates a planning sub-agent that analyzes a complex task and breaks it down     #'   into structured sub-tasks. Use this tool FIRST to create a roadmap for complex requests.     #' @param task:string* The complex research task to plan. The tool will return a list of sub-tasks to enact.      # Create a fresh TODO-list for this planning subagent     todo_list <- create_todo_list()      planner <- argent::OpenRouter$new(default_model = \"x-ai/grok-4.1-fast\", auto_save_history = FALSE)      planning_system <- stringr::str_glue(         \"You are a Lead Architect and Planning Specialist.         Your goal is to analyze complex requests and break them down into clear, actionable         sub-tasks.          <process>         1. ANALYZE: Use `web_search` to understand the domain and identify key components.         2. THINK: Reflect on the necessary steps to achieve the user's goal.         3. PLAN: Use `set_todos` to create a comprehensive plan.         4. SUMMARIZE: Return a concise 'Handoff Summary' explaining the strategy.         <\/process>          <guidelines>         - Create independent sub-tasks that can be delegated to a research specialist            with access to web pages & GitHub code repositories.         - Be specific about what needs to be researched or implemented in each step.         - Do not attempt to execute the research yourself, only plan it.         <\/guidelines>          The current date is {lubridate::today()}.\"     )      planning_prompt <- stringr::str_glue(         \"Analyze this task and create a detailed plan:\\\\n\\\\n<task>\\\\n{task}\\\\n<\/task>\"     )      # Create closure-based tools that bind to this todo_manager instance     get_todos_planner <- function() {         #' @description Get the current TODO list         get_todos(todo_list)     }     get_todos_planner_tool <- as_tool(get_todos_planner)      set_todos_planner <- function(todos) {         set_todos(todo_list, todos)     }     # We define the tool manually since there is no way to specify nested lists as function annotations.     set_todos_planner_tool <- tool(         name = \"set_todos_planner\",         description = \"Update your task list for planning and tracking multi-step work. This             replaces the entire TODO list. Always include all TODOs (completed, in_progress, and             pending) as a list of objects.\",         todos = list(             type = \"[object]*\",             description = \"List of TODO items, each with 'content' (string) and 'status'                 (pending, in_progress, or completed)\",             content = \"string* Task description\",             status = \"string* One of: 'pending', 'in_progress', or 'completed'\"         ),         fn = set_todos_planner     )      web_search_tool <- argent::as_tool(web_search)      # We create a flat list of tools that the planner can use     tools <- argent::flat_list(web_search_tool, get_todos_planner_tool, set_todos_planner_tool)      planner$chat(planning_prompt, system = planning_system, tools = tools)      list(         plan = get_todos_planner(),         handoff_summary = planner$get_content_text()     ) } research_subagent <- function(subtask) {     #' @description Creates a research sub-agent that executes a specific research task using     #'   web search, web fetch, and GitHub tools. Use this for deep-dive information gathering.     #' @param subtask:string* The specific research sub-task to execute. It must be a specific     #'   task/topic with a limited scope.      researcher <- argent::OpenRouter$new(default_model = \"x-ai/grok-4.1-fast\", auto_save_history = FALSE)      system <- stringr::str_glue(         \"You are a Senior Research Specialist.         You will be given a specific research task to execute thoroughly.          <responsibilities>         - Use `web_search` to find diverse and authoritative sources.         - Use `web_fetch` to read full content of promising pages.         - Use GitHub MCP tools to explore code examples where relevant.         - Cross-reference information to ensure accuracy.         <\/responsibilities>          <output_format>         Return your findings in a structured Markdown format:         # Executive Summary         (Brief overview of findings)          # Key Findings         (Detailed analysis, grouped by concepts)          # Source/Code Analysis         (Specific details from code or technical docs)          # References         (List of all URLs and repos visited)         <\/output_format>          The current date is {lubridate::today()}.\"     )      prompt <- stringr::str_glue(\"Here is the task to research:\\\\n\\\\n<subtask>\\\\n{subtask}\\\\n<\/subtask>\")      web_search_tool <- argent::as_tool(web_search)     web_fetch_tool <- argent::as_tool(web_fetch)      tools <- argent::flat_list(web_search_tool, web_fetch_tool, github_mcp_tools)      researcher$chat(prompt, system = system, tools = tools)      return(researcher$get_content_text()) } main_agent <- OpenRouter$new(default_model = \"x-ai/grok-4.1-fast\")  main_system <- \"You are the Project Lead and Orchestrator. Your goal is to deliver high-quality results by effectively delegating work to specialized sub-agents.  <workflow> 1. PLAN: Call `planning_subagent` to create a roadmap for the user's request. 2. DELEGATE: Iterate through the plan and delegate tasks to `research_subagent`. 3. REVIEW: Analyze the findings returned by the researchers. 4. SYNTHESIZE: Compile the final answer for the user based on all research, using the provided output schema. <\/workflow>  <tools> - `planning_subagent`: Use this ONCE at the start to get a list of tasks. - `research_subagent`: Use this for EACH task in the plan to get detailed information.     A task must be a specific task/topic with a limited scope. <\/tools> \"  main_prompt <- \" Research all LLM-adjacent packages in R, such as: LLM APIs, prompt engineering, embeddings, vector databases, RAG, AI agents, MCPs, and LLM tooling.  For each package, identify: 1. The package name 2. Its GitHub repository URL 3. Its main purpose and functionality 4. Which other LLM-adjacent packages from your findings it is depended on or imports  Stop once you've found 20 packages. \" main_output_schema <- schema(     name = \"r_llm_packages\",     description = \"A comprehensive list of LLM-adjacent R packages\",     packages = list(         type = \"[object]*\",         description = \"Array of package objects\",         package_name = \"string* Package name\",         package_repo = \"string* GitHub repository URL\",         package_description = \"string* Brief description of the package's main purpose\",         package_dependencies = \"[string]* List of other LLM-adjacent packages this package depends on/imports\"     ) )  main_tools <- list(as_tool(planning_subagent), as_tool(research_subagent)) main_agent$chat(     main_prompt,     system = main_system,     tools = main_tools,     output_schema = main_output_schema ) print(main_agent, show_tools = TRUE) # Entirety of the chat history of the main agent cat(main_agent$get_content_text()) # Just the final answer of the main agent {   \"packages\": [     {       \"package_name\": \"ellmer\",       \"package_repo\": \"https://github.com/tidyverse/ellmer\",       \"package_description\": \"Unified tidy chat interface for LLM APIs (OpenAI, Anthropic,         Gemini, Ollama, etc.); streaming, tools, structured output, auto-creds.\",       \"package_dependencies\": []     },     {       \"package_name\": \"tidyllm\",       \"package_repo\": \"https://github.com/edubruell/tidyllm\",       \"package_description\": \"Tidy verbs/pipelines for LLMs (Claude, OpenAI, Gemini, Ollama);         embeddings, chaining, rate limits.\",       \"package_dependencies\": []     },     {       \"package_name\": \"LLMR\",       \"package_repo\": \"https://github.com/asanaei/LLMR\",       \"package_description\": \"Unified API with batching, retries, embeddings, structured output         (OpenAI/Azure/Mistral/Ollama).\",       \"package_dependencies\": []     },     {       \"package_name\": \"ollamar\",       \"package_repo\": \"https://github.com/hauselin/ollama-r\",       \"package_description\": \"Local Ollama API client for chat/completions/embeddings; parallel         via httr2.\",       \"package_dependencies\": []     },     {       \"package_name\": \"localLLM\",       \"package_repo\": \"https://github.com/EddieYang211/localLLM\",       \"package_description\": \"Lightweight llama.cpp bindings for local GGUF LLMs in R         (no server).\",       \"package_dependencies\": []     },     {       \"package_name\": \"btw\",       \"package_repo\": \"https://github.com/posit-dev/btw\",       \"package_description\": \"R env/object/package inspector for LLM prompts; tools for ellmer         chats/agents.\",       \"package_dependencies\": \"ellmer\"     },     {       \"package_name\": \"tidyprompt\",       \"package_repo\": \"https://github.com/KennispuntTwente/tidyprompt\",       \"package_description\": \"Prompt 'wraps' for engineering/chaining (JSON/reasoning/tools);         provider-agnostic.\",       \"package_dependencies\": []     },     {       \"package_name\": \"ragnar\",       \"package_repo\": \"https://github.com/tidyverse/ragnar\",       \"package_description\": \"End-to-end RAG: chunking, embeddings, hybrid retrieval, LLM         integration.\",       \"package_dependencies\": [\"ellmer\", \"btw\"]     },     {       \"package_name\": \"LLMAgentR\",       \"package_repo\": \"https://github.com/knowusuboaky/LLMAgentR\",       \"package_description\": \"Graph-based agents for R tasks (data wrangling/viz/forecasting         via tidymodels).\",       \"package_dependencies\": \"ellmer\"     },     {       \"package_name\": \"chattr\",       \"package_repo\": \"https://github.com/mlverse/chattr\",       \"package_description\": \"Interactive Shiny/console LLM chats; ellmer backend.\",       \"package_dependencies\": \"ellmer\"     },     {       \"package_name\": \"gptstudio\",       \"package_repo\": \"https://github.com/MichelNivard/gptstudio\",       \"package_description\": \"RStudio addins for LLM code/doc tasks (OpenAI/Anthropic/Cohere).\",       \"package_dependencies\": []     },     {       \"package_name\": \"ragent\",       \"package_repo\": \"https://github.com/cynkra/ragent\",       \"package_description\": \"Simple Ollama-based agents with tools (calc/RAG).\",       \"package_dependencies\": \"ollamar\"     },     {       \"package_name\": \"mini007\",       \"package_repo\": \"https://github.com/feddelegrand7/mini007\",       \"package_description\": \"Lightweight multi-agent orchestration (R6 classes, delegation).\",       \"package_dependencies\": \"ellmer\"     },     {       \"package_name\": \"mcptools\",       \"package_repo\": \"https://github.com/posit-dev/mcptools\",       \"package_description\": \"MCP server/client for R-LLM integration (Claude Desktop/Copilot).\",       \"package_dependencies\": [\"ellmer\", \"btw\"]     },     {       \"package_name\": \"rchroma\",       \"package_repo\": \"https://github.com/cynkra/rchroma\",       \"package_description\": \"ChromaDB client for vector storage/query (RAG enabler).\",       \"package_dependencies\": []     },     {       \"package_name\": \"RAGFlowChainR\",       \"package_repo\": \"https://github.com/knowusuboaky/RAGFlowChainR\",       \"package_description\": \"LangChain-like RAG chains w/ DuckDB vectors/web search.\",       \"package_dependencies\": \"LLMAgentR\"     },     {       \"package_name\": \"tidychatmodels\",       \"package_repo\": \"https://github.com/AlbertRapp/tidychatmodels\",       \"package_description\": \"Tidymodels-inspired modular chatbots (OpenAI/Claude/Ollama).\",       \"package_dependencies\": []     },     {       \"package_name\": \"mall\",       \"package_repo\": \"https://github.com/mlverse/mall\",       \"package_description\": \"Row-wise LLM NLP on data frames (summ/classify via ellmer).\",       \"package_dependencies\": \"ellmer\"     },     {       \"package_name\": \"mcpr\",       \"package_repo\": \"https://github.com/chi2labs/mcpr\",       \"package_description\": \"MCP framework for R servers/clients (w/ llmr).\",       \"package_dependencies\": []     },     {       \"package_name\": \"rmcp\",       \"package_repo\": \"https://github.com/finite-sample/rmcp\",       \"package_description\": \"Stats-focused MCP server (52 tools from CRAN).\",       \"package_dependencies\": []     }   ] }"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-argentic-workflow.html","id":"planning-sub-agent","dir":"Articles","previous_headings":"","what":"Planning Sub-Agent","title":"Advanced Agentic Workflows","text":"planning sub-agent’s role us analyze complex requests breaks set actionable tasks. uses web search tool understand domain creates structured plan using TODO-list tool. Warning Don’t forget add namespaces functions use defining tools, otherwise, parallel tool calling fail.","code":"planning_subagent <- function(task) {     #' @description Creates a planning sub-agent that analyzes a complex task and breaks it down     #'   into structured sub-tasks. Use this tool FIRST to create a roadmap for complex requests.     #' @param task:string* The complex research task to plan. The tool will return a list of sub-tasks to enact.      # Create a fresh TODO-list for this planning subagent     todo_list <- create_todo_list()      planner <- argent::OpenRouter$new(default_model = \"x-ai/grok-4.1-fast\", auto_save_history = FALSE)      planning_system <- stringr::str_glue(         \"You are a Lead Architect and Planning Specialist.         Your goal is to analyze complex requests and break them down into clear, actionable         sub-tasks.          <process>         1. ANALYZE: Use `web_search` to understand the domain and identify key components.         2. THINK: Reflect on the necessary steps to achieve the user's goal.         3. PLAN: Use `set_todos` to create a comprehensive plan.         4. SUMMARIZE: Return a concise 'Handoff Summary' explaining the strategy.         <\/process>          <guidelines>         - Create independent sub-tasks that can be delegated to a research specialist            with access to web pages & GitHub code repositories.         - Be specific about what needs to be researched or implemented in each step.         - Do not attempt to execute the research yourself, only plan it.         <\/guidelines>          The current date is {lubridate::today()}.\"     )      planning_prompt <- stringr::str_glue(         \"Analyze this task and create a detailed plan:\\\\n\\\\n<task>\\\\n{task}\\\\n<\/task>\"     )      # Create closure-based tools that bind to this todo_manager instance     get_todos_planner <- function() {         #' @description Get the current TODO list         get_todos(todo_list)     }     get_todos_planner_tool <- as_tool(get_todos_planner)      set_todos_planner <- function(todos) {         set_todos(todo_list, todos)     }     # We define the tool manually since there is no way to specify nested lists as function annotations.     set_todos_planner_tool <- tool(         name = \"set_todos_planner\",         description = \"Update your task list for planning and tracking multi-step work. This             replaces the entire TODO list. Always include all TODOs (completed, in_progress, and             pending) as a list of objects.\",         todos = list(             type = \"[object]*\",             description = \"List of TODO items, each with 'content' (string) and 'status'                 (pending, in_progress, or completed)\",             content = \"string* Task description\",             status = \"string* One of: 'pending', 'in_progress', or 'completed'\"         ),         fn = set_todos_planner     )      web_search_tool <- argent::as_tool(web_search)      # We create a flat list of tools that the planner can use     tools <- argent::flat_list(web_search_tool, get_todos_planner_tool, set_todos_planner_tool)      planner$chat(planning_prompt, system = planning_system, tools = tools)      list(         plan = get_todos_planner(),         handoff_summary = planner$get_content_text()     ) }"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-argentic-workflow.html","id":"research-sub-agent","dir":"Articles","previous_headings":"","what":"Research Sub-Agent","title":"Advanced Agentic Workflows","text":"research sub-agent executes specific research tasks using web search, web fetch, GitHub MCP tools.","code":"research_subagent <- function(subtask) {     #' @description Creates a research sub-agent that executes a specific research task using     #'   web search, web fetch, and GitHub tools. Use this for deep-dive information gathering.     #' @param subtask:string* The specific research sub-task to execute. It must be a specific     #'   task/topic with a limited scope.      researcher <- argent::OpenRouter$new(default_model = \"x-ai/grok-4.1-fast\", auto_save_history = FALSE)      system <- stringr::str_glue(         \"You are a Senior Research Specialist.         You will be given a specific research task to execute thoroughly.          <responsibilities>         - Use `web_search` to find diverse and authoritative sources.         - Use `web_fetch` to read full content of promising pages.         - Use GitHub MCP tools to explore code examples where relevant.         - Cross-reference information to ensure accuracy.         <\/responsibilities>          <output_format>         Return your findings in a structured Markdown format:         # Executive Summary         (Brief overview of findings)          # Key Findings         (Detailed analysis, grouped by concepts)          # Source/Code Analysis         (Specific details from code or technical docs)          # References         (List of all URLs and repos visited)         <\/output_format>          The current date is {lubridate::today()}.\"     )      prompt <- stringr::str_glue(\"Here is the task to research:\\\\n\\\\n<subtask>\\\\n{subtask}\\\\n<\/subtask>\")      web_search_tool <- argent::as_tool(web_search)     web_fetch_tool <- argent::as_tool(web_fetch)      tools <- argent::flat_list(web_search_tool, web_fetch_tool, github_mcp_tools)      researcher$chat(prompt, system = system, tools = tools)      return(researcher$get_content_text()) }"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-argentic-workflow.html","id":"main-agent","dir":"Articles","previous_headings":"","what":"Main Agent","title":"Advanced Agentic Workflows","text":"main agent coordinates workflow: calls planner, delegates tasks researchers, synthesizes results. Let’s define main agent’s system prompt prompt: , let’s define main agent’s output schema tools: Now, let’s run main agent: Finally, can print main agent’s output:","code":"main_agent <- OpenRouter$new(default_model = \"x-ai/grok-4.1-fast\")  main_system <- \"You are the Project Lead and Orchestrator. Your goal is to deliver high-quality results by effectively delegating work to specialized sub-agents.  <workflow> 1. PLAN: Call `planning_subagent` to create a roadmap for the user's request. 2. DELEGATE: Iterate through the plan and delegate tasks to `research_subagent`. 3. REVIEW: Analyze the findings returned by the researchers. 4. SYNTHESIZE: Compile the final answer for the user based on all research, using the provided output schema. <\/workflow>  <tools> - `planning_subagent`: Use this ONCE at the start to get a list of tasks. - `research_subagent`: Use this for EACH task in the plan to get detailed information.     A task must be a specific task/topic with a limited scope. <\/tools> \"  main_prompt <- \" Research all LLM-adjacent packages in R, such as: LLM APIs, prompt engineering, embeddings, vector databases, RAG, AI agents, MCPs, and LLM tooling.  For each package, identify: 1. The package name 2. Its GitHub repository URL 3. Its main purpose and functionality 4. Which other LLM-adjacent packages from your findings it is depended on or imports  Stop once you've found 20 packages. \" main_output_schema <- schema(     name = \"r_llm_packages\",     description = \"A comprehensive list of LLM-adjacent R packages\",     packages = list(         type = \"[object]*\",         description = \"Array of package objects\",         package_name = \"string* Package name\",         package_repo = \"string* GitHub repository URL\",         package_description = \"string* Brief description of the package's main purpose\",         package_dependencies = \"[string]* List of other LLM-adjacent packages this package depends on/imports\"     ) )  main_tools <- list(as_tool(planning_subagent), as_tool(research_subagent)) main_agent$chat(     main_prompt,     system = main_system,     tools = main_tools,     output_schema = main_output_schema ) print(main_agent, show_tools = TRUE) # Entirety of the chat history of the main agent cat(main_agent$get_content_text()) # Just the final answer of the main agent {   \"packages\": [     {       \"package_name\": \"ellmer\",       \"package_repo\": \"https://github.com/tidyverse/ellmer\",       \"package_description\": \"Unified tidy chat interface for LLM APIs (OpenAI, Anthropic,         Gemini, Ollama, etc.); streaming, tools, structured output, auto-creds.\",       \"package_dependencies\": []     },     {       \"package_name\": \"tidyllm\",       \"package_repo\": \"https://github.com/edubruell/tidyllm\",       \"package_description\": \"Tidy verbs/pipelines for LLMs (Claude, OpenAI, Gemini, Ollama);         embeddings, chaining, rate limits.\",       \"package_dependencies\": []     },     {       \"package_name\": \"LLMR\",       \"package_repo\": \"https://github.com/asanaei/LLMR\",       \"package_description\": \"Unified API with batching, retries, embeddings, structured output         (OpenAI/Azure/Mistral/Ollama).\",       \"package_dependencies\": []     },     {       \"package_name\": \"ollamar\",       \"package_repo\": \"https://github.com/hauselin/ollama-r\",       \"package_description\": \"Local Ollama API client for chat/completions/embeddings; parallel         via httr2.\",       \"package_dependencies\": []     },     {       \"package_name\": \"localLLM\",       \"package_repo\": \"https://github.com/EddieYang211/localLLM\",       \"package_description\": \"Lightweight llama.cpp bindings for local GGUF LLMs in R         (no server).\",       \"package_dependencies\": []     },     {       \"package_name\": \"btw\",       \"package_repo\": \"https://github.com/posit-dev/btw\",       \"package_description\": \"R env/object/package inspector for LLM prompts; tools for ellmer         chats/agents.\",       \"package_dependencies\": \"ellmer\"     },     {       \"package_name\": \"tidyprompt\",       \"package_repo\": \"https://github.com/KennispuntTwente/tidyprompt\",       \"package_description\": \"Prompt 'wraps' for engineering/chaining (JSON/reasoning/tools);         provider-agnostic.\",       \"package_dependencies\": []     },     {       \"package_name\": \"ragnar\",       \"package_repo\": \"https://github.com/tidyverse/ragnar\",       \"package_description\": \"End-to-end RAG: chunking, embeddings, hybrid retrieval, LLM         integration.\",       \"package_dependencies\": [\"ellmer\", \"btw\"]     },     {       \"package_name\": \"LLMAgentR\",       \"package_repo\": \"https://github.com/knowusuboaky/LLMAgentR\",       \"package_description\": \"Graph-based agents for R tasks (data wrangling/viz/forecasting         via tidymodels).\",       \"package_dependencies\": \"ellmer\"     },     {       \"package_name\": \"chattr\",       \"package_repo\": \"https://github.com/mlverse/chattr\",       \"package_description\": \"Interactive Shiny/console LLM chats; ellmer backend.\",       \"package_dependencies\": \"ellmer\"     },     {       \"package_name\": \"gptstudio\",       \"package_repo\": \"https://github.com/MichelNivard/gptstudio\",       \"package_description\": \"RStudio addins for LLM code/doc tasks (OpenAI/Anthropic/Cohere).\",       \"package_dependencies\": []     },     {       \"package_name\": \"ragent\",       \"package_repo\": \"https://github.com/cynkra/ragent\",       \"package_description\": \"Simple Ollama-based agents with tools (calc/RAG).\",       \"package_dependencies\": \"ollamar\"     },     {       \"package_name\": \"mini007\",       \"package_repo\": \"https://github.com/feddelegrand7/mini007\",       \"package_description\": \"Lightweight multi-agent orchestration (R6 classes, delegation).\",       \"package_dependencies\": \"ellmer\"     },     {       \"package_name\": \"mcptools\",       \"package_repo\": \"https://github.com/posit-dev/mcptools\",       \"package_description\": \"MCP server/client for R-LLM integration (Claude Desktop/Copilot).\",       \"package_dependencies\": [\"ellmer\", \"btw\"]     },     {       \"package_name\": \"rchroma\",       \"package_repo\": \"https://github.com/cynkra/rchroma\",       \"package_description\": \"ChromaDB client for vector storage/query (RAG enabler).\",       \"package_dependencies\": []     },     {       \"package_name\": \"RAGFlowChainR\",       \"package_repo\": \"https://github.com/knowusuboaky/RAGFlowChainR\",       \"package_description\": \"LangChain-like RAG chains w/ DuckDB vectors/web search.\",       \"package_dependencies\": \"LLMAgentR\"     },     {       \"package_name\": \"tidychatmodels\",       \"package_repo\": \"https://github.com/AlbertRapp/tidychatmodels\",       \"package_description\": \"Tidymodels-inspired modular chatbots (OpenAI/Claude/Ollama).\",       \"package_dependencies\": []     },     {       \"package_name\": \"mall\",       \"package_repo\": \"https://github.com/mlverse/mall\",       \"package_description\": \"Row-wise LLM NLP on data frames (summ/classify via ellmer).\",       \"package_dependencies\": \"ellmer\"     },     {       \"package_name\": \"mcpr\",       \"package_repo\": \"https://github.com/chi2labs/mcpr\",       \"package_description\": \"MCP framework for R servers/clients (w/ llmr).\",       \"package_dependencies\": []     },     {       \"package_name\": \"rmcp\",       \"package_repo\": \"https://github.com/finite-sample/rmcp\",       \"package_description\": \"Stats-focused MCP server (52 tools from CRAN).\",       \"package_dependencies\": []     }   ] }"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-mcp.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Using MCP Servers with argent","text":"Model Context Protocol (MCP) enables LLMs securely interact external tools data sources. vignette demonstrates integrate MCP servers argent extend AI agent’s capabilities. MCP (Model Context Protocol) open protocol standardizes applications provide context LLMs. MCP servers expose three main primitives: Tools: Executable functions LLM can call Resources: File-like structured data LLM can access (supported yet) Prompts: Predefined templates interactions (supported yet) two main types MCP servers: HTTP stdio. HTTP servers typically used cloud-based services like GitHub, stdio servers typically used local services like Docker npx. types MCP servers can used argent. Note rely mcptools package interacting stdio MCP servers.","code":""},{"path":"https://ma-riviere.github.io/argent/articles/advanced-mcp.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Using MCP Servers with argent","text":"","code":"mirai::daemons(4) # To enable parallel tool calling"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-mcp.html","id":"basic-usage","dir":"Articles","previous_headings":"","what":"Basic Usage","title":"Using MCP Servers with argent","text":"Let’s look GitHub MCP server example. HTTP server can used interact GitHub API. , can get tools want GitHub MCP server: Finally, can call get_file_contents tool manually see works: Let’s look BTW MCP server example (requires btw package installed). stdio server can used interact btw package. , can get tools want btw MCP server: Finally, can call help_topics tool manually see works:","code":"github_mcp_server <- mcp_server(     name = \"github\",     type = \"http\",     url = \"https://api.githubcopilot.com/mcp\",     headers = list(         Authorization = paste(\"Bearer\", Sys.getenv(\"GITHUB_PAT\"))     ) ) github_mcp_tools <- mcp_tools(github_mcp_server, tools = c(\"get_file_contents\", \"search_code\")) get_file_contents_mcp_tool <- purrr::keep(github_mcp_tools, \\(tool) tool$name == \"get_file_contents\")[[1]]  execute_mcp_tool(     tool_def = get_file_contents_mcp_tool,     arguments = list(         owner = \"ma-riviere\",         repo = \"argent\",         path = \"R/aaa-utils.R\",         ref = \"main\"     ) ) btw_mcp_server <- mcp_server(     name = \"btw\",     type = \"stdio\",     command = \"Rscript\",     args = c(         \"-e\",         \"btw::btw_mcp_server(tools = btw::btw_tools(c('docs', 'env', 'ide', 'search', 'session')))\"     ),     env = list() ) btw_mcp_tools <- mcp_tools(     btw_mcp_server,     tools = c(         \"btw_tool_session_check_package_installed\",         \"btw_tool_docs_available_vignettes\",         \"btw_tool_docs_package_help_topics\",         \"btw_tool_docs_help_page\",         \"btw_tool_docs_vignette\",         \"btw_tool_session_package_info\",         \"btw_tool_docs_package_help_topics\"     ) ) help_topics_tool <- purrr::keep(btw_mcp_tools, \\(tool) tool$name == \"btw_tool_docs_package_help_topics\")[[1]]  execute_mcp_tool(     tool_def = help_topics_tool,     arguments = list(         package_name = \"argent\",         `_intent` = \"Vignettes explaining how to use MCP servers with argent\"     ) )"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-mcp.html","id":"using-an-http-mcp-server","dir":"Articles","previous_headings":"","what":"Using an ‘HTTP’ MCP Server","title":"Using MCP Servers with argent","text":"Let’s look GitHub MCP server example. HTTP server can used interact GitHub API. , can get tools want GitHub MCP server: Finally, can call get_file_contents tool manually see works:","code":"github_mcp_server <- mcp_server(     name = \"github\",     type = \"http\",     url = \"https://api.githubcopilot.com/mcp\",     headers = list(         Authorization = paste(\"Bearer\", Sys.getenv(\"GITHUB_PAT\"))     ) ) github_mcp_tools <- mcp_tools(github_mcp_server, tools = c(\"get_file_contents\", \"search_code\")) get_file_contents_mcp_tool <- purrr::keep(github_mcp_tools, \\(tool) tool$name == \"get_file_contents\")[[1]]  execute_mcp_tool(     tool_def = get_file_contents_mcp_tool,     arguments = list(         owner = \"ma-riviere\",         repo = \"argent\",         path = \"R/aaa-utils.R\",         ref = \"main\"     ) )"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-mcp.html","id":"using-a-stdio-mcp-server","dir":"Articles","previous_headings":"","what":"Using a ‘stdio’ MCP Server","title":"Using MCP Servers with argent","text":"Let’s look BTW MCP server example (requires btw package installed). stdio server can used interact btw package. , can get tools want btw MCP server: Finally, can call help_topics tool manually see works:","code":"btw_mcp_server <- mcp_server(     name = \"btw\",     type = \"stdio\",     command = \"Rscript\",     args = c(         \"-e\",         \"btw::btw_mcp_server(tools = btw::btw_tools(c('docs', 'env', 'ide', 'search', 'session')))\"     ),     env = list() ) btw_mcp_tools <- mcp_tools(     btw_mcp_server,     tools = c(         \"btw_tool_session_check_package_installed\",         \"btw_tool_docs_available_vignettes\",         \"btw_tool_docs_package_help_topics\",         \"btw_tool_docs_help_page\",         \"btw_tool_docs_vignette\",         \"btw_tool_session_package_info\",         \"btw_tool_docs_package_help_topics\"     ) ) help_topics_tool <- purrr::keep(btw_mcp_tools, \\(tool) tool$name == \"btw_tool_docs_package_help_topics\")[[1]]  execute_mcp_tool(     tool_def = help_topics_tool,     arguments = list(         package_name = \"argent\",         `_intent` = \"Vignettes explaining how to use MCP servers with argent\"     ) )"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-mcp.html","id":"argent-mcp-servers","dir":"Articles","previous_headings":"","what":"Argent + MCP Servers","title":"Using MCP Servers with argent","text":"Let’s use GitHub MCP server BTW MCP server ask complex question mcptools ellmer packages. MCP Specification MCP Servers mcptools Documentation GitHub MCP Server Claude Desktop MCP Guide","code":"google <- Google$new()  tools <- flat_list(github_mcp_tools, btw_mcp_tools, as_tool(web_search), as_tool(web_fetch))  res <- google$chat(     \"Has 'posit-dev/mcptools' implemented the ability to use HTTP MCP servers with 'ellmer' ?\",     \"Use the `get_file_contents` tool to list the contents of GitHub subdirectories, e.g. with path = '/' or 'dir/'.\",     \"Use the `btw` tools to explore the help pages and vignettes of the local installation of the `mcptools` package.\",     model = \"gemini-2.5-flash\",     tools = tools )  print(google, show_tools = TRUE) `posit-dev/mcptools` does not directly implement the ability to use HTTP MCP servers with `ellmer`. When `mcptools` acts as an MCP *client* via `ellmer`, it only supports the local (stdio) protocol. To connect to remote (HTTP) MCP servers, the `mcptools` documentation recommends using `mcp-remote`, an external tool (a local stdio MCP server) that converts remote HTTP servers to `mcptools`-compatible local ones. This allows `ellmer` (using the stdio protocol) to interact with remote HTTP MCP servers through `mcp-remote`. ── [ <Google> turns: 10 | Current context: 62853 | Cumulated tokens: 196436 ] ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────   ── user [1789 / 2107] ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  Has 'posit-dev/mcptools' implemented the ability to use HTTP MCP servers with ellmer ? Use the `get_file_contents` tool to list the contents of GitHub subdirectories, e.g. with path = '/' or 'dir/'. Use the `btw` tools to explore the help pages and vignettes of the local installation of the `mcptools` package.  ── System ──  You are a helpful AI assistant. Use your knowledge, the files you have access to, and the tools at your disposal to answer the user's query. You can use your tools multiple times, but use them sparingly. Make parallel tool calls if relevant to the user's query. Answer the user's query as soon as you have the information necessary to answer. Self-reflect and double-check your answer before responding. If you don't know the answer even after using your tools, say 'I don't know'. If you do not have all the information necessary to use a provided tool, use NA for required arguments. Today's date is 2025-11-19  ── Tool Definitions ──  • get_file_contents(owner, path, ref, repo, sha): Get the contents of a file or directory from a GitHub repository • search_code(order, page, perPage, query, sort): Fast and precise code search across ALL GitHub repositories using GitHub's native search engine. Best for finding exact symbols, functions, classes, or specific   code patterns. • btw_tool_docs_package_help_topics(package_name, _intent): Get available help topics for an R package. • btw_tool_docs_help_page(package_name, topic, _intent): Get help page from package. • btw_tool_docs_available_vignettes(package_name, _intent): List available vignettes for an R package. Vignettes are articles describing key concepts or features of an R package. Returns the listing as a JSON   array of `vignette` and `title`. To read a vignette, use `btw_tool_docs_vignette(package_name, vignette)`. • btw_tool_docs_vignette(package_name, vignette, _intent): Get a package vignette in plain text. • btw_tool_session_check_package_installed(package_name, _intent): Check if a package is installed in the current session. • btw_tool_session_package_info(packages, dependencies, _intent): Verify that a specific package is installed, or find out which packages are in use in the current session. As a last resort, this function can   also list all installed packages. • web_search(query): Search the web for information using Tavily API. Returns a JSON array of search results with titles, URLs, and content snippets. Use this when you need current information, facts, news, or   any data not in your training data. • web_fetch(url): Fetch and extract the main text content from a web page as clean markdown. Returns the page content with formatting preserved, stripped of navigation, ads, and boilerplate. Use this to read   articles, documentation, blog posts, or any web page content.  ── assistant [2107 / 2107] ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  ── Tool Calls ──  • get_file_contents(repo = \"mcptools\", owner = \"posit-dev\", path = \"/\")  ── tool [8379 / 10686] ─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  • Result from get_file_contents:  name: get_file_contents arguments:   repo: mcptools   owner: posit-dev   path: / result:   content:   - type: text     text: '[{\"type\":\"file\",\"size\":210,\"name\":\".Rbuildignore\",\"path\":\".Rbuildignore\",\"sha\":\"8fe0bfc60f1d4655aff00d34de75b22f1e6bb7c3\",\"url\":\"https://api.github.com/repos/posit-dev/mcptools/contents/.Rbuildignore?ref=4f91a58684555e66fd46414f2dc63c46c3802d89\",\"git_url\":\"https://api.github.com/repos/posit-dev/mcptools/git/blobs/8fe0bfc60f1d4655aff00d34de75b22f1e6bb7c3\",\"html_url\":\"https://github.com/posit-dev/mcptools/blob/4f91a58684555e66fd46414f2dc63c46c3802d89/.Rbuildignore\",\"download_url\":\"https://raw.githubusercontent.com/posit-dev/mcptools/4f91a58684555e66fd46414f2dc63c46c3802d89/.Rbuildignore\"},{\"type\":\"dir\",\"size\":0,\"name\":\".github\",\"path\":\".github\",\"sha\":\"c1a387a5ec7f8bf8c6626c7d04d8d00551252def\",\"url\":\"https://api.github.com/repos/posit-dev/mcptools/contents/.github?ref=4f91a58684555e66fd46414f2dc63c46c3802d89\",\"git_url\":\"https://api.github.com/repos/posit-dev/mcpto...  ── assistant [8579 / 10686] ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  ── Tool Calls ──  • search_code(query = \"ellmer repo:posit-dev/mcptools\") • search_code(query = \"http mcp server repo:posit-dev/mcptools\")  ── tool [60874 / 72190] ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  • Result from search_code:  name: search_code arguments:   query: ellmer repo:posit-dev/mcptools result:   content:   - type: text     text: '{\"total_count\":17,\"incomplete_results\":false,\"items\":[{\"name\":\"example-ellmer-tools.R\",\"path\":\"inst/example-ellmer-tools.R\",\"sha\":\"bfde4b0d734301e24a4908ec1054a5e0c7997b4c\",\"html_url\":\"https://github.com/posit-dev/mcptools/blob/4f91a58684555e66fd46414f2dc63c46c3802d89/inst/example-ellmer-tools.R\",\"repository\":{\"id\":955571244,\"node_id\":\"R_kgDOOPTcLA\",\"owner\":{\"login\":\"posit-dev\",\"id\":107264312,\"node_id\":\"O_kgDOBmS5OA\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/107264312?v=4\",\"html_url\":\"https://github.com/posit-dev\",\"gravatar_id\":\"\",\"type\":\"Organization\",\"site_admin\":false,\"url\":\"https://api.github.com/users/posit-dev\",\"events_url\":\"https://api.github.com/users/posit-dev/events{/privacy}\",\"following_url\":\"https://api.github.com/users/posit-dev/following{/other_user}\",\"followers_url\":\"https://api.github.com/users/posit-dev/followers\",\"gists_url\":\"https://api.githu...  • Result from search_code:  name: search_code arguments:   query: http mcp server repo:posit-dev/mcptools result:   content:   - type: text     text: '{\"total_count\":16,\"incomplete_results\":false,\"items\":[{\"name\":\"server.R\",\"path\":\"R/server.R\",\"sha\":\"c554b70c68889735629019c4b8d14d9a83f05a4b\",\"html_url\":\"https://github.com/posit-dev/mcptools/blob/4f91a58684555e66fd46414f2dc63c46c3802d89/R/server.R\",\"repository\":{\"id\":955571244,\"node_id\":\"R_kgDOOPTcLA\",\"owner\":{\"login\":\"posit-dev\",\"id\":107264312,\"node_id\":\"O_kgDOBmS5OA\",\"avatar_url\":\"https://avatars.githubusercontent.com/u/107264312?v=4\",\"html_url\":\"https://github.com/posit-dev\",\"gravatar_id\":\"\",\"type\":\"Organization\",\"site_admin\":false,\"url\":\"https://api.github.com/users/posit-dev\",\"events_url\":\"https://api.github.com/users/posit-dev/events{/privacy}\",\"following_url\":\"https://api.github.com/users/posit-dev/following{/other_user}\",\"followers_url\":\"https://api.github.com/users/posit-dev/followers\",\"gists_url\":\"https://api.github.com/users/posit-dev/gists{/gist_id}\",...  ── assistant [61504 / 72190] ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  ── Tool Calls ──  • btw_tool_docs_package_help_topics(package_name = \"mcptools\", _intent = \"Check for relevant help topics in mcptools documentation regarding ellmer and HTTP MCP servers\")  ── tool [61172 / 133583] ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  • Result from btw_tool_docs_package_help_topics:  name: btw_tool_docs_package_help_topics arguments:   package_name: mcptools   _intent: Check for relevant help topics in mcptools documentation regarding ellmer     and HTTP MCP servers result:   content:   - type: text     text: |-       [         {\"topic_id\":\"client\",\"title\":\"R as a client: Define ellmer tools from MCP servers\",\"aliases\":[\"client\",\"mcp_tools\",\"mcp_client\"]},         {\"topic_id\":\"mcptools-package\",\"title\":\"mcptools: Model Context Protocol Servers and Clients\",\"aliases\":[\"mcptools\",\"mcptools-package\"]},         {\"topic_id\":\"server\",\"title\":\"R as a server: Configure R-based tools with LLM-enabled apps\",\"aliases\":[\"server\",\"mcp_server\",\"mcp_session\",\"server\"]}       ]   isError: no   ── assistant [61393 / 133583] ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  ── Tool Calls ──  • btw_tool_docs_help_page(package_name = \"mcptools\", topic = \"client\", _intent = \"Get details on how ellmer tools are defined from MCP servers\")  ── tool [62486 / 196436] ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  • Result from btw_tool_docs_help_page:  name: btw_tool_docs_help_page arguments:   package_name: mcptools   topic: client   _intent: Get details on how ellmer tools are defined from MCP servers result:   content:   - type: text     text: |-       ## `help(package = \"mcptools\", \"client\")`        ### R as a client: Define ellmer tools from MCP servers        #### Description        These functions implement R as an MCP *client*, so that ellmer chats can       register functionality from third-party MCP servers such as those listed       here: <https://github.com/modelcontextprotocol/servers>.        `mcp_tools()` fetches tools from MCP servers configured in the mcptools       server config file and converts them to a list of tools compatible with       the `⁠$set_tools()⁠` method of ellmer::Chat objects.        #### Arguments        |  |  |       |----|----|       | `config` | A single string indicating the path to the mcptools MCP servers configuration f...  ── assistant [62853 / 196436] ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  `posit-dev/mcptools` does not directly implement the ability to use HTTP MCP servers with `ellmer`. When `mcptools` acts as an MCP *client* via `ellmer`, it only supports the local (stdio) protocol. To connect to remote (HTTP) MCP servers, the `mcptools` documentation recommends using `mcp-remote`, an external tool (a local stdio MCP server) that converts remote HTTP servers to `mcptools`-compatible local ones. This allows `ellmer` (using the stdio protocol) to interact with remote HTTP MCP servers through `mcp-remote`."},{"path":"https://ma-riviere.github.io/argent/articles/advanced-mcp.html","id":"further-reading","dir":"Articles","previous_headings":"","what":"Further Reading","title":"Using MCP Servers with argent","text":"MCP Specification MCP Servers mcptools Documentation GitHub MCP Server Claude Desktop MCP Guide","code":""},{"path":"https://ma-riviere.github.io/argent/articles/advanced-rag.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Use-case: RAG with argent and ragnar","text":"","code":"library(ragnar) library(cachem) library(stringr) library(purrr)  mirai::daemons(4) # To enable parallel tool calling"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-rag.html","id":"basic-rag-system","dir":"Articles","previous_headings":"","what":"Basic RAG System","title":"Use-case: RAG with argent and ragnar","text":"First, create knowledge store documentation. ’ll use Quarto documentation example, stay similar ragnar documentation. , let’s collect (part ) Quarto documentation Quarto website. Note made embedding function LocalLLM class local embedding model: Google, OpenAI (3 classes), OpenRouter, local LLM support embeddings. enable progressive exploration knowledge base (avoiding duplicate chunks across multiple queries), need track chunks already retrieved. Since argent’s tools stateless, use cachem::cache_mem() maintain state across tool calls. Now can use retrieval tool conversations. LLM automatically retrieve relevant documentation needed. can create tables Quarto using Markdown pipe tables. ’s example: render table three columns: “fruit” “price”, row stripes highlighting hover due {.striped .hover} attribute caption. can also control relative column widths using dashes line separating header body. example, ---|- make first column 3/4 second column 1/4 full text width. Keep mind pipe table cells contain block elements like paragraphs lists, span multiple lines LLM : Recognize needs documentation Quarto tables Call retrieve_docs(\"tables Quarto\") automatically Receive relevant documentation chunks Generate answer based retrieved context","code":"quarto_docs_url_filter <- function(urls) {     str_subset(urls, \"https://quarto.org/docs/\") |>          str_subset(regex(\"\\\\.html$\", ignore_case = FALSE)) }  paths <- ragnar::ragnar_find_links(     \"https://quarto.org/\",      depth = 1,      url_filter = quarto_docs_url_filter ) gemini_embedding_fn <- \\(text) ragnar::embed_google_gemini(text, model = \"text-embedding-004\")  store <- ragnar::ragnar_store_create(     \"data/quarto-docs.duckdb\",     embed = gemini_embedding_fn ) local_embedding_fn <- function(text) {     argent::LocalLLM$new(         base_url = \"http://127.0.0.1:5000\",         default_model = \"Qwen3-Embedding-8B-Q8_0.gguf\"     )$embeddings(text) }  store <- ragnar::ragnar_store_create(     \"data/quarto-docs.duckdb\",     embed = local_embedding_fn ) insert_chunk_into_store <- function(path) {     ragnar::read_as_markdown(path) |>         ragnar::markdown_chunk() |>         ragnar::ragnar_store_insert(store, chunks = _) }  walk(paths, insert_chunk_into_store, .progress = TRUE)  ragnar::ragnar_store_build_index(store) retrieve_state <- cachem::cache_mem()  retrieve_docs <- function(query) {     #' @description Retrieve relevant documentation chunks from Quarto docs     #' @param query:string* Search query describing what information is needed      # Get previously retrieved chunk IDs     retrieved_ids <- retrieve_state$get(\"retrieved_chunks\") %||% character(0)      # Perform hybrid search using DuckDB (vector + keyword matching)     chunks <- ragnar::ragnar_retrieve(         store,         query,         n = 5,         exclude_chunk_ids = retrieved_ids     )      # Update state with newly retrieved chunk IDs     retrieve_state$set(\"retrieved_chunks\", c(retrieved_ids, purrr::flatten_int(chunks$chunk_id)))      results <- dplyr::select(chunks, text, origin, context)     return(jsonlite::toJSON(results, auto_unbox = TRUE)) } gemini <- Google$new()  gemini$chat(     \"How do I create tables in Quarto?\",     tools = list(as_tool(retrieve_docs)) ) | fruit  | price  | |--------|--------| | apple  | 2.05   | | pear   | 1.37   | | orange | 3.09   |  : Fruit prices {.striped .hover}"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-rag.html","id":"building-a-knowledge-store","dir":"Articles","previous_headings":"","what":"Building a Knowledge Store","title":"Use-case: RAG with argent and ragnar","text":"First, create knowledge store documentation. ’ll use Quarto documentation example, stay similar ragnar documentation. , let’s collect (part ) Quarto documentation Quarto website. Note made embedding function LocalLLM class local embedding model: Google, OpenAI (3 classes), OpenRouter, local LLM support embeddings.","code":"quarto_docs_url_filter <- function(urls) {     str_subset(urls, \"https://quarto.org/docs/\") |>          str_subset(regex(\"\\\\.html$\", ignore_case = FALSE)) }  paths <- ragnar::ragnar_find_links(     \"https://quarto.org/\",      depth = 1,      url_filter = quarto_docs_url_filter ) gemini_embedding_fn <- \\(text) ragnar::embed_google_gemini(text, model = \"text-embedding-004\")  store <- ragnar::ragnar_store_create(     \"data/quarto-docs.duckdb\",     embed = gemini_embedding_fn ) local_embedding_fn <- function(text) {     argent::LocalLLM$new(         base_url = \"http://127.0.0.1:5000\",         default_model = \"Qwen3-Embedding-8B-Q8_0.gguf\"     )$embeddings(text) }  store <- ragnar::ragnar_store_create(     \"data/quarto-docs.duckdb\",     embed = local_embedding_fn ) insert_chunk_into_store <- function(path) {     ragnar::read_as_markdown(path) |>         ragnar::markdown_chunk() |>         ragnar::ragnar_store_insert(store, chunks = _) }  walk(paths, insert_chunk_into_store, .progress = TRUE)  ragnar::ragnar_store_build_index(store)"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-rag.html","id":"collecting-documents","dir":"Articles","previous_headings":"Basic RAG System","what":"Collecting Documents","title":"Use-case: RAG with argent and ragnar","text":", let’s collect (part ) Quarto documentation Quarto website.","code":"quarto_docs_url_filter <- function(urls) {     str_subset(urls, \"https://quarto.org/docs/\") |>          str_subset(regex(\"\\\\.html$\", ignore_case = FALSE)) }  paths <- ragnar::ragnar_find_links(     \"https://quarto.org/\",      depth = 1,      url_filter = quarto_docs_url_filter )"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-rag.html","id":"create-store-with-embeddings","dir":"Articles","previous_headings":"Basic RAG System","what":"Create Store with Embeddings","title":"Use-case: RAG with argent and ragnar","text":"Note made embedding function LocalLLM class local embedding model: Google, OpenAI (3 classes), OpenRouter, local LLM support embeddings.","code":"gemini_embedding_fn <- \\(text) ragnar::embed_google_gemini(text, model = \"text-embedding-004\")  store <- ragnar::ragnar_store_create(     \"data/quarto-docs.duckdb\",     embed = gemini_embedding_fn ) local_embedding_fn <- function(text) {     argent::LocalLLM$new(         base_url = \"http://127.0.0.1:5000\",         default_model = \"Qwen3-Embedding-8B-Q8_0.gguf\"     )$embeddings(text) }  store <- ragnar::ragnar_store_create(     \"data/quarto-docs.duckdb\",     embed = local_embedding_fn )"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-rag.html","id":"process-and-index-documents","dir":"Articles","previous_headings":"Basic RAG System","what":"Process and Index Documents","title":"Use-case: RAG with argent and ragnar","text":"","code":"insert_chunk_into_store <- function(path) {     ragnar::read_as_markdown(path) |>         ragnar::markdown_chunk() |>         ragnar::ragnar_store_insert(store, chunks = _) }  walk(paths, insert_chunk_into_store, .progress = TRUE)  ragnar::ragnar_store_build_index(store)"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-rag.html","id":"creating-a-stateful-retrieval-tool","dir":"Articles","previous_headings":"","what":"Creating a Stateful Retrieval Tool","title":"Use-case: RAG with argent and ragnar","text":"enable progressive exploration knowledge base (avoiding duplicate chunks across multiple queries), need track chunks already retrieved. Since argent’s tools stateless, use cachem::cache_mem() maintain state across tool calls.","code":"retrieve_state <- cachem::cache_mem()  retrieve_docs <- function(query) {     #' @description Retrieve relevant documentation chunks from Quarto docs     #' @param query:string* Search query describing what information is needed      # Get previously retrieved chunk IDs     retrieved_ids <- retrieve_state$get(\"retrieved_chunks\") %||% character(0)      # Perform hybrid search using DuckDB (vector + keyword matching)     chunks <- ragnar::ragnar_retrieve(         store,         query,         n = 5,         exclude_chunk_ids = retrieved_ids     )      # Update state with newly retrieved chunk IDs     retrieve_state$set(\"retrieved_chunks\", c(retrieved_ids, purrr::flatten_int(chunks$chunk_id)))      results <- dplyr::select(chunks, text, origin, context)     return(jsonlite::toJSON(results, auto_unbox = TRUE)) }"},{"path":"https://ma-riviere.github.io/argent/articles/advanced-rag.html","id":"using-the-rag-system","dir":"Articles","previous_headings":"","what":"Using the RAG System","title":"Use-case: RAG with argent and ragnar","text":"Now can use retrieval tool conversations. LLM automatically retrieve relevant documentation needed. can create tables Quarto using Markdown pipe tables. ’s example: render table three columns: “fruit” “price”, row stripes highlighting hover due {.striped .hover} attribute caption. can also control relative column widths using dashes line separating header body. example, ---|- make first column 3/4 second column 1/4 full text width. Keep mind pipe table cells contain block elements like paragraphs lists, span multiple lines LLM : Recognize needs documentation Quarto tables Call retrieve_docs(\"tables Quarto\") automatically Receive relevant documentation chunks Generate answer based retrieved context","code":"gemini <- Google$new()  gemini$chat(     \"How do I create tables in Quarto?\",     tools = list(as_tool(retrieve_docs)) ) | fruit  | price  | |--------|--------| | apple  | 2.05   | | pear   | 1.37   | | orange | 3.09   |  : Fruit prices {.striped .hover}"},{"path":"https://ma-riviere.github.io/argent/articles/anthropic.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Anthropic (Claude)","text":"article covers using Anthropic (Claude) provider argent. Anthropic offers powerful extended thinking capabilities, server-side tools, prompt caching cost optimization.","code":""},{"path":"https://ma-riviere.github.io/argent/articles/anthropic.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Anthropic (Claude)","text":"","code":"anthropic <- Anthropic$new(api_key = Sys.getenv(\"ANTHROPIC_API_KEY\")) mirai::daemons(4) # To enable parallel tool calling"},{"path":"https://ma-riviere.github.io/argent/articles/anthropic.html","id":"discovering-models","dir":"Articles","previous_headings":"","what":"Discovering Models","title":"Anthropic (Claude)","text":"","code":"anthropic$list_models()"},{"path":"https://ma-riviere.github.io/argent/articles/anthropic.html","id":"basic-completion","dir":"Articles","previous_headings":"","what":"Basic Completion","title":"Anthropic (Claude)","text":"","code":"anthropic$chat(     \"What's the R programming language? Answer in three sentences.\",     model = \"claude-haiku-4-5-20251001\" )"},{"path":"https://ma-riviere.github.io/argent/articles/anthropic.html","id":"tool-calling-structured-output-thinking","dir":"Articles","previous_headings":"","what":"Tool Calling + Structured Output + Thinking","title":"Anthropic (Claude)","text":"Anthropic supports extended thinking via thinking_budget parameter, allows models reason problems responding. First, define web-related tools (search, crawl, fetch, general-use tool) bundled web_tools list: Search: Fetch: , let’s define JSON schema structured output using schema(): , run agent: model keep calling tools enough information answer question. Alternatively, simply print(anthropic) see reasoning answers’ text console, turn turn. Note get_reasoning_text() get_content_text() use last API response (anthropic$get_last_response()) default.","code":"web_search <- function(query) {     #' @description Search the web for information using Tavily API. Returns a JSON array of search results with titles, URLs, and content snippets. Use this when you need current information, facts, news, or any data not in your training data.     #' @param query:string* The search query string. Be specific and use keywords that will yield the most relevant results.          return(web_search_tavily(query)) }  web_search_tavily <- function(query) {     res <- httr2::request(\"https://api.tavily.com/search\") |>          httr2::req_body_json(list(             query = query,             search_depth = \"basic\",             include_answer = FALSE,             max_results = 10,             api_key = Sys.getenv(\"TAVILY_API_KEY\")         )) |>          httr2::req_error(is_error = \\(resp) FALSE) |>          httr2::req_throttle(rate = 20/60, realm = \"tavily\") |>          httr2::req_perform() |>          httr2::resp_body_json() |>          purrr::discard_at(c(\"response_time\", \"follow_up_questions\", \"images\"))      results <- purrr::map(res$results, \\(x) purrr::discard_at(x, \"raw_content\"))      return(jsonlite::toJSON(results, pretty = FALSE, auto_unbox = TRUE)) } web_fetch <- function(url) {     #' @description Fetch and extract the main text content from a web page as clean markdown. Returns the page content with formatting preserved, stripped of navigation, ads, and boilerplate. Use this to read articles, documentation, blog posts, or any web page content.     #' @param url:string* The complete URL of the web page to fetch (e.g., \"https://example.com/article\"). Must be a valid HTTP/HTTPS URL.          trafilatura_installed <- tryCatch({         system(\"which trafilatura\", intern = TRUE, ignore.stderr = TRUE)         return(TRUE)     },     warning = function(e) {         cli::cli_alert_warning(\"trafilatura is not installed. Install with: {.code pip install trafilatura}\")         return(FALSE)     })      if (trafilatura_installed) {         res <- web_fetch_trafilatura(url)          could_not_fetch <- c(             \"Impossible to fetch the contents of this web page\",             \"Please reload this page\",             \"There was an error while loading\",             \"404\"         )         if (is.null(res) || is.na(res) || nchar(res) == 0 ||             any(stringr::str_detect(res, stringr::fixed(could_not_fetch, ignore_case = TRUE)))) {             return(web_fetch_rvest(url))         }         return(res)     }      return(web_fetch_rvest(url)) }  web_fetch_trafilatura <- function(url) {     # pip install trafilatura     tryCatch({         res <- paste0(\"trafilatura -u \", url, \" --markdown --no-comments --links \") |>              system(intern = TRUE) |>              purrr::keep(nzchar) |>             paste0(collapse = \"\\n\")                  return(res)     },     error = function(e) {         return(\"Impossible to fetch the contents of this web page. It might not allow scraping\")     }) }  web_fetch_rvest <- function(url) {     tags_to_ignore <- c(         \"a\", \"script\", \"code\", \"img\", \"svg\", \"footer\", \"g\", \"path\", \"polygon\", \"label\", \"button\", \"form\", \"input\", \"select\",          \"style\", \"link\", \"meta\", \"noscript\", \"iframe\", \"embed\", \"object\", \"param\", \"video\", \"audio\", \"track\", \"source\",          \"canvas\", \"map\", \"area\", \"math\", \"col\", \"colgroup\", \"dl\", \"dt\", \"dd\", \"hr\", \"pre\", \"address\", \"figure\", \"figcaption\",         \"dfn\", \"em\", \"kbd\", \"samp\", \"var\", \"del\", \"ins\", \"mark\", \"circle\"     )      remove_tags <- function(xml, tags) {         purrr::walk(tags, \\(tag) purrr::walk(xml2::xml_find_all(xml, paste0(\".//\", tag)), \\(node) xml2::xml_remove(node)))         return(xml)     }      cleaned_contents <- tryCatch(         rvest::read_html(url)         |> rvest::html_element(\"body\")         |> remove_tags(tags_to_ignore)         |> rvest::html_children()         |> rvest::html_text2()         |> purrr::discard(\\(x) x == \"\")         |> paste0(collapse = \"\\n\\n\"),         error = \\(e) return(\"\")     )     return(cleaned_contents) } package_info_schema <- schema(     name = \"package_info\",     description = \"Information about an R package release\",     release_version = \"string* The release version of the package\",     release_date = \"string* The release date of the `release_version`\" ) anthropic$chat(     \"When was the first release of the R 'ellmer' package on GitHub?\",     model = \"claude-haiku-4-5-20251001\",     thinking_budget = 1024,     tools = list(as_tool(web_search), as_tool(web_fetch)),     output_schema = package_info_schema ) $release_version [1] \"0.1.0\"  $release_date [1] \"2025-01-09\" cat(anthropic$get_reasoning_text())"},{"path":"https://ma-riviere.github.io/argent/articles/anthropic.html","id":"extracting-reasoning","dir":"Articles","previous_headings":"","what":"Extracting Reasoning","title":"Anthropic (Claude)","text":"Alternatively, simply print(anthropic) see reasoning answers’ text console, turn turn. Note get_reasoning_text() get_content_text() use last API response (anthropic$get_last_response()) default.","code":"cat(anthropic$get_reasoning_text())"},{"path":"https://ma-riviere.github.io/argent/articles/anthropic.html","id":"server-side-tools","dir":"Articles","previous_headings":"","what":"Server-side Tools","title":"Anthropic (Claude)","text":"Server-side tools tools can call without define . run provider’s server. Anthropic provides three server-side tools: web_search - Web search citations web_fetch - Fetch process URLs code_execution - Execute commands, write execute python code, analyze data, create visualizations files Tip web_search tool supports additional parameters like allowed_domains, exclude_domains, max_uses, user_location. See Anthropic’s documentation details. can call anthropic$get_supplementary() get citations. server tool lets LLM fetch contents URLs given prompt. web_fetch tool also supports parameters like allowed_domains, exclude_domains, max_uses. See Anthropic’s documentation details. can call anthropic$get_supplementary() get details searches server made. Anthropic’s code_execution tool provides sandboxed environment executing shell commands file operations. Note Pricing: $0.05/session-hour 5-minute minimum Resources: 5GB RAM/disk, 1 CPU, internet access Workspace: Linked API key (files can downloaded) bash_code_execution - Execute bash commands text_editor_code_execution - Edit files Expiration: 30 days creation Claude automatically determines sub-tool use. Example: Analyzing Penguins Dataset Inspect generated code: Download generated output file /data: Continue asking questions context: Cleanup:","code":"anthropic$chat(     \"What's the latest version of the R 'ellmer' package?\",     model = \"claude-haiku-4-5-20251001\",     tools = list(\"web_search\"),     output_schema = package_info_schema ) anthropic$chat(     \"What's the latest version of the R 'ellmer' package?\",     model = \"claude-haiku-4-5-20251001\",     tools = list(         list(             type = \"web_search_20250305\",             name = \"web_search\",             blocked_domains = list(\"rdrr.io\"),             max_uses = 2         )     ),     output_schema = package_info_schema ) anthropic$chat(     \"Summarize the main changes in the current development version of the R 'ellmer' package from: https://raw.githubusercontent.com/tidyverse/ellmer/main/NEWS.md\",     model = \"claude-haiku-4-5-20251001\",     tools = list(\"web_fetch\") ) penguins_url <- \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/refs/heads/main/inst/extdata/penguins.csv\" penguins_file <- anthropic$upload_file(penguins_url)  code_exec_tool <- list(type = \"code_execution\", file_ids = list(penguins_file$id))  anthropic$chat(     \"Create a summary table showing average body_mass grouped by species, sex, and year. Save as CSV.\",     model = \"claude-sonnet-4-5-20250929\",     tools = list(code_exec_tool) ) cat(anthropic$get_generated_code(langs = c(\"python\"), as_chunks = TRUE)) import pandas as pd import os  # Read the CSV file input_file = os.path.join(os.environ['INPUT_DIR'], 'fileaa34f5b158276.csv') df = pd.read_csv(input_file)  # Display basic info about the data print(\"Original data shape:\", df.shape) print(\"\\nColumn names:\", df.columns.tolist()) print(\"\\nFirst few rows:\") print(df.head())  # Check for missing values in key columns print(\"\\nMissing values in key columns:\") print(f\"species: {df['species'].isna().sum()}\") print(f\"sex: {df['sex'].isna().sum()}\") print(f\"year: {df['year'].isna().sum()}\") print(f\"body_mass: {df['body_mass'].isna().sum()}\")  # Remove rows where body_mass is NA (can't calculate average for missing values) # Also remove rows where sex is NA if you want clean grouping df_clean = df.dropna(subset=['body_mass', 'sex', 'species', 'year'])  print(f\"\\nData shape after removing rows with missing values: {df_clean.shape}\")  # Group by species, sex, and year, and calculate average body_mass summary = df_clean.groupby(['species', 'sex', 'year'])['body_mass'].mean().reset_index() summary.columns = ['species', 'sex', 'year', 'average_body_mass']  # Round to 2 decimal places for readability summary['average_body_mass'] = summary['average_body_mass'].round(2)  # Sort by species, year, and sex for better readability summary = summary.sort_values(['species', 'year', 'sex']).reset_index(drop=True)  print(\"\\nSummary table:\") print(summary)  # Save to CSV output_file = '/tmp/body_mass_summary.csv' summary.to_csv(output_file, index=False) print(f\"\\nSummary saved to {output_file}\")  # Copy to OUTPUT_DIR for export output_export = os.path.join(os.environ['OUTPUT_DIR'], 'body_mass_summary.csv') summary.to_csv(output_export, index=False) print(f\"Summary exported to {output_export}\") downloaded_path <- anthropic$download_generated_files(dest_path = \"data\") #> ✔ [Anthropic] File downloaded to: data/body_mass_summary.csv read.csv(downloaded_path) species    sex year average_body_mass 1     Adelie female 2007           3389.77 2     Adelie   male 2007           4038.64 3     Adelie female 2008           3386.00 4     Adelie   male 2008           4098.00 5     Adelie female 2009           3334.62 6     Adelie   male 2009           3995.19 7  Chinstrap female 2007           3569.23 8  Chinstrap   male 2007           3819.23 9  Chinstrap female 2008           3472.22 10 Chinstrap   male 2008           4127.78 11 Chinstrap female 2009           3522.92 12 Chinstrap   male 2009           3927.08 13    Gentoo female 2007           4618.75 14    Gentoo   male 2007           5552.94 15    Gentoo female 2008           4627.27 16    Gentoo   male 2008           5410.87 17    Gentoo female 2009           4786.25 18    Gentoo   male 2009           5510.71 penguin_output_schema <- schema(     name = \"penguin_output\",     description = \"Schema for the penguin output\",     average_body_mass = \"number* The average body_mass\",     species = \"string* The species\",     sex = \"string* The sex\",     year = \"integer* The year\" )  anthropic$chat(     \"What's the average body_mass for the Adelie females in 2009?\",     tools = list(\"code_execution\"),     output_schema = penguin_output_schema ) $species [1] \"Adelie\"  $sex [1] \"female\"  $year [1] 2009  $average_body_mass [1] 3334.62 anthropic$delete_file(penguins_file$id)"},{"path":"https://ma-riviere.github.io/argent/articles/anthropic.html","id":"server-side-web-search","dir":"Articles","previous_headings":"","what":"Server-side: Web Search","title":"Anthropic (Claude)","text":"Tip web_search tool supports additional parameters like allowed_domains, exclude_domains, max_uses, user_location. See Anthropic’s documentation details. can call anthropic$get_supplementary() get citations.","code":"anthropic$chat(     \"What's the latest version of the R 'ellmer' package?\",     model = \"claude-haiku-4-5-20251001\",     tools = list(\"web_search\"),     output_schema = package_info_schema ) anthropic$chat(     \"What's the latest version of the R 'ellmer' package?\",     model = \"claude-haiku-4-5-20251001\",     tools = list(         list(             type = \"web_search_20250305\",             name = \"web_search\",             blocked_domains = list(\"rdrr.io\"),             max_uses = 2         )     ),     output_schema = package_info_schema )"},{"path":"https://ma-riviere.github.io/argent/articles/anthropic.html","id":"server-side-web-fetch","dir":"Articles","previous_headings":"","what":"Server-side: Web Fetch","title":"Anthropic (Claude)","text":"server tool lets LLM fetch contents URLs given prompt. web_fetch tool also supports parameters like allowed_domains, exclude_domains, max_uses. See Anthropic’s documentation details. can call anthropic$get_supplementary() get details searches server made.","code":"anthropic$chat(     \"Summarize the main changes in the current development version of the R 'ellmer' package from: https://raw.githubusercontent.com/tidyverse/ellmer/main/NEWS.md\",     model = \"claude-haiku-4-5-20251001\",     tools = list(\"web_fetch\") )"},{"path":"https://ma-riviere.github.io/argent/articles/anthropic.html","id":"server-side-code-execution","dir":"Articles","previous_headings":"","what":"Server-side: Code Execution","title":"Anthropic (Claude)","text":"Anthropic’s code_execution tool provides sandboxed environment executing shell commands file operations. Note Pricing: $0.05/session-hour 5-minute minimum Resources: 5GB RAM/disk, 1 CPU, internet access Workspace: Linked API key (files can downloaded) bash_code_execution - Execute bash commands text_editor_code_execution - Edit files Expiration: 30 days creation Claude automatically determines sub-tool use. Example: Analyzing Penguins Dataset Inspect generated code: Download generated output file /data: Continue asking questions context: Cleanup:","code":"penguins_url <- \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/refs/heads/main/inst/extdata/penguins.csv\" penguins_file <- anthropic$upload_file(penguins_url)  code_exec_tool <- list(type = \"code_execution\", file_ids = list(penguins_file$id))  anthropic$chat(     \"Create a summary table showing average body_mass grouped by species, sex, and year. Save as CSV.\",     model = \"claude-sonnet-4-5-20250929\",     tools = list(code_exec_tool) ) cat(anthropic$get_generated_code(langs = c(\"python\"), as_chunks = TRUE)) import pandas as pd import os  # Read the CSV file input_file = os.path.join(os.environ['INPUT_DIR'], 'fileaa34f5b158276.csv') df = pd.read_csv(input_file)  # Display basic info about the data print(\"Original data shape:\", df.shape) print(\"\\nColumn names:\", df.columns.tolist()) print(\"\\nFirst few rows:\") print(df.head())  # Check for missing values in key columns print(\"\\nMissing values in key columns:\") print(f\"species: {df['species'].isna().sum()}\") print(f\"sex: {df['sex'].isna().sum()}\") print(f\"year: {df['year'].isna().sum()}\") print(f\"body_mass: {df['body_mass'].isna().sum()}\")  # Remove rows where body_mass is NA (can't calculate average for missing values) # Also remove rows where sex is NA if you want clean grouping df_clean = df.dropna(subset=['body_mass', 'sex', 'species', 'year'])  print(f\"\\nData shape after removing rows with missing values: {df_clean.shape}\")  # Group by species, sex, and year, and calculate average body_mass summary = df_clean.groupby(['species', 'sex', 'year'])['body_mass'].mean().reset_index() summary.columns = ['species', 'sex', 'year', 'average_body_mass']  # Round to 2 decimal places for readability summary['average_body_mass'] = summary['average_body_mass'].round(2)  # Sort by species, year, and sex for better readability summary = summary.sort_values(['species', 'year', 'sex']).reset_index(drop=True)  print(\"\\nSummary table:\") print(summary)  # Save to CSV output_file = '/tmp/body_mass_summary.csv' summary.to_csv(output_file, index=False) print(f\"\\nSummary saved to {output_file}\")  # Copy to OUTPUT_DIR for export output_export = os.path.join(os.environ['OUTPUT_DIR'], 'body_mass_summary.csv') summary.to_csv(output_export, index=False) print(f\"Summary exported to {output_export}\") downloaded_path <- anthropic$download_generated_files(dest_path = \"data\") #> ✔ [Anthropic] File downloaded to: data/body_mass_summary.csv read.csv(downloaded_path) species    sex year average_body_mass 1     Adelie female 2007           3389.77 2     Adelie   male 2007           4038.64 3     Adelie female 2008           3386.00 4     Adelie   male 2008           4098.00 5     Adelie female 2009           3334.62 6     Adelie   male 2009           3995.19 7  Chinstrap female 2007           3569.23 8  Chinstrap   male 2007           3819.23 9  Chinstrap female 2008           3472.22 10 Chinstrap   male 2008           4127.78 11 Chinstrap female 2009           3522.92 12 Chinstrap   male 2009           3927.08 13    Gentoo female 2007           4618.75 14    Gentoo   male 2007           5552.94 15    Gentoo female 2008           4627.27 16    Gentoo   male 2008           5410.87 17    Gentoo female 2009           4786.25 18    Gentoo   male 2009           5510.71 penguin_output_schema <- schema(     name = \"penguin_output\",     description = \"Schema for the penguin output\",     average_body_mass = \"number* The average body_mass\",     species = \"string* The species\",     sex = \"string* The sex\",     year = \"integer* The year\" )  anthropic$chat(     \"What's the average body_mass for the Adelie females in 2009?\",     tools = list(\"code_execution\"),     output_schema = penguin_output_schema ) $species [1] \"Adelie\"  $sex [1] \"female\"  $year [1] 2009  $average_body_mass [1] 3334.62 anthropic$delete_file(penguins_file$id)"},{"path":"https://ma-riviere.github.io/argent/articles/anthropic.html","id":"multimodal-inputs","dir":"Articles","previous_headings":"","what":"Multimodal Inputs","title":"Anthropic (Claude)","text":"Anthropic Claude supports sending: PDFs: URLs (-, base64, text content), files (base64, text content) Images: URLs (-, base64), files (base64) Remote files (as_file_content()) Plain text & code files Text-based data files (csv, tsv, json, ..) R objects Sending local image: providing path local image, automatically converted base64 sent server. Sending image URL: Note Image URLs sent -Anthropic. used as_image_content(bsg04_cast_image_url) helper make sure path/url converted base64, , optionally, resize image sending . say ! Sending local PDF: providing prompt path local PDF, automatically converted base64 sent server. Damn right! Sending PDF URLs: Anthropic, default, PDF URLs sent -server. However, can use as_text_content() helper pdftools::pdf_convert() parse PDFs pass text contents model instead. can also send files directly model. Thanks as_file_content() helper, can pass options provider:","code":"bsg04_cast_image_url <- \"https://upload.wikimedia.org/wikipedia/en/1/1a/Battlestar_Galactica_%282004%29_cast.jpg\" bsg04_cast_image_path <- download_temp_file(bsg04_cast_image_url) anthropic$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_path,     model = \"claude-haiku-4-5-20251001\" ) anthropic$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_url,     model = \"claude-haiku-4-5-20251001\" ) This is the cast from **Battlestar Galactica** (2004-2009), the science fiction series that rebooted the original 1978 show.  The main characters visible in this promotional image include:  - **Edward James Olmos** as Admiral William Adama - **Mary McDonnell** as President Laura Roslin - **Katee Sackhoff** as Kara \"Starbuck\" Thrace - **Jamie Bamber** as Lee \"Apollo\" Adama - **Tricia Helfer** as Number Six - **James Callis** as Gaius Baltar - **Grace Park** as Sharon \"Athena\" Agathon  The show follows the last surviving battleship and its fleet of civilian ships as they search for the mythical planet Earth while being hunted by robotic Cylons. It became known for its complex characters, political intrigue, and exploration of themes like identity, religion, and survival. my_cv_url <- \"https://ma-riviere.com/res/cv.pdf\" my_cv_pdf_path <- download_temp_file(my_cv_url) anthropic$chat(     \"What's my favorite programming language?\",     my_cv_pdf_path,     model = \"claude-haiku-4-5-20251001\" ) Based on your resume, your favorite programming language is **R**. r6_pdf_url <- \"https://cran.r-project.org/web/packages/R6/R6.pdf\" s7_pdf_url <- \"https://cran.r-project.org/web/packages/S7/S7.pdf\"  multimodal_prompt <- list(     \"Give a short summary of the pros and cons of the two class systems in the following PDFs:\",     as_text_content(r6_pdf_url),     as_text_content(s7_pdf_url) )  anthropic$chat(!!!multimodal_prompt, model = \"claude-sonnet-4-5-20250929\") cv_metadata <- anthropic$upload_file(my_cv_url)  cv_file_ref <- as_file_content(     cv_metadata$id,     .provider_options = list(         title = \"Some stuff I've done\",          context = \"The `argent` R package is not yet mentionned on it but will soon be.\",          citations = TRUE     ) )  anthropic$chat(     \"What R packages or tools are mentionned in my 'some stuff I've done' document?\",     cv_file_ref,     model = \"claude-haiku-4-5-20251001\" )  anthropic$delete_file(cv_metadata$id)"},{"path":"https://ma-riviere.github.io/argent/articles/anthropic.html","id":"image-comprehension","dir":"Articles","previous_headings":"","what":"Image Comprehension","title":"Anthropic (Claude)","text":"Sending local image: providing path local image, automatically converted base64 sent server. Sending image URL: Note Image URLs sent -Anthropic. used as_image_content(bsg04_cast_image_url) helper make sure path/url converted base64, , optionally, resize image sending . say !","code":"bsg04_cast_image_url <- \"https://upload.wikimedia.org/wikipedia/en/1/1a/Battlestar_Galactica_%282004%29_cast.jpg\" bsg04_cast_image_path <- download_temp_file(bsg04_cast_image_url) anthropic$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_path,     model = \"claude-haiku-4-5-20251001\" ) anthropic$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_url,     model = \"claude-haiku-4-5-20251001\" ) This is the cast from **Battlestar Galactica** (2004-2009), the science fiction series that rebooted the original 1978 show.  The main characters visible in this promotional image include:  - **Edward James Olmos** as Admiral William Adama - **Mary McDonnell** as President Laura Roslin - **Katee Sackhoff** as Kara \"Starbuck\" Thrace - **Jamie Bamber** as Lee \"Apollo\" Adama - **Tricia Helfer** as Number Six - **James Callis** as Gaius Baltar - **Grace Park** as Sharon \"Athena\" Agathon  The show follows the last surviving battleship and its fleet of civilian ships as they search for the mythical planet Earth while being hunted by robotic Cylons. It became known for its complex characters, political intrigue, and exploration of themes like identity, religion, and survival."},{"path":"https://ma-riviere.github.io/argent/articles/anthropic.html","id":"pdf-comprehension","dir":"Articles","previous_headings":"","what":"PDF Comprehension","title":"Anthropic (Claude)","text":"Sending local PDF: providing prompt path local PDF, automatically converted base64 sent server. Damn right! Sending PDF URLs: Anthropic, default, PDF URLs sent -server. However, can use as_text_content() helper pdftools::pdf_convert() parse PDFs pass text contents model instead.","code":"my_cv_url <- \"https://ma-riviere.com/res/cv.pdf\" my_cv_pdf_path <- download_temp_file(my_cv_url) anthropic$chat(     \"What's my favorite programming language?\",     my_cv_pdf_path,     model = \"claude-haiku-4-5-20251001\" ) Based on your resume, your favorite programming language is **R**. r6_pdf_url <- \"https://cran.r-project.org/web/packages/R6/R6.pdf\" s7_pdf_url <- \"https://cran.r-project.org/web/packages/S7/S7.pdf\"  multimodal_prompt <- list(     \"Give a short summary of the pros and cons of the two class systems in the following PDFs:\",     as_text_content(r6_pdf_url),     as_text_content(s7_pdf_url) )  anthropic$chat(!!!multimodal_prompt, model = \"claude-sonnet-4-5-20250929\")"},{"path":"https://ma-riviere.github.io/argent/articles/anthropic.html","id":"sending-files","dir":"Articles","previous_headings":"","what":"Sending files","title":"Anthropic (Claude)","text":"can also send files directly model. Thanks as_file_content() helper, can pass options provider:","code":"cv_metadata <- anthropic$upload_file(my_cv_url)  cv_file_ref <- as_file_content(     cv_metadata$id,     .provider_options = list(         title = \"Some stuff I've done\",          context = \"The `argent` R package is not yet mentionned on it but will soon be.\",          citations = TRUE     ) )  anthropic$chat(     \"What R packages or tools are mentionned in my 'some stuff I've done' document?\",     cv_file_ref,     model = \"claude-haiku-4-5-20251001\" )  anthropic$delete_file(cv_metadata$id)"},{"path":"https://ma-riviere.github.io/argent/articles/anthropic.html","id":"prompt-caching","dir":"Articles","previous_headings":"","what":"Prompt Caching","title":"Anthropic (Claude)","text":"Anthropic supports prompt caching reduce costs latency repeated context. Enable caching cache_prompt, cache_system, cache_tools parameters: can check content indeed cached: Note Cached content stored 5 minutes reused across requests, significantly reducing token costs repeated context. See Anthropic’s prompt caching documentation details.","code":"multimodal_prompt <- list(     \"Give a 3 sentences summary of the advantages of S7 over R6\",     as_text_content(r6_pdf_url),     as_text_content(s7_pdf_url) )  anthropic$chat(!!!multimodal_prompt, model = \"claude-haiku-4-5-20251001\", cache_prompt = TRUE) purrr::pluck(anthropic$get_last_response(), \"usage\") $input_tokens [1] 2  $cache_creation_input_tokens [1] 19770  $cache_read_input_tokens [1] 0  $cache_creation $cache_creation$ephemeral_5m_input_tokens [1] 19770  $cache_creation$ephemeral_1h_input_tokens [1] 0   $output_tokens [1] 140  $service_tier [1] \"standard\""},{"path":"https://ma-riviere.github.io/argent/articles/argent.html","id":"getting-started","dir":"Articles","previous_headings":"","what":"Getting Started","title":"Getting Started with argent","text":"quick showcase various features argent using Google (Gemini) provider. Tip Parallel tool calling available argent, using mirai & purrr::in_parallel(). However, need set daemons using : can customize rate limit initializing rate_limit parameter, default model default_model parameter (‘gemini-2.5-flash’ Google). chat history can visualized printing provider object: First, define web-related tools (search, crawl, fetch, general-use tool) bundled web_tools list: Search: Fetch: , let’s define schema output using schema(): Note Tools schemas automatically converted provider’s specific format internally. Run agent: model use tools provided (searching fetching web pages) repeatedly enough information answer question, return structured JSON output. Providers like Google, Anthropic, OpenAI server-side tools. tools can call without define . run provider’s server. example, Google Gemini server-side google_search tool offer similar capabilities client-side web_search tool. providers support multimodal inputs (degree). can pass text, images, PDFs, data files, URLs, remote files, R objects model single request. Example URL PDF file: Damn right! Note , URL automatically detected, downloaded temporary file, converted base64, passed model. providers may different behavior. example, image PDF’s URL passed -Anthropic’s API. can use as_text_content(url) helper force conversion text content. Tip want include URL want parsed converted (example used url_context server tool), add within text block. can pass R object chat() : Note object captured automatically converted JSON (text JSON conversion fails), added information like name object classes. Upload file reference as_file_content(): Note , using as_file_content() necessary signal model use remote file reference, rather just text content. Tip can use $list_files() list uploaded files metadata. Detailed guides provider: Google Gemini Anthropic Claude OpenRouter Local LLMs Guides OpenAI’s three different APIs: Chat Completions API - Standard OpenAI chat interface Responses API - Newest API combining functionalities Chat Assistants Assistants API - Deprecated Using Compatible APIs - Use argent providers offering compatible APIs (e.g., Minimax, Qwen, …) RAG - use argent & ragnar RAG MCP Servers & Tools - use MCP server tools argent providers (except OpenAI Assistants) support client-side chat history management. main methods interact chat history : get_chat_history() - retrieve chat history (.e. cumulative inputs answers LLM) get_session_history() - retrieve session history (.e. unprocessed API calls responses) reset_history() - clear object’s history load_history() - load history JSON file chat history maintains list messages exchanged user model resent successive API call. Check chat history: See total tokens used last API call: Reset object’s history: default, history automatically saved timestamped JSON files data/history/{provider}/. can: Change directory via argent.history_dir option Disable auto-save setting auto_save_history = FALSE initializing provider object Toggle auto-save initialization via set_auto_save_history(TRUE/FALSE) Check current setting via get_auto_save_history() Note Resetting history dumps current history (non-empty) current JSON history file {argent.history_dir}/{provider}/. Subsequent chats saved new file. neater way visualize chat history print provider object: argent supports structured outputs providers, even without native support, using “forced tool call” mechanism: Define tools using direct specification annotations: Option 1: Define function tool definition separately: Note fn parameter stores function implementation tool definition. important : Closures: Functions capture local variables (e.g., within another function) Consistency: Ensures exact function called, different one name Portability: tool carries implementation, just reference global name fn provided, argent look function name global environment LLM calls . Option 2: 2--1: Define function tool definition one go adding plumber-style annotations: , use tool within chat:","code":"gemini <- Google$new(api_key = Sys.getenv(\"GEMINI_API_KEY\")) mirai::daemons(4) gemini$chat(     \"What is the R programming language? Answer in two sentences.\",     model = \"gemini-2.5-flash\" # Not necessary, it's the default model for Google ) print(gemini) web_search <- function(query) {     #' @description Search the web for information using Tavily API. Returns a JSON array of search results with titles, URLs, and content snippets. Use this when you need current information, facts, news, or any data not in your training data.     #' @param query:string* The search query string. Be specific and use keywords that will yield the most relevant results.          return(web_search_tavily(query)) }  web_search_tavily <- function(query) {     res <- httr2::request(\"https://api.tavily.com/search\") |>          httr2::req_body_json(list(             query = query,             search_depth = \"basic\",             include_answer = FALSE,             max_results = 10,             api_key = Sys.getenv(\"TAVILY_API_KEY\")         )) |>          httr2::req_error(is_error = \\(resp) FALSE) |>          httr2::req_throttle(rate = 20/60, realm = \"tavily\") |>          httr2::req_perform() |>          httr2::resp_body_json() |>          purrr::discard_at(c(\"response_time\", \"follow_up_questions\", \"images\"))      results <- purrr::map(res$results, \\(x) purrr::discard_at(x, \"raw_content\"))      return(jsonlite::toJSON(results, pretty = FALSE, auto_unbox = TRUE)) } web_fetch <- function(url) {     #' @description Fetch and extract the main text content from a web page as clean markdown. Returns the page content with formatting preserved, stripped of navigation, ads, and boilerplate. Use this to read articles, documentation, blog posts, or any web page content.     #' @param url:string* The complete URL of the web page to fetch (e.g., \"https://example.com/article\"). Must be a valid HTTP/HTTPS URL.          trafilatura_installed <- tryCatch({         system(\"which trafilatura\", intern = TRUE, ignore.stderr = TRUE)         return(TRUE)     },     warning = function(e) {         cli::cli_alert_warning(\"trafilatura is not installed. Install with: {.code pip install trafilatura}\")         return(FALSE)     })      if (trafilatura_installed) {         res <- web_fetch_trafilatura(url)          could_not_fetch <- c(             \"Impossible to fetch the contents of this web page\",             \"Please reload this page\",             \"There was an error while loading\",             \"404\"         )         if (is.null(res) || is.na(res) || nchar(res) == 0 ||             any(stringr::str_detect(res, stringr::fixed(could_not_fetch, ignore_case = TRUE)))) {             return(web_fetch_rvest(url))         }         return(res)     }      return(web_fetch_rvest(url)) }  web_fetch_trafilatura <- function(url) {     # pip install trafilatura     tryCatch({         res <- paste0(\"trafilatura -u \", url, \" --markdown --no-comments --links \") |>              system(intern = TRUE) |>              purrr::keep(nzchar) |>             paste0(collapse = \"\\n\")                  return(res)     },     error = function(e) {         return(\"Impossible to fetch the contents of this web page. It might not allow scraping\")     }) }  web_fetch_rvest <- function(url) {     tags_to_ignore <- c(         \"a\", \"script\", \"code\", \"img\", \"svg\", \"footer\", \"g\", \"path\", \"polygon\", \"label\", \"button\", \"form\", \"input\", \"select\",          \"style\", \"link\", \"meta\", \"noscript\", \"iframe\", \"embed\", \"object\", \"param\", \"video\", \"audio\", \"track\", \"source\",          \"canvas\", \"map\", \"area\", \"math\", \"col\", \"colgroup\", \"dl\", \"dt\", \"dd\", \"hr\", \"pre\", \"address\", \"figure\", \"figcaption\",         \"dfn\", \"em\", \"kbd\", \"samp\", \"var\", \"del\", \"ins\", \"mark\", \"circle\"     )      remove_tags <- function(xml, tags) {         purrr::walk(tags, \\(tag) purrr::walk(xml2::xml_find_all(xml, paste0(\".//\", tag)), \\(node) xml2::xml_remove(node)))         return(xml)     }      cleaned_contents <- tryCatch(         rvest::read_html(url)         |> rvest::html_element(\"body\")         |> remove_tags(tags_to_ignore)         |> rvest::html_children()         |> rvest::html_text2()         |> purrr::discard(\\(x) x == \"\")         |> paste0(collapse = \"\\n\\n\"),         error = \\(e) return(\"\")     )     return(cleaned_contents) } package_info_schema <- schema(     name = \"package_info\",     description = \"Information about an R package release\",     release_version = \"string* The release version of the package\",     release_date = \"string* The release date of the `release_version`\" ) gemini$chat(     \"When was the first release of the R 'ellmer' package on GitHub?\",     model = \"gemini-2.5-flash\",     tools = list(as_tool(web_search), as_tool(web_fetch)),     output_schema = package_info_schema ) $release_version [1] \"ellmer_0.1.0\"  $release_date [1] \"2025-01-09\" gemini$chat(     \"When was the last version of the R 'ragnar' package released on GitHub?\",     model = \"gemini-2.5-flash\",     tools = list(\"google_search\"),     output_schema = package_info_schema ) $release_version [1] \"0.2.1\"  $release_date [1] \"August 19, 2025\" gemini$chat(     \"What's my favorite programming language ?\",     \"https://ma-riviere.com/res/cv.pdf\",     model = \"gemini-2.5-flash\" ) Based on your resume, your favorite programming language appears to be **R**. lm_obj <- lm(body_mass ~ species + sex, data = datasets::penguins)  google_gemini$chat(     \"What can we deduct from this regression model ?\",     lm_obj,     model = \"gemini-2.5-flash\" ) From the provided regression model output, we can deduce the following three points:  1.  **Model Type and Purpose**: This is a linear regression model (`lm_obj`) attempting to explain or predict `body_mass` based on two categorical predictor variables: `species` and `sex`. The model was fitted using the `datasets::penguins` dataset. 2.  **Intercept and Reference Group Body Mass**: The estimated intercept is 3372.4. Given that `contr.treatment` was used for both categorical variables, this intercept represents the estimated average `body_mass` (likely in grams) for the baseline group: an **Adelie female penguin**. 3.  **Estimated Effects of Predictors**:     *   **Species Effect**: Compared to Adelie penguins (the reference species), Chinstrap penguins are estimated to have an average `body_mass` that is 26.9 units higher, and Gentoo penguins are estimated to have an average `body_mass` that is 1377.9 units higher, assuming sex is held constant.     *   **Sex Effect**: Compared to female penguins (the reference sex), male penguins are estimated to have an average `body_mass` that is 667.6 units higher, assuming species is held constant. file_metadata <- gemini$upload_file(\"https://ma-riviere.com/res/cv.pdf\")  multipart_prompt <- list(     \"What are my two favorite frameworks/tools ?\",     as_file_content(file_metadata$name) )  gemini$chat(!!!multipart_prompt, model = \"gemini-2.5-flash\")  gemini$delete_file(file_metadata$name) Based on the \"Frameworks & Tools\" section and the overall context of the resume, your two favorite frameworks/tools appear to be:  1.  **Shiny** 2.  **Quarto, R Markdown** google_gemini <- Google$new()  google_gemini$chat(     prompt = \"My name is Alice\",     model = \"gemini-2.5-flash\" ) #> [1] \"Hello Alice! It's nice to meet you. How can I assist you today?\"  google_gemini$chat(     prompt = \"What's my name?\",     model = \"gemini-2.5-flash\" ) #> [1] \"Your name is Alice.\" cat(yaml::as.yaml(google_gemini$get_chat_history())) [   {     \"role\": \"user\",     \"parts\": [       {         \"text\": \"My name is Alice\"       }     ]   },   {     \"parts\": [       {         \"text\": \"Hello Alice! It's nice to meet you. How can I assist you today?\"       }     ],     \"role\": \"model\"   },   {     \"role\": \"user\",     \"parts\": [       {         \"text\": \"What's my name?\"       }     ]   },   {     \"parts\": [       {         \"text\": \"Your name is Alice.\"       }     ],     \"role\": \"model\"   } ] google_gemini$get_session_last_token_count() # Total (input + output) tokens used at last API call google_gemini$get_session_cumulative_token_count() # Cumulative tokens used in this chat session google_gemini$reset_history() options(     argent.history_dir = \"data/history/\"  # Default )  google_gemini <- Google$new(     auto_save_history = FALSE  # Disable automatic saving ) current_history_file_path <- google_gemini$get_history_file_path()  google_gemini <- Google$new() # Equivalent of resetting the provider object  google_gemini$load_history(current_history_file_path) print(google_gemini) # Define schema using direct specification response_schema <- schema(     name = \"response_format\",     description = \"Schema description\",     field1 = \"string* Required string field\",     field2 = \"number Optional numeric field\" )  result <- provider$chat(     \"Your question here\",     output_schema = response_schema ) my_function <- function(arg1) {     return(arg1) }  my_tool <- tool(     name = \"my_function\",     description = \"What the function does\",     arg1 = \"string* Required string argument description\",     fn = my_function  # Store the function with the tool definition ) my_tool my_function <- function(arg1) {     #' @description What the function does     #' @param arg1:string* Required string argument description      return(arg1) } my_tool <- as_tool(my_function)  # Automatically stores the function in .fn gemini$chat(     \"Use the tool to answer this\",     tools = list(my_tool) )"},{"path":"https://ma-riviere.github.io/argent/articles/argent.html","id":"basic-completion","dir":"Articles","previous_headings":"","what":"Basic Completion","title":"Getting Started with argent","text":"chat history can visualized printing provider object:","code":"gemini$chat(     \"What is the R programming language? Answer in two sentences.\",     model = \"gemini-2.5-flash\" # Not necessary, it's the default model for Google ) print(gemini)"},{"path":"https://ma-riviere.github.io/argent/articles/argent.html","id":"tool-calling-structured-output-thinking","dir":"Articles","previous_headings":"","what":"Tool Calling + Structured Output + Thinking","title":"Getting Started with argent","text":"First, define web-related tools (search, crawl, fetch, general-use tool) bundled web_tools list: Search: Fetch: , let’s define schema output using schema(): Note Tools schemas automatically converted provider’s specific format internally. Run agent: model use tools provided (searching fetching web pages) repeatedly enough information answer question, return structured JSON output. Providers like Google, Anthropic, OpenAI server-side tools. tools can call without define . run provider’s server. example, Google Gemini server-side google_search tool offer similar capabilities client-side web_search tool. providers support multimodal inputs (degree). can pass text, images, PDFs, data files, URLs, remote files, R objects model single request. Example URL PDF file: Damn right! Note , URL automatically detected, downloaded temporary file, converted base64, passed model. providers may different behavior. example, image PDF’s URL passed -Anthropic’s API. can use as_text_content(url) helper force conversion text content. Tip want include URL want parsed converted (example used url_context server tool), add within text block. can pass R object chat() : Note object captured automatically converted JSON (text JSON conversion fails), added information like name object classes. Upload file reference as_file_content(): Note , using as_file_content() necessary signal model use remote file reference, rather just text content. Tip can use $list_files() list uploaded files metadata.","code":"web_search <- function(query) {     #' @description Search the web for information using Tavily API. Returns a JSON array of search results with titles, URLs, and content snippets. Use this when you need current information, facts, news, or any data not in your training data.     #' @param query:string* The search query string. Be specific and use keywords that will yield the most relevant results.          return(web_search_tavily(query)) }  web_search_tavily <- function(query) {     res <- httr2::request(\"https://api.tavily.com/search\") |>          httr2::req_body_json(list(             query = query,             search_depth = \"basic\",             include_answer = FALSE,             max_results = 10,             api_key = Sys.getenv(\"TAVILY_API_KEY\")         )) |>          httr2::req_error(is_error = \\(resp) FALSE) |>          httr2::req_throttle(rate = 20/60, realm = \"tavily\") |>          httr2::req_perform() |>          httr2::resp_body_json() |>          purrr::discard_at(c(\"response_time\", \"follow_up_questions\", \"images\"))      results <- purrr::map(res$results, \\(x) purrr::discard_at(x, \"raw_content\"))      return(jsonlite::toJSON(results, pretty = FALSE, auto_unbox = TRUE)) } web_fetch <- function(url) {     #' @description Fetch and extract the main text content from a web page as clean markdown. Returns the page content with formatting preserved, stripped of navigation, ads, and boilerplate. Use this to read articles, documentation, blog posts, or any web page content.     #' @param url:string* The complete URL of the web page to fetch (e.g., \"https://example.com/article\"). Must be a valid HTTP/HTTPS URL.          trafilatura_installed <- tryCatch({         system(\"which trafilatura\", intern = TRUE, ignore.stderr = TRUE)         return(TRUE)     },     warning = function(e) {         cli::cli_alert_warning(\"trafilatura is not installed. Install with: {.code pip install trafilatura}\")         return(FALSE)     })      if (trafilatura_installed) {         res <- web_fetch_trafilatura(url)          could_not_fetch <- c(             \"Impossible to fetch the contents of this web page\",             \"Please reload this page\",             \"There was an error while loading\",             \"404\"         )         if (is.null(res) || is.na(res) || nchar(res) == 0 ||             any(stringr::str_detect(res, stringr::fixed(could_not_fetch, ignore_case = TRUE)))) {             return(web_fetch_rvest(url))         }         return(res)     }      return(web_fetch_rvest(url)) }  web_fetch_trafilatura <- function(url) {     # pip install trafilatura     tryCatch({         res <- paste0(\"trafilatura -u \", url, \" --markdown --no-comments --links \") |>              system(intern = TRUE) |>              purrr::keep(nzchar) |>             paste0(collapse = \"\\n\")                  return(res)     },     error = function(e) {         return(\"Impossible to fetch the contents of this web page. It might not allow scraping\")     }) }  web_fetch_rvest <- function(url) {     tags_to_ignore <- c(         \"a\", \"script\", \"code\", \"img\", \"svg\", \"footer\", \"g\", \"path\", \"polygon\", \"label\", \"button\", \"form\", \"input\", \"select\",          \"style\", \"link\", \"meta\", \"noscript\", \"iframe\", \"embed\", \"object\", \"param\", \"video\", \"audio\", \"track\", \"source\",          \"canvas\", \"map\", \"area\", \"math\", \"col\", \"colgroup\", \"dl\", \"dt\", \"dd\", \"hr\", \"pre\", \"address\", \"figure\", \"figcaption\",         \"dfn\", \"em\", \"kbd\", \"samp\", \"var\", \"del\", \"ins\", \"mark\", \"circle\"     )      remove_tags <- function(xml, tags) {         purrr::walk(tags, \\(tag) purrr::walk(xml2::xml_find_all(xml, paste0(\".//\", tag)), \\(node) xml2::xml_remove(node)))         return(xml)     }      cleaned_contents <- tryCatch(         rvest::read_html(url)         |> rvest::html_element(\"body\")         |> remove_tags(tags_to_ignore)         |> rvest::html_children()         |> rvest::html_text2()         |> purrr::discard(\\(x) x == \"\")         |> paste0(collapse = \"\\n\\n\"),         error = \\(e) return(\"\")     )     return(cleaned_contents) } package_info_schema <- schema(     name = \"package_info\",     description = \"Information about an R package release\",     release_version = \"string* The release version of the package\",     release_date = \"string* The release date of the `release_version`\" ) gemini$chat(     \"When was the first release of the R 'ellmer' package on GitHub?\",     model = \"gemini-2.5-flash\",     tools = list(as_tool(web_search), as_tool(web_fetch)),     output_schema = package_info_schema ) $release_version [1] \"ellmer_0.1.0\"  $release_date [1] \"2025-01-09\" gemini$chat(     \"When was the last version of the R 'ragnar' package released on GitHub?\",     model = \"gemini-2.5-flash\",     tools = list(\"google_search\"),     output_schema = package_info_schema ) $release_version [1] \"0.2.1\"  $release_date [1] \"August 19, 2025\" gemini$chat(     \"What's my favorite programming language ?\",     \"https://ma-riviere.com/res/cv.pdf\",     model = \"gemini-2.5-flash\" ) Based on your resume, your favorite programming language appears to be **R**. lm_obj <- lm(body_mass ~ species + sex, data = datasets::penguins)  google_gemini$chat(     \"What can we deduct from this regression model ?\",     lm_obj,     model = \"gemini-2.5-flash\" ) From the provided regression model output, we can deduce the following three points:  1.  **Model Type and Purpose**: This is a linear regression model (`lm_obj`) attempting to explain or predict `body_mass` based on two categorical predictor variables: `species` and `sex`. The model was fitted using the `datasets::penguins` dataset. 2.  **Intercept and Reference Group Body Mass**: The estimated intercept is 3372.4. Given that `contr.treatment` was used for both categorical variables, this intercept represents the estimated average `body_mass` (likely in grams) for the baseline group: an **Adelie female penguin**. 3.  **Estimated Effects of Predictors**:     *   **Species Effect**: Compared to Adelie penguins (the reference species), Chinstrap penguins are estimated to have an average `body_mass` that is 26.9 units higher, and Gentoo penguins are estimated to have an average `body_mass` that is 1377.9 units higher, assuming sex is held constant.     *   **Sex Effect**: Compared to female penguins (the reference sex), male penguins are estimated to have an average `body_mass` that is 667.6 units higher, assuming species is held constant. file_metadata <- gemini$upload_file(\"https://ma-riviere.com/res/cv.pdf\")  multipart_prompt <- list(     \"What are my two favorite frameworks/tools ?\",     as_file_content(file_metadata$name) )  gemini$chat(!!!multipart_prompt, model = \"gemini-2.5-flash\")  gemini$delete_file(file_metadata$name) Based on the \"Frameworks & Tools\" section and the overall context of the resume, your two favorite frameworks/tools appear to be:  1.  **Shiny** 2.  **Quarto, R Markdown**"},{"path":"https://ma-riviere.github.io/argent/articles/argent.html","id":"server-side-tools","dir":"Articles","previous_headings":"Getting Started","what":"Server-side Tools","title":"Getting Started with argent","text":"Providers like Google, Anthropic, OpenAI server-side tools. tools can call without define . run provider’s server. example, Google Gemini server-side google_search tool offer similar capabilities client-side web_search tool.","code":"gemini$chat(     \"When was the last version of the R 'ragnar' package released on GitHub?\",     model = \"gemini-2.5-flash\",     tools = list(\"google_search\"),     output_schema = package_info_schema ) $release_version [1] \"0.2.1\"  $release_date [1] \"August 19, 2025\""},{"path":"https://ma-riviere.github.io/argent/articles/argent.html","id":"multimodal-input","dir":"Articles","previous_headings":"Getting Started","what":"Multimodal Input","title":"Getting Started with argent","text":"providers support multimodal inputs (degree). can pass text, images, PDFs, data files, URLs, remote files, R objects model single request. Example URL PDF file: Damn right! Note , URL automatically detected, downloaded temporary file, converted base64, passed model. providers may different behavior. example, image PDF’s URL passed -Anthropic’s API. can use as_text_content(url) helper force conversion text content. Tip want include URL want parsed converted (example used url_context server tool), add within text block. can pass R object chat() : Note object captured automatically converted JSON (text JSON conversion fails), added information like name object classes. Upload file reference as_file_content(): Note , using as_file_content() necessary signal model use remote file reference, rather just text content. Tip can use $list_files() list uploaded files metadata.","code":"gemini$chat(     \"What's my favorite programming language ?\",     \"https://ma-riviere.com/res/cv.pdf\",     model = \"gemini-2.5-flash\" ) Based on your resume, your favorite programming language appears to be **R**. lm_obj <- lm(body_mass ~ species + sex, data = datasets::penguins)  google_gemini$chat(     \"What can we deduct from this regression model ?\",     lm_obj,     model = \"gemini-2.5-flash\" ) From the provided regression model output, we can deduce the following three points:  1.  **Model Type and Purpose**: This is a linear regression model (`lm_obj`) attempting to explain or predict `body_mass` based on two categorical predictor variables: `species` and `sex`. The model was fitted using the `datasets::penguins` dataset. 2.  **Intercept and Reference Group Body Mass**: The estimated intercept is 3372.4. Given that `contr.treatment` was used for both categorical variables, this intercept represents the estimated average `body_mass` (likely in grams) for the baseline group: an **Adelie female penguin**. 3.  **Estimated Effects of Predictors**:     *   **Species Effect**: Compared to Adelie penguins (the reference species), Chinstrap penguins are estimated to have an average `body_mass` that is 26.9 units higher, and Gentoo penguins are estimated to have an average `body_mass` that is 1377.9 units higher, assuming sex is held constant.     *   **Sex Effect**: Compared to female penguins (the reference sex), male penguins are estimated to have an average `body_mass` that is 667.6 units higher, assuming species is held constant. file_metadata <- gemini$upload_file(\"https://ma-riviere.com/res/cv.pdf\")  multipart_prompt <- list(     \"What are my two favorite frameworks/tools ?\",     as_file_content(file_metadata$name) )  gemini$chat(!!!multipart_prompt, model = \"gemini-2.5-flash\")  gemini$delete_file(file_metadata$name) Based on the \"Frameworks & Tools\" section and the overall context of the resume, your two favorite frameworks/tools appear to be:  1.  **Shiny** 2.  **Quarto, R Markdown**"},{"path":"https://ma-riviere.github.io/argent/articles/argent.html","id":"passing-files-or-urls","dir":"Articles","previous_headings":"Getting Started","what":"Passing Files or URLs","title":"Getting Started with argent","text":"Example URL PDF file: Damn right! Note , URL automatically detected, downloaded temporary file, converted base64, passed model. providers may different behavior. example, image PDF’s URL passed -Anthropic’s API. can use as_text_content(url) helper force conversion text content. Tip want include URL want parsed converted (example used url_context server tool), add within text block.","code":"gemini$chat(     \"What's my favorite programming language ?\",     \"https://ma-riviere.com/res/cv.pdf\",     model = \"gemini-2.5-flash\" ) Based on your resume, your favorite programming language appears to be **R**."},{"path":"https://ma-riviere.github.io/argent/articles/argent.html","id":"passing-r-objects","dir":"Articles","previous_headings":"Getting Started","what":"Passing R Objects","title":"Getting Started with argent","text":"can pass R object chat() : Note object captured automatically converted JSON (text JSON conversion fails), added information like name object classes.","code":"lm_obj <- lm(body_mass ~ species + sex, data = datasets::penguins)  google_gemini$chat(     \"What can we deduct from this regression model ?\",     lm_obj,     model = \"gemini-2.5-flash\" ) From the provided regression model output, we can deduce the following three points:  1.  **Model Type and Purpose**: This is a linear regression model (`lm_obj`) attempting to explain or predict `body_mass` based on two categorical predictor variables: `species` and `sex`. The model was fitted using the `datasets::penguins` dataset. 2.  **Intercept and Reference Group Body Mass**: The estimated intercept is 3372.4. Given that `contr.treatment` was used for both categorical variables, this intercept represents the estimated average `body_mass` (likely in grams) for the baseline group: an **Adelie female penguin**. 3.  **Estimated Effects of Predictors**:     *   **Species Effect**: Compared to Adelie penguins (the reference species), Chinstrap penguins are estimated to have an average `body_mass` that is 26.9 units higher, and Gentoo penguins are estimated to have an average `body_mass` that is 1377.9 units higher, assuming sex is held constant.     *   **Sex Effect**: Compared to female penguins (the reference sex), male penguins are estimated to have an average `body_mass` that is 667.6 units higher, assuming species is held constant."},{"path":"https://ma-riviere.github.io/argent/articles/argent.html","id":"passing-uploaded-files","dir":"Articles","previous_headings":"Getting Started","what":"Passing Uploaded Files","title":"Getting Started with argent","text":"Upload file reference as_file_content(): Note , using as_file_content() necessary signal model use remote file reference, rather just text content. Tip can use $list_files() list uploaded files metadata.","code":"file_metadata <- gemini$upload_file(\"https://ma-riviere.com/res/cv.pdf\")  multipart_prompt <- list(     \"What are my two favorite frameworks/tools ?\",     as_file_content(file_metadata$name) )  gemini$chat(!!!multipart_prompt, model = \"gemini-2.5-flash\")  gemini$delete_file(file_metadata$name) Based on the \"Frameworks & Tools\" section and the overall context of the resume, your two favorite frameworks/tools appear to be:  1.  **Shiny** 2.  **Quarto, R Markdown**"},{"path":"https://ma-riviere.github.io/argent/articles/argent.html","id":"documentation","dir":"Articles","previous_headings":"","what":"Documentation","title":"Getting Started with argent","text":"Detailed guides provider: Google Gemini Anthropic Claude OpenRouter Local LLMs Guides OpenAI’s three different APIs: Chat Completions API - Standard OpenAI chat interface Responses API - Newest API combining functionalities Chat Assistants Assistants API - Deprecated Using Compatible APIs - Use argent providers offering compatible APIs (e.g., Minimax, Qwen, …) RAG - use argent & ragnar RAG MCP Servers & Tools - use MCP server tools argent","code":""},{"path":"https://ma-riviere.github.io/argent/articles/argent.html","id":"provider-guides","dir":"Articles","previous_headings":"Getting Started","what":"Provider Guides","title":"Getting Started with argent","text":"Detailed guides provider: Google Gemini Anthropic Claude OpenRouter Local LLMs Guides OpenAI’s three different APIs: Chat Completions API - Standard OpenAI chat interface Responses API - Newest API combining functionalities Chat Assistants Assistants API - Deprecated Using Compatible APIs - Use argent providers offering compatible APIs (e.g., Minimax, Qwen, …)","code":""},{"path":"https://ma-riviere.github.io/argent/articles/argent.html","id":"openai-apis","dir":"Articles","previous_headings":"Getting Started","what":"OpenAI APIs","title":"Getting Started with argent","text":"Guides OpenAI’s three different APIs: Chat Completions API - Standard OpenAI chat interface Responses API - Newest API combining functionalities Chat Assistants Assistants API - Deprecated","code":""},{"path":"https://ma-riviere.github.io/argent/articles/argent.html","id":"other-providers","dir":"Articles","previous_headings":"Getting Started","what":"Other Providers","title":"Getting Started with argent","text":"Using Compatible APIs - Use argent providers offering compatible APIs (e.g., Minimax, Qwen, …)","code":""},{"path":"https://ma-riviere.github.io/argent/articles/argent.html","id":"advanced-topics","dir":"Articles","previous_headings":"Getting Started","what":"Advanced Topics","title":"Getting Started with argent","text":"RAG - use argent & ragnar RAG MCP Servers & Tools - use MCP server tools argent","code":""},{"path":[]},{"path":"https://ma-riviere.github.io/argent/articles/argent.html","id":"chat-history-management","dir":"Articles","previous_headings":"","what":"Chat History Management","title":"Getting Started with argent","text":"providers (except OpenAI Assistants) support client-side chat history management. main methods interact chat history : get_chat_history() - retrieve chat history (.e. cumulative inputs answers LLM) get_session_history() - retrieve session history (.e. unprocessed API calls responses) reset_history() - clear object’s history load_history() - load history JSON file chat history maintains list messages exchanged user model resent successive API call. Check chat history: See total tokens used last API call: Reset object’s history: default, history automatically saved timestamped JSON files data/history/{provider}/. can: Change directory via argent.history_dir option Disable auto-save setting auto_save_history = FALSE initializing provider object Toggle auto-save initialization via set_auto_save_history(TRUE/FALSE) Check current setting via get_auto_save_history() Note Resetting history dumps current history (non-empty) current JSON history file {argent.history_dir}/{provider}/. Subsequent chats saved new file. neater way visualize chat history print provider object: argent supports structured outputs providers, even without native support, using “forced tool call” mechanism: Define tools using direct specification annotations: Option 1: Define function tool definition separately: Note fn parameter stores function implementation tool definition. important : Closures: Functions capture local variables (e.g., within another function) Consistency: Ensures exact function called, different one name Portability: tool carries implementation, just reference global name fn provided, argent look function name global environment LLM calls . Option 2: 2--1: Define function tool definition one go adding plumber-style annotations: , use tool within chat:","code":"google_gemini <- Google$new()  google_gemini$chat(     prompt = \"My name is Alice\",     model = \"gemini-2.5-flash\" ) #> [1] \"Hello Alice! It's nice to meet you. How can I assist you today?\"  google_gemini$chat(     prompt = \"What's my name?\",     model = \"gemini-2.5-flash\" ) #> [1] \"Your name is Alice.\" cat(yaml::as.yaml(google_gemini$get_chat_history())) [   {     \"role\": \"user\",     \"parts\": [       {         \"text\": \"My name is Alice\"       }     ]   },   {     \"parts\": [       {         \"text\": \"Hello Alice! It's nice to meet you. How can I assist you today?\"       }     ],     \"role\": \"model\"   },   {     \"role\": \"user\",     \"parts\": [       {         \"text\": \"What's my name?\"       }     ]   },   {     \"parts\": [       {         \"text\": \"Your name is Alice.\"       }     ],     \"role\": \"model\"   } ] google_gemini$get_session_last_token_count() # Total (input + output) tokens used at last API call google_gemini$get_session_cumulative_token_count() # Cumulative tokens used in this chat session google_gemini$reset_history() options(     argent.history_dir = \"data/history/\"  # Default )  google_gemini <- Google$new(     auto_save_history = FALSE  # Disable automatic saving ) current_history_file_path <- google_gemini$get_history_file_path()  google_gemini <- Google$new() # Equivalent of resetting the provider object  google_gemini$load_history(current_history_file_path) print(google_gemini) # Define schema using direct specification response_schema <- schema(     name = \"response_format\",     description = \"Schema description\",     field1 = \"string* Required string field\",     field2 = \"number Optional numeric field\" )  result <- provider$chat(     \"Your question here\",     output_schema = response_schema ) my_function <- function(arg1) {     return(arg1) }  my_tool <- tool(     name = \"my_function\",     description = \"What the function does\",     arg1 = \"string* Required string argument description\",     fn = my_function  # Store the function with the tool definition ) my_tool my_function <- function(arg1) {     #' @description What the function does     #' @param arg1:string* Required string argument description      return(arg1) } my_tool <- as_tool(my_function)  # Automatically stores the function in .fn gemini$chat(     \"Use the tool to answer this\",     tools = list(my_tool) )"},{"path":"https://ma-riviere.github.io/argent/articles/argent.html","id":"automatic-history-persistence","dir":"Articles","previous_headings":"Getting Started","what":"Automatic History Persistence","title":"Getting Started with argent","text":"default, history automatically saved timestamped JSON files data/history/{provider}/. can: Change directory via argent.history_dir option Disable auto-save setting auto_save_history = FALSE initializing provider object Toggle auto-save initialization via set_auto_save_history(TRUE/FALSE) Check current setting via get_auto_save_history() Note Resetting history dumps current history (non-empty) current JSON history file {argent.history_dir}/{provider}/. Subsequent chats saved new file.","code":"options(     argent.history_dir = \"data/history/\"  # Default )  google_gemini <- Google$new(     auto_save_history = FALSE  # Disable automatic saving )"},{"path":"https://ma-riviere.github.io/argent/articles/argent.html","id":"loading-previous-conversations","dir":"Articles","previous_headings":"Getting Started","what":"Loading Previous Conversations","title":"Getting Started with argent","text":"neater way visualize chat history print provider object:","code":"current_history_file_path <- google_gemini$get_history_file_path()  google_gemini <- Google$new() # Equivalent of resetting the provider object  google_gemini$load_history(current_history_file_path) print(google_gemini)"},{"path":"https://ma-riviere.github.io/argent/articles/argent.html","id":"structured-outputs","dir":"Articles","previous_headings":"Getting Started","what":"Structured Outputs","title":"Getting Started with argent","text":"argent supports structured outputs providers, even without native support, using “forced tool call” mechanism:","code":"# Define schema using direct specification response_schema <- schema(     name = \"response_format\",     description = \"Schema description\",     field1 = \"string* Required string field\",     field2 = \"number Optional numeric field\" )  result <- provider$chat(     \"Your question here\",     output_schema = response_schema )"},{"path":"https://ma-riviere.github.io/argent/articles/argent.html","id":"tool-calling","dir":"Articles","previous_headings":"Getting Started","what":"Tool Calling","title":"Getting Started with argent","text":"Define tools using direct specification annotations: Option 1: Define function tool definition separately: Note fn parameter stores function implementation tool definition. important : Closures: Functions capture local variables (e.g., within another function) Consistency: Ensures exact function called, different one name Portability: tool carries implementation, just reference global name fn provided, argent look function name global environment LLM calls . Option 2: 2--1: Define function tool definition one go adding plumber-style annotations: , use tool within chat:","code":"my_function <- function(arg1) {     return(arg1) }  my_tool <- tool(     name = \"my_function\",     description = \"What the function does\",     arg1 = \"string* Required string argument description\",     fn = my_function  # Store the function with the tool definition ) my_tool my_function <- function(arg1) {     #' @description What the function does     #' @param arg1:string* Required string argument description      return(arg1) } my_tool <- as_tool(my_function)  # Automatically stores the function in .fn gemini$chat(     \"Use the tool to answer this\",     tools = list(my_tool) )"},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Google (Gemini)","text":"article covers using Google Gemini provider argent. Google Gemini offers powerful multimodal capabilities, server-side tools, extended thinking features.","code":""},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Google (Gemini)","text":"","code":"google_gemini <- Google$new(api_key = Sys.getenv(\"GEMINI_API_KEY\")) mirai::daemons(4) # To enable parallel tool calling"},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"discovering-models","dir":"Articles","previous_headings":"","what":"Discovering Models","title":"Google (Gemini)","text":"","code":"google_gemini$list_models() |>     dplyr::filter(stringr::str_detect(name, \"2.5-flash|2.5-pro|3-pro\")) |>     dplyr::select(-description)"},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"basic-completion","dir":"Articles","previous_headings":"","what":"Basic Completion","title":"Google (Gemini)","text":"","code":"google_gemini$chat(     \"What's the R programming language? Answer in three sentences.\",     model = \"gemini-2.5-flash\" # Default model for Google )"},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"tool-calling-structured-output-thinking","dir":"Articles","previous_headings":"","what":"Tool Calling + Structured Output + Thinking","title":"Google (Gemini)","text":"First, define web-related tools (search, crawl, fetch, general-use tool) bundled web_tools list: Search: Fetch: , let’s define JSON schema structured output using schema(): , run agent: model keep calling tools enough information answer question. Alternatively, simply print(google_gemini) see reasoning answers’ text console, turn turn. Note get_reasoning_text() get_content_text() use last API response (google_gemini$get_last_response()) default.","code":"web_search <- function(query) {     #' @description Search the web for information using Tavily API. Returns a JSON array of search results with titles, URLs, and content snippets. Use this when you need current information, facts, news, or any data not in your training data.     #' @param query:string* The search query string. Be specific and use keywords that will yield the most relevant results.          return(web_search_tavily(query)) }  web_search_tavily <- function(query) {     res <- httr2::request(\"https://api.tavily.com/search\") |>          httr2::req_body_json(list(             query = query,             search_depth = \"basic\",             include_answer = FALSE,             max_results = 10,             api_key = Sys.getenv(\"TAVILY_API_KEY\")         )) |>          httr2::req_error(is_error = \\(resp) FALSE) |>          httr2::req_throttle(rate = 20/60, realm = \"tavily\") |>          httr2::req_perform() |>          httr2::resp_body_json() |>          purrr::discard_at(c(\"response_time\", \"follow_up_questions\", \"images\"))      results <- purrr::map(res$results, \\(x) purrr::discard_at(x, \"raw_content\"))      return(jsonlite::toJSON(results, pretty = FALSE, auto_unbox = TRUE)) } web_fetch <- function(url) {     #' @description Fetch and extract the main text content from a web page as clean markdown. Returns the page content with formatting preserved, stripped of navigation, ads, and boilerplate. Use this to read articles, documentation, blog posts, or any web page content.     #' @param url:string* The complete URL of the web page to fetch (e.g., \"https://example.com/article\"). Must be a valid HTTP/HTTPS URL.          trafilatura_installed <- tryCatch({         system(\"which trafilatura\", intern = TRUE, ignore.stderr = TRUE)         return(TRUE)     },     warning = function(e) {         cli::cli_alert_warning(\"trafilatura is not installed. Install with: {.code pip install trafilatura}\")         return(FALSE)     })      if (trafilatura_installed) {         res <- web_fetch_trafilatura(url)          could_not_fetch <- c(             \"Impossible to fetch the contents of this web page\",             \"Please reload this page\",             \"There was an error while loading\",             \"404\"         )         if (is.null(res) || is.na(res) || nchar(res) == 0 ||             any(stringr::str_detect(res, stringr::fixed(could_not_fetch, ignore_case = TRUE)))) {             return(web_fetch_rvest(url))         }         return(res)     }      return(web_fetch_rvest(url)) }  web_fetch_trafilatura <- function(url) {     # pip install trafilatura     tryCatch({         res <- paste0(\"trafilatura -u \", url, \" --markdown --no-comments --links \") |>              system(intern = TRUE) |>              purrr::keep(nzchar) |>             paste0(collapse = \"\\n\")                  return(res)     },     error = function(e) {         return(\"Impossible to fetch the contents of this web page. It might not allow scraping\")     }) }  web_fetch_rvest <- function(url) {     tags_to_ignore <- c(         \"a\", \"script\", \"code\", \"img\", \"svg\", \"footer\", \"g\", \"path\", \"polygon\", \"label\", \"button\", \"form\", \"input\", \"select\",          \"style\", \"link\", \"meta\", \"noscript\", \"iframe\", \"embed\", \"object\", \"param\", \"video\", \"audio\", \"track\", \"source\",          \"canvas\", \"map\", \"area\", \"math\", \"col\", \"colgroup\", \"dl\", \"dt\", \"dd\", \"hr\", \"pre\", \"address\", \"figure\", \"figcaption\",         \"dfn\", \"em\", \"kbd\", \"samp\", \"var\", \"del\", \"ins\", \"mark\", \"circle\"     )      remove_tags <- function(xml, tags) {         purrr::walk(tags, \\(tag) purrr::walk(xml2::xml_find_all(xml, paste0(\".//\", tag)), \\(node) xml2::xml_remove(node)))         return(xml)     }      cleaned_contents <- tryCatch(         rvest::read_html(url)         |> rvest::html_element(\"body\")         |> remove_tags(tags_to_ignore)         |> rvest::html_children()         |> rvest::html_text2()         |> purrr::discard(\\(x) x == \"\")         |> paste0(collapse = \"\\n\\n\"),         error = \\(e) return(\"\")     )     return(cleaned_contents) } package_info_schema <- schema(     name = \"package_info\",     description = \"Information about an R package release\",     release_version = \"string* The release version of the package\",     release_date = \"string* The release date of the `release_version`\" ) google_gemini$chat(     \"When was the first release of the R 'ellmer' package on GitHub?\",     model = \"gemini-2.5-flash\",     thinking_budget = 512,     include_thoughts = TRUE,     tools = list(as_tool(web_search), as_tool(web_fetch)),     output_schema = package_info_schema ) $release_version [1] \"0.1.0\"  $release_date [1] \"2025-01-09\" cat(google_gemini$get_reasoning_text())"},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"extracting-reasoning","dir":"Articles","previous_headings":"","what":"Extracting Reasoning","title":"Google (Gemini)","text":"Alternatively, simply print(google_gemini) see reasoning answers’ text console, turn turn. Note get_reasoning_text() get_content_text() use last API response (google_gemini$get_last_response()) default.","code":"cat(google_gemini$get_reasoning_text())"},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"server-side-tools","dir":"Articles","previous_headings":"","what":"Server-side Tools","title":"Google (Gemini)","text":"Server-side tools tools can call without define . run provider’s server. Google Gemini supports five server-side tools can used alongside client-side tools: google_search - Web search grounding google_maps - Location-aware information Google Maps data url_context - Fetch process URLs code_execution - Execute Python code file_search - Search uploaded files using vector stores (.e. server-side RAG) server-side web search tool provides search results citations: Extract grounding metadata (citations sources): Note server-side tool equivalent client-side web_search tool. server tool lets LLM fetch contents URLs given prompt. supports fetching text (HTML, JSON, XML), images (PNG, JPEG, WebP), PDFs. Limitations: Maximum 20 URLs per request 34MB limit per URL Retrieved content counts toward input token usage Note server-side tool equivalent client-side web_fetch tool. Google Gemini, can create file search store use file_search tool search uploaded files, effectively turning server-side RAG application. See File Search & RAG section complete example. Google Maps grounding tool connects Gemini Google Maps’ rich, factual, --date location data. enables accurate, location-aware responses geographical queries. can also provide specific location relevant results: Extract grounding metadata (citations sources): Enabling Maps Widget Enable widget context token render interactive Google Maps widgets using Maps API: Extract widget context token: widget token can passed Google Maps API display interactive maps markers frontend. Note works specific Gemini models (2.5 Pro, 2.5 Flash, 2.5 Flash-Lite, 2.0 Flash) Free tier: 500 successful requests per day 25$ per 1,000 grounded prompts code execution tool allows model write execute Python code server-side sandboxed environment data analysis, calculations, visualizations. Example: Analyzing Penguins Dataset Let’s upload penguins dataset remote file use code execution tool: Inspect generated code: Download generated files: Continue asking questions context: Google Gemini supports sending: PDF (files URLs) text base64 Images (files URLs) base64 Remote files (as_file_content()) Plain text & code files Text-based data files (csv, tsv, json, ..) R objects Sending local image: providing path local image, automatically converted base64 sent server. say ! Sending image URL: Note , URL automatically detected, downloaded temporary file, converted base64, passed model. providers may different behavior. example, Anthropic supports passing image files URLs directly. Tip used as_image_content(bsg04_cast_image_url) helper make sure path/url treated image, , optionally, resize image sending . Sending local PDF: Damn right! Sending PDF URLs: Google, default, PDF URLs sent base64 server. However, can use as_text_content() helper pdftools::pdf_convert() parse PDFs pass text contents model instead. can pass R object chat() : Note R object passed chat() automatically converted JSON (text JSON conversion fails), added information like name object classes. Upload file reference as_file_content(): Note , using as_file_content() necessary signal model use remote file reference, rather just text content. Tip can use $list_files() list uploaded files metadata.","code":"google_gemini$chat(     \"What's the latest version of the R 'ellmer' package?\",     model = \"gemini-2.5-flash\",     tools = list(\"google_search\"),     output_schema = package_info_schema ) google_gemini$get_supplementary() # Defaults to the last response google_gemini$chat(     \"Summarize the main changes in the current development version of the R 'ellmer' package in 2 sentences: https://raw.githubusercontent.com/tidyverse/ellmer/main/NEWS.md\",     model = \"gemini-2.5-flash\",     tools = list(\"url_context\") ) The current development version of the R 'ellmer' package introduces several significant changes, including enhanced control over model reasoning effort with new `reasoning_effort` and `reasoning_tokens` parameters for various chat functions. Additionally, it refines API key handling by using a `credentials` function to retrieve keys on demand, preventing their storage in chat objects, and supports image generation through `chat_google_gemini()` and `chat_openai_responses()`. google_gemini$chat(     \"What are the 5 best Italian restaurants within a 15-minute walk from Times Square?\",     model = \"gemini-2.5-flash\",     tools = list(\"google_maps\") ) google_gemini$chat(     \"Give me 5 good coffee shops with outdoor seating in a 1km radius from here.\",     model = \"gemini-2.5-flash\",     tools = list(         list(             type = \"google_maps\",             location = list(                 latitude = 63.43047233964285,                 longitude = -10.394945036790627             )         )     ) ) Here are 5 good coffee shops with outdoor seating near you:  1.  **Jacobsen & Svart Coffee Roasters** - This coffee shop has a rating of 4.7 stars and is currently open until 5:00 PM today. 2.  **Tantes Hage** - With a rating of 4.8 stars, this cafe offers outdoor seating. Please note that Tantes Hage is closed today, but open on Sundays from 10:00 AM to 6:00 PM. 3.  **Café Løkka** - This cafe has a 4.3-star rating and is currently open until 12:00 AM. 4.  **Dromedar Kaffebar, Nordre** - Rated 4.3 stars, this coffee shop is open until 7:00 PM today. 5.  **Espresso House** (Nordre gate 1-3) - This cafe has a rating of 4.2 stars and is currently open until 8:00 PM. google_gemini$get_supplementary() google_gemini$chat(     \"Plan a day in Trondheim. I want to see the Nidaros, swim in a lake, and have a beer by the river.\",     model = \"gemini-2.5-flash\",     tools = list(         list(             type = \"google_maps\",             enable_widget = TRUE,             location = list(                 latitude = 63.43047233964285,                 longitude = -10.394945036790627             )         )     ) ) Here's a possible plan for your day in Trondheim:  Start your day by visiting the magnificent Nidaros Cathedral, a Gothic cathedral dating from the 13th century located at Kongsgårdsgata 2. The cathedral typically opens at 9:00 AM on weekdays.  After exploring the cathedral, head to Theisendammen badeplass for a refreshing swim in a lake. This park is open 24 hours, highly-rated, and is a good option for swimming.  In the evening, enjoy a beer by the Nidelva river at Den Gode Nabo AS. This highly-rated bar, located at Øvre Bakklandet 66, offers outdoor seating, perfect for enjoying the riverside atmosphere. google_gemini$get_supplementary() |>     purrr::pluck(\"grounding_metadata\", \"googleMapsWidgetContextToken\") penguins_url <- \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/refs/heads/main/inst/extdata/penguins.csv\" penguins_file_metadata <- google_gemini$upload_file(penguins_url)  google_gemini$chat(     \"Create a summary table showing average body_mass grouped by species, sex, and year. Save as CSV.\",     model = \"gemini-2.5-flash\",     tools = list(list(type = \"code_execution\", file_ids = list(penguins_file_metadata$name))) ) cat(google_gemini$get_generated_code(langs = c(\"python\"), as_chunks = TRUE)) import pandas as pd  # Load the CSV file df = pd.read_csv(\"input_file_0.csv\")  # Display the first few rows and information about the DataFrame print(\"DataFrame Head:\") print(df.head()) print(\"\\nDataFrame Info:\") print(df.info()) # Group by species, sex, and year and calculate the average body_mass_g summary_table = df.groupby(['species', 'sex', 'year'])['body_mass_g'].mean().reset_index()  # Rename the 'body_mass_g' column to 'average_body_mass_g' for clarity summary_table = summary_table.rename(columns={'body_mass_g': 'average_body_mass_g'})  # Save the summary table to a CSV file output_filename = \"average_body_mass_summary.csv\" summary_table.to_csv(output_filename, index=False)  print(f\"Summary table saved to {output_filename}\") print(\"\\nSummary Table Head:\") print(summary_table.head()) downloaded_path <- google_gemini$download_generated_files(dest_path = \"data\") #> ✔ File saved: data/generated_file_9604db3d-57fc-4aab-bd60-bf4c194ab6e5.csv read.csv(downloaded_path) species    sex year average_body_mass_g 1     Adelie female 2007            3389.773 2     Adelie female 2008            3386.000 3     Adelie female 2009            3334.615 4     Adelie   male 2007            4038.636 5     Adelie   male 2008            4098.000 6     Adelie   male 2009            3995.192 7  Chinstrap female 2007            3569.231 8  Chinstrap female 2008            3472.222 9  Chinstrap female 2009            3522.917 10 Chinstrap   male 2007            3819.231 11 Chinstrap   male 2008            4127.778 12 Chinstrap   male 2009            3927.083 13    Gentoo female 2007            4618.750 14    Gentoo female 2008            4627.273 15    Gentoo female 2009            4786.250 16    Gentoo   male 2007            5552.941 17    Gentoo   male 2008            5410.870 18    Gentoo   male 2009            5510.714 penguin_output_schema <- schema(     name = \"penguin_output\",     description = \"Schema for the penguin output\",     average_body_mass = \"number* The average body_mass\",     species = \"string* The species\",     sex = \"string* The sex\",     year = \"integer* The year\" )  google_gemini$chat(     \"What's the average body_mass for the Adelie females in 2009?\",     tools = list(\"code_execution\"),     output_schema = penguin_output_schema ) $species [1] \"Adelie\"  $sex [1] \"females\"  $year [1] 2009  $average_body_mass [1] 3334.62 bsg04_cast_image_url <- \"https://upload.wikimedia.org/wikipedia/en/1/1a/Battlestar_Galactica_%282004%29_cast.jpg\" bsg04_cast_image_path <- download_temp_file(bsg04_cast_image_url) google_gemini$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_path ) The characters in the image are from the show **Battlestar Galactica**.  From left to right, the characters are: *   **Admiral William Adama** (played by Edward James Olmos) *   **President Laura Roslin** (played by Mary McDonnell) *   **Captain Lee \"Apollo\" Adama** (played by Jamie Bamber) *   **Lieutenant Kara \"Starbuck\" Thrace** (played by Katee Sackhoff) *   **Number Six** (played by Tricia Helfer) *   **Gaius Baltar** (played by James Callis) *   **Sharon \"Boomer\" Valerii / Sharon \"Athena\" Agathon** (played by Grace Park) google_gemini$reset_history()  google_gemini$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_url ) my_cv_url <- \"https://ma-riviere.com/res/cv.pdf\" my_cv_pdf_path <- download_temp_file(my_cv_url) google_gemini$chat(     \"What's my favorite programming language ?\",     my_cv_pdf_path ) Based on your resume, your favorite programming language appears to be **R**. r6_pdf_url <- \"https://cran.r-project.org/web/packages/R6/R6.pdf\" s7_pdf_url <- \"https://cran.r-project.org/web/packages/S7/S7.pdf\"  multimodal_prompt <- list(     \"Give a 3 sentences summary of the advantages of S7 over R6\",     as_text_content(r6_pdf_url),     as_text_content(s7_pdf_url),     \"And give me the current versions of both packages\" # To make sure you actually read the PDFs )  google_gemini$chat(!!!multimodal_prompt) S7 offers a more formal and comprehensive object-oriented system for R, explicitly designed as a successor to both S3 and S4, featuring formal class, generic, and method specifications. It supports limited multiple dispatch, allowing method selection based on the types of multiple arguments, a capability not natively offered by R6. Furthermore, S7 is the result of a collaborative effort by the R Consortium Object-Oriented Programming Working Group, suggesting a broader community consensus and potential for standardization in R's future.  The current versions of the packages are: *   **R6:** 2.6.1 (published 2025-02-15) *   **S7:** 0.2.0 (published 2024-11-07) lm_obj <- lm(body_mass ~ species + sex, data = datasets::penguins)  google_gemini$chat(     \"What can we deduct from this regression model ?\",     lm_obj,     model = \"gemini-2.5-flash\" ) From the provided regression model output, we can deduce the following three points:  1.  **Model Type and Purpose**: This is a linear regression model (`lm_obj`) attempting to explain or predict `body_mass` based on two categorical predictor variables: `species` and `sex`. The model was fitted using the `datasets::penguins` dataset. 2.  **Intercept and Reference Group Body Mass**: The estimated intercept is 3372.4. Given that `contr.treatment` was used for both categorical variables, this intercept represents the estimated average `body_mass` (likely in grams) for the baseline group: an **Adelie female penguin**. 3.  **Estimated Effects of Predictors**:     *   **Species Effect**: Compared to Adelie penguins (the reference species), Chinstrap penguins are estimated to have an average `body_mass` that is 26.9 units higher, and Gentoo penguins are estimated to have an average `body_mass` that is 1377.9 units higher, assuming sex is held constant.     *   **Sex Effect**: Compared to female penguins (the reference sex), male penguins are estimated to have an average `body_mass` that is 667.6 units higher, assuming species is held constant. file_metadata <- google_gemini$upload_file(my_cv_url) # Pass an URL or a local file path  multipart_prompt <- list(     \"What are my two favorite frameworks/tools ?\",     as_file_content(file_metadata$name) )  google_gemini$chat(!!!multipart_prompt, model = \"gemini-2.5-flash\")  google_gemini$delete_file(file_metadata$name) Based on your resume, your two favorite frameworks/tools appear to be **Shiny** and **Quarto/R Markdown**.  Here's why: *   Your most recent role is \"R/Shiny Developer,\" highlighting Shiny as a primary tool. *   Shiny is listed first and prominently in your \"Frameworks & Tools\" section. *   \"Quarto, R Markdown\" is also listed prominently and is explicitly mentioned in your \"Consulting\" section, and is a natural pairing with R and Shiny for scientific publishing and interactive content."},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"server-side-google-search","dir":"Articles","previous_headings":"","what":"Server-side: Google Search","title":"Google (Gemini)","text":"server-side web search tool provides search results citations: Extract grounding metadata (citations sources): Note server-side tool equivalent client-side web_search tool.","code":"google_gemini$chat(     \"What's the latest version of the R 'ellmer' package?\",     model = \"gemini-2.5-flash\",     tools = list(\"google_search\"),     output_schema = package_info_schema ) google_gemini$get_supplementary() # Defaults to the last response"},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"server-side-url-context","dir":"Articles","previous_headings":"","what":"Server-side: URL Context","title":"Google (Gemini)","text":"server tool lets LLM fetch contents URLs given prompt. supports fetching text (HTML, JSON, XML), images (PNG, JPEG, WebP), PDFs. Limitations: Maximum 20 URLs per request 34MB limit per URL Retrieved content counts toward input token usage Note server-side tool equivalent client-side web_fetch tool.","code":"google_gemini$chat(     \"Summarize the main changes in the current development version of the R 'ellmer' package in 2 sentences: https://raw.githubusercontent.com/tidyverse/ellmer/main/NEWS.md\",     model = \"gemini-2.5-flash\",     tools = list(\"url_context\") ) The current development version of the R 'ellmer' package introduces several significant changes, including enhanced control over model reasoning effort with new `reasoning_effort` and `reasoning_tokens` parameters for various chat functions. Additionally, it refines API key handling by using a `credentials` function to retrieve keys on demand, preventing their storage in chat objects, and supports image generation through `chat_google_gemini()` and `chat_openai_responses()`."},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"server-side-file-search","dir":"Articles","previous_headings":"","what":"Server-side: File Search","title":"Google (Gemini)","text":"Google Gemini, can create file search store use file_search tool search uploaded files, effectively turning server-side RAG application. See File Search & RAG section complete example.","code":""},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"server-side-google-maps","dir":"Articles","previous_headings":"","what":"Server-side: Google Maps","title":"Google (Gemini)","text":"Google Maps grounding tool connects Gemini Google Maps’ rich, factual, --date location data. enables accurate, location-aware responses geographical queries. can also provide specific location relevant results: Extract grounding metadata (citations sources): Enabling Maps Widget Enable widget context token render interactive Google Maps widgets using Maps API: Extract widget context token: widget token can passed Google Maps API display interactive maps markers frontend. Note works specific Gemini models (2.5 Pro, 2.5 Flash, 2.5 Flash-Lite, 2.0 Flash) Free tier: 500 successful requests per day 25$ per 1,000 grounded prompts","code":"google_gemini$chat(     \"What are the 5 best Italian restaurants within a 15-minute walk from Times Square?\",     model = \"gemini-2.5-flash\",     tools = list(\"google_maps\") ) google_gemini$chat(     \"Give me 5 good coffee shops with outdoor seating in a 1km radius from here.\",     model = \"gemini-2.5-flash\",     tools = list(         list(             type = \"google_maps\",             location = list(                 latitude = 63.43047233964285,                 longitude = -10.394945036790627             )         )     ) ) Here are 5 good coffee shops with outdoor seating near you:  1.  **Jacobsen & Svart Coffee Roasters** - This coffee shop has a rating of 4.7 stars and is currently open until 5:00 PM today. 2.  **Tantes Hage** - With a rating of 4.8 stars, this cafe offers outdoor seating. Please note that Tantes Hage is closed today, but open on Sundays from 10:00 AM to 6:00 PM. 3.  **Café Løkka** - This cafe has a 4.3-star rating and is currently open until 12:00 AM. 4.  **Dromedar Kaffebar, Nordre** - Rated 4.3 stars, this coffee shop is open until 7:00 PM today. 5.  **Espresso House** (Nordre gate 1-3) - This cafe has a rating of 4.2 stars and is currently open until 8:00 PM. google_gemini$get_supplementary() google_gemini$chat(     \"Plan a day in Trondheim. I want to see the Nidaros, swim in a lake, and have a beer by the river.\",     model = \"gemini-2.5-flash\",     tools = list(         list(             type = \"google_maps\",             enable_widget = TRUE,             location = list(                 latitude = 63.43047233964285,                 longitude = -10.394945036790627             )         )     ) ) Here's a possible plan for your day in Trondheim:  Start your day by visiting the magnificent Nidaros Cathedral, a Gothic cathedral dating from the 13th century located at Kongsgårdsgata 2. The cathedral typically opens at 9:00 AM on weekdays.  After exploring the cathedral, head to Theisendammen badeplass for a refreshing swim in a lake. This park is open 24 hours, highly-rated, and is a good option for swimming.  In the evening, enjoy a beer by the Nidelva river at Den Gode Nabo AS. This highly-rated bar, located at Øvre Bakklandet 66, offers outdoor seating, perfect for enjoying the riverside atmosphere. google_gemini$get_supplementary() |>     purrr::pluck(\"grounding_metadata\", \"googleMapsWidgetContextToken\")"},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"basic-usage","dir":"Articles","previous_headings":"Server-side Tools","what":"Basic Usage","title":"Google (Gemini)","text":"can also provide specific location relevant results: Extract grounding metadata (citations sources): Enabling Maps Widget Enable widget context token render interactive Google Maps widgets using Maps API: Extract widget context token: widget token can passed Google Maps API display interactive maps markers frontend. Note works specific Gemini models (2.5 Pro, 2.5 Flash, 2.5 Flash-Lite, 2.0 Flash) Free tier: 500 successful requests per day 25$ per 1,000 grounded prompts","code":"google_gemini$chat(     \"What are the 5 best Italian restaurants within a 15-minute walk from Times Square?\",     model = \"gemini-2.5-flash\",     tools = list(\"google_maps\") ) google_gemini$chat(     \"Give me 5 good coffee shops with outdoor seating in a 1km radius from here.\",     model = \"gemini-2.5-flash\",     tools = list(         list(             type = \"google_maps\",             location = list(                 latitude = 63.43047233964285,                 longitude = -10.394945036790627             )         )     ) ) Here are 5 good coffee shops with outdoor seating near you:  1.  **Jacobsen & Svart Coffee Roasters** - This coffee shop has a rating of 4.7 stars and is currently open until 5:00 PM today. 2.  **Tantes Hage** - With a rating of 4.8 stars, this cafe offers outdoor seating. Please note that Tantes Hage is closed today, but open on Sundays from 10:00 AM to 6:00 PM. 3.  **Café Løkka** - This cafe has a 4.3-star rating and is currently open until 12:00 AM. 4.  **Dromedar Kaffebar, Nordre** - Rated 4.3 stars, this coffee shop is open until 7:00 PM today. 5.  **Espresso House** (Nordre gate 1-3) - This cafe has a rating of 4.2 stars and is currently open until 8:00 PM. google_gemini$get_supplementary() google_gemini$chat(     \"Plan a day in Trondheim. I want to see the Nidaros, swim in a lake, and have a beer by the river.\",     model = \"gemini-2.5-flash\",     tools = list(         list(             type = \"google_maps\",             enable_widget = TRUE,             location = list(                 latitude = 63.43047233964285,                 longitude = -10.394945036790627             )         )     ) ) Here's a possible plan for your day in Trondheim:  Start your day by visiting the magnificent Nidaros Cathedral, a Gothic cathedral dating from the 13th century located at Kongsgårdsgata 2. The cathedral typically opens at 9:00 AM on weekdays.  After exploring the cathedral, head to Theisendammen badeplass for a refreshing swim in a lake. This park is open 24 hours, highly-rated, and is a good option for swimming.  In the evening, enjoy a beer by the Nidelva river at Den Gode Nabo AS. This highly-rated bar, located at Øvre Bakklandet 66, offers outdoor seating, perfect for enjoying the riverside atmosphere. google_gemini$get_supplementary() |>     purrr::pluck(\"grounding_metadata\", \"googleMapsWidgetContextToken\")"},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"server-side-code-execution","dir":"Articles","previous_headings":"","what":"Server-side: Code Execution","title":"Google (Gemini)","text":"code execution tool allows model write execute Python code server-side sandboxed environment data analysis, calculations, visualizations. Example: Analyzing Penguins Dataset Let’s upload penguins dataset remote file use code execution tool: Inspect generated code: Download generated files: Continue asking questions context:","code":"penguins_url <- \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/refs/heads/main/inst/extdata/penguins.csv\" penguins_file_metadata <- google_gemini$upload_file(penguins_url)  google_gemini$chat(     \"Create a summary table showing average body_mass grouped by species, sex, and year. Save as CSV.\",     model = \"gemini-2.5-flash\",     tools = list(list(type = \"code_execution\", file_ids = list(penguins_file_metadata$name))) ) cat(google_gemini$get_generated_code(langs = c(\"python\"), as_chunks = TRUE)) import pandas as pd  # Load the CSV file df = pd.read_csv(\"input_file_0.csv\")  # Display the first few rows and information about the DataFrame print(\"DataFrame Head:\") print(df.head()) print(\"\\nDataFrame Info:\") print(df.info()) # Group by species, sex, and year and calculate the average body_mass_g summary_table = df.groupby(['species', 'sex', 'year'])['body_mass_g'].mean().reset_index()  # Rename the 'body_mass_g' column to 'average_body_mass_g' for clarity summary_table = summary_table.rename(columns={'body_mass_g': 'average_body_mass_g'})  # Save the summary table to a CSV file output_filename = \"average_body_mass_summary.csv\" summary_table.to_csv(output_filename, index=False)  print(f\"Summary table saved to {output_filename}\") print(\"\\nSummary Table Head:\") print(summary_table.head()) downloaded_path <- google_gemini$download_generated_files(dest_path = \"data\") #> ✔ File saved: data/generated_file_9604db3d-57fc-4aab-bd60-bf4c194ab6e5.csv read.csv(downloaded_path) species    sex year average_body_mass_g 1     Adelie female 2007            3389.773 2     Adelie female 2008            3386.000 3     Adelie female 2009            3334.615 4     Adelie   male 2007            4038.636 5     Adelie   male 2008            4098.000 6     Adelie   male 2009            3995.192 7  Chinstrap female 2007            3569.231 8  Chinstrap female 2008            3472.222 9  Chinstrap female 2009            3522.917 10 Chinstrap   male 2007            3819.231 11 Chinstrap   male 2008            4127.778 12 Chinstrap   male 2009            3927.083 13    Gentoo female 2007            4618.750 14    Gentoo female 2008            4627.273 15    Gentoo female 2009            4786.250 16    Gentoo   male 2007            5552.941 17    Gentoo   male 2008            5410.870 18    Gentoo   male 2009            5510.714 penguin_output_schema <- schema(     name = \"penguin_output\",     description = \"Schema for the penguin output\",     average_body_mass = \"number* The average body_mass\",     species = \"string* The species\",     sex = \"string* The sex\",     year = \"integer* The year\" )  google_gemini$chat(     \"What's the average body_mass for the Adelie females in 2009?\",     tools = list(\"code_execution\"),     output_schema = penguin_output_schema ) $species [1] \"Adelie\"  $sex [1] \"females\"  $year [1] 2009  $average_body_mass [1] 3334.62"},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"multimodal-inputs","dir":"Articles","previous_headings":"","what":"Multimodal Inputs","title":"Google (Gemini)","text":"Google Gemini supports sending: PDF (files URLs) text base64 Images (files URLs) base64 Remote files (as_file_content()) Plain text & code files Text-based data files (csv, tsv, json, ..) R objects Sending local image: providing path local image, automatically converted base64 sent server. say ! Sending image URL: Note , URL automatically detected, downloaded temporary file, converted base64, passed model. providers may different behavior. example, Anthropic supports passing image files URLs directly. Tip used as_image_content(bsg04_cast_image_url) helper make sure path/url treated image, , optionally, resize image sending . Sending local PDF: Damn right! Sending PDF URLs: Google, default, PDF URLs sent base64 server. However, can use as_text_content() helper pdftools::pdf_convert() parse PDFs pass text contents model instead. can pass R object chat() : Note R object passed chat() automatically converted JSON (text JSON conversion fails), added information like name object classes. Upload file reference as_file_content(): Note , using as_file_content() necessary signal model use remote file reference, rather just text content. Tip can use $list_files() list uploaded files metadata.","code":"bsg04_cast_image_url <- \"https://upload.wikimedia.org/wikipedia/en/1/1a/Battlestar_Galactica_%282004%29_cast.jpg\" bsg04_cast_image_path <- download_temp_file(bsg04_cast_image_url) google_gemini$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_path ) The characters in the image are from the show **Battlestar Galactica**.  From left to right, the characters are: *   **Admiral William Adama** (played by Edward James Olmos) *   **President Laura Roslin** (played by Mary McDonnell) *   **Captain Lee \"Apollo\" Adama** (played by Jamie Bamber) *   **Lieutenant Kara \"Starbuck\" Thrace** (played by Katee Sackhoff) *   **Number Six** (played by Tricia Helfer) *   **Gaius Baltar** (played by James Callis) *   **Sharon \"Boomer\" Valerii / Sharon \"Athena\" Agathon** (played by Grace Park) google_gemini$reset_history()  google_gemini$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_url ) my_cv_url <- \"https://ma-riviere.com/res/cv.pdf\" my_cv_pdf_path <- download_temp_file(my_cv_url) google_gemini$chat(     \"What's my favorite programming language ?\",     my_cv_pdf_path ) Based on your resume, your favorite programming language appears to be **R**. r6_pdf_url <- \"https://cran.r-project.org/web/packages/R6/R6.pdf\" s7_pdf_url <- \"https://cran.r-project.org/web/packages/S7/S7.pdf\"  multimodal_prompt <- list(     \"Give a 3 sentences summary of the advantages of S7 over R6\",     as_text_content(r6_pdf_url),     as_text_content(s7_pdf_url),     \"And give me the current versions of both packages\" # To make sure you actually read the PDFs )  google_gemini$chat(!!!multimodal_prompt) S7 offers a more formal and comprehensive object-oriented system for R, explicitly designed as a successor to both S3 and S4, featuring formal class, generic, and method specifications. It supports limited multiple dispatch, allowing method selection based on the types of multiple arguments, a capability not natively offered by R6. Furthermore, S7 is the result of a collaborative effort by the R Consortium Object-Oriented Programming Working Group, suggesting a broader community consensus and potential for standardization in R's future.  The current versions of the packages are: *   **R6:** 2.6.1 (published 2025-02-15) *   **S7:** 0.2.0 (published 2024-11-07) lm_obj <- lm(body_mass ~ species + sex, data = datasets::penguins)  google_gemini$chat(     \"What can we deduct from this regression model ?\",     lm_obj,     model = \"gemini-2.5-flash\" ) From the provided regression model output, we can deduce the following three points:  1.  **Model Type and Purpose**: This is a linear regression model (`lm_obj`) attempting to explain or predict `body_mass` based on two categorical predictor variables: `species` and `sex`. The model was fitted using the `datasets::penguins` dataset. 2.  **Intercept and Reference Group Body Mass**: The estimated intercept is 3372.4. Given that `contr.treatment` was used for both categorical variables, this intercept represents the estimated average `body_mass` (likely in grams) for the baseline group: an **Adelie female penguin**. 3.  **Estimated Effects of Predictors**:     *   **Species Effect**: Compared to Adelie penguins (the reference species), Chinstrap penguins are estimated to have an average `body_mass` that is 26.9 units higher, and Gentoo penguins are estimated to have an average `body_mass` that is 1377.9 units higher, assuming sex is held constant.     *   **Sex Effect**: Compared to female penguins (the reference sex), male penguins are estimated to have an average `body_mass` that is 667.6 units higher, assuming species is held constant. file_metadata <- google_gemini$upload_file(my_cv_url) # Pass an URL or a local file path  multipart_prompt <- list(     \"What are my two favorite frameworks/tools ?\",     as_file_content(file_metadata$name) )  google_gemini$chat(!!!multipart_prompt, model = \"gemini-2.5-flash\")  google_gemini$delete_file(file_metadata$name) Based on your resume, your two favorite frameworks/tools appear to be **Shiny** and **Quarto/R Markdown**.  Here's why: *   Your most recent role is \"R/Shiny Developer,\" highlighting Shiny as a primary tool. *   Shiny is listed first and prominently in your \"Frameworks & Tools\" section. *   \"Quarto, R Markdown\" is also listed prominently and is explicitly mentioned in your \"Consulting\" section, and is a natural pairing with R and Shiny for scientific publishing and interactive content."},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"image-comprehension","dir":"Articles","previous_headings":"Server-side Tools","what":"Image Comprehension","title":"Google (Gemini)","text":"Sending local image: providing path local image, automatically converted base64 sent server. say ! Sending image URL: Note , URL automatically detected, downloaded temporary file, converted base64, passed model. providers may different behavior. example, Anthropic supports passing image files URLs directly. Tip used as_image_content(bsg04_cast_image_url) helper make sure path/url treated image, , optionally, resize image sending .","code":"bsg04_cast_image_url <- \"https://upload.wikimedia.org/wikipedia/en/1/1a/Battlestar_Galactica_%282004%29_cast.jpg\" bsg04_cast_image_path <- download_temp_file(bsg04_cast_image_url) google_gemini$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_path ) The characters in the image are from the show **Battlestar Galactica**.  From left to right, the characters are: *   **Admiral William Adama** (played by Edward James Olmos) *   **President Laura Roslin** (played by Mary McDonnell) *   **Captain Lee \"Apollo\" Adama** (played by Jamie Bamber) *   **Lieutenant Kara \"Starbuck\" Thrace** (played by Katee Sackhoff) *   **Number Six** (played by Tricia Helfer) *   **Gaius Baltar** (played by James Callis) *   **Sharon \"Boomer\" Valerii / Sharon \"Athena\" Agathon** (played by Grace Park) google_gemini$reset_history()  google_gemini$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_url )"},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"pdf-comprehension","dir":"Articles","previous_headings":"Server-side Tools","what":"PDF Comprehension","title":"Google (Gemini)","text":"Sending local PDF: Damn right! Sending PDF URLs: Google, default, PDF URLs sent base64 server. However, can use as_text_content() helper pdftools::pdf_convert() parse PDFs pass text contents model instead.","code":"my_cv_url <- \"https://ma-riviere.com/res/cv.pdf\" my_cv_pdf_path <- download_temp_file(my_cv_url) google_gemini$chat(     \"What's my favorite programming language ?\",     my_cv_pdf_path ) Based on your resume, your favorite programming language appears to be **R**. r6_pdf_url <- \"https://cran.r-project.org/web/packages/R6/R6.pdf\" s7_pdf_url <- \"https://cran.r-project.org/web/packages/S7/S7.pdf\"  multimodal_prompt <- list(     \"Give a 3 sentences summary of the advantages of S7 over R6\",     as_text_content(r6_pdf_url),     as_text_content(s7_pdf_url),     \"And give me the current versions of both packages\" # To make sure you actually read the PDFs )  google_gemini$chat(!!!multimodal_prompt) S7 offers a more formal and comprehensive object-oriented system for R, explicitly designed as a successor to both S3 and S4, featuring formal class, generic, and method specifications. It supports limited multiple dispatch, allowing method selection based on the types of multiple arguments, a capability not natively offered by R6. Furthermore, S7 is the result of a collaborative effort by the R Consortium Object-Oriented Programming Working Group, suggesting a broader community consensus and potential for standardization in R's future.  The current versions of the packages are: *   **R6:** 2.6.1 (published 2025-02-15) *   **S7:** 0.2.0 (published 2024-11-07)"},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"passing-r-objects","dir":"Articles","previous_headings":"Server-side Tools","what":"Passing R Objects","title":"Google (Gemini)","text":"can pass R object chat() : Note R object passed chat() automatically converted JSON (text JSON conversion fails), added information like name object classes.","code":"lm_obj <- lm(body_mass ~ species + sex, data = datasets::penguins)  google_gemini$chat(     \"What can we deduct from this regression model ?\",     lm_obj,     model = \"gemini-2.5-flash\" ) From the provided regression model output, we can deduce the following three points:  1.  **Model Type and Purpose**: This is a linear regression model (`lm_obj`) attempting to explain or predict `body_mass` based on two categorical predictor variables: `species` and `sex`. The model was fitted using the `datasets::penguins` dataset. 2.  **Intercept and Reference Group Body Mass**: The estimated intercept is 3372.4. Given that `contr.treatment` was used for both categorical variables, this intercept represents the estimated average `body_mass` (likely in grams) for the baseline group: an **Adelie female penguin**. 3.  **Estimated Effects of Predictors**:     *   **Species Effect**: Compared to Adelie penguins (the reference species), Chinstrap penguins are estimated to have an average `body_mass` that is 26.9 units higher, and Gentoo penguins are estimated to have an average `body_mass` that is 1377.9 units higher, assuming sex is held constant.     *   **Sex Effect**: Compared to female penguins (the reference sex), male penguins are estimated to have an average `body_mass` that is 667.6 units higher, assuming species is held constant."},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"passing-uploaded-files","dir":"Articles","previous_headings":"Server-side Tools","what":"Passing Uploaded Files","title":"Google (Gemini)","text":"Upload file reference as_file_content(): Note , using as_file_content() necessary signal model use remote file reference, rather just text content. Tip can use $list_files() list uploaded files metadata.","code":"file_metadata <- google_gemini$upload_file(my_cv_url) # Pass an URL or a local file path  multipart_prompt <- list(     \"What are my two favorite frameworks/tools ?\",     as_file_content(file_metadata$name) )  google_gemini$chat(!!!multipart_prompt, model = \"gemini-2.5-flash\")  google_gemini$delete_file(file_metadata$name) Based on your resume, your two favorite frameworks/tools appear to be **Shiny** and **Quarto/R Markdown**.  Here's why: *   Your most recent role is \"R/Shiny Developer,\" highlighting Shiny as a primary tool. *   Shiny is listed first and prominently in your \"Frameworks & Tools\" section. *   \"Quarto, R Markdown\" is also listed prominently and is explicitly mentioned in your \"Consulting\" section, and is a natural pairing with R and Shiny for scientific publishing and interactive content."},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"server-side-rag","dir":"Articles","previous_headings":"","what":"File Search & RAG","title":"Google (Gemini)","text":"file search tool enables Retrieval-Augmented Generation (RAG) allowing model search uploaded files. Google’s managed solution file search using vector embeddings. First, create file search store hold files: can either upload files directly import existing File API files: Direct upload: Import File API: Another way upload files use upload_file(), import uploaded file store. result . files uploaded, use file_search tool chat requests: Extract grounding metadata citations: Add custom metadata files filtering: Control files chunked better retrieval. can also query specific files without using chat interface: List stores: List files store: Get file details: Delete file: Delete store: Note Pricing & Limits: - File search currently preview pricing may change - Maximum file size: 2GB - Supported formats: PDF, TXT, HTML, DOCX, ","code":"store <- google_gemini$create_store(name = \"R OOP Store\") #> ✔ [Google] File search store created: fileSearchStores/r-oop-store-bkd207kkacou r6_pdf_url <- \"https://cran.r-project.org/web/packages/R6/R6.pdf\"  doc <- google_gemini$add_file_to_store(     file_path = r6_pdf_url,     store_name = store$name,     file_name = \"R6 Package Documentation\" ) #> ✔ [Google] File uploaded to store: fileSearchStores/r-oop-store-bkd207kkacou/documents/r6-package-documentation-9uwwjohjvjj1 file_metadata <- google_gemini$upload_file(r6_pdf_url, name = \"R6 Package Documentation 2\") #> ✔ [Google] File uploaded: files/ie6e5jfpuwpf  doc <- google_gemini$import_file_to_store(     file_name = file_metadata$name,     store_name = store$name ) #> ✔ [Google] File imported: fileSearchStores/r-oop-store-bkd207kkacou/documents/ie6e5jfpuwpf-3z73sypazx1o google_gemini$chat(     \"What are the key features of R6 classes compared to reference classes? In three sentences.\",     model = \"gemini-2.5-flash\",     tools = list(         list(type = \"file_search\", store_names = list(store$name))     ) ) cat(yaml::as.yaml(google_gemini$get_supplementary())) grounding_metadata:   groundingChunks:   - retrievedContext:       title: je792h7nxtr4       text: |-         above) of the object.          The default print method of R6 objects can be redefined, by supplying a public print method.          (print members that are not functions are ignored.) This method is automatically called when-          ever the object is printed, e.g. when the object's name is typed at the command prompt, or when          print(obj) is called. It can also be called directly via obj\\$print(). All extra arguments from a          print(obj,...) call are passed on to the obj\\$print(...) method.          Portable and non-portable classes          When R6 classes are portable (the default), they can be inherited across packages without compli-          cation. However, when in portable mode, members must be accessed with self and private, as in          self\\$x and private\\$y.          --- PAGE 5 ---          R6Class          5          When used in non-portable mode, R6 classes behave more like reference classes: inheritance across          packages will not work well, and self and private are not necessary for accessing fields.          Cloning objects          R6 objects have a method named clone by default. To disable this, use cloneable=FALSE. Having          the clone method present will slightly increase the memory footprint of R6 objects, but since the          method will be shared across all R6 objects, the memory use will be negligible.          By default, calling x\\$clone() on an R6 object will result in a shallow       fileSearchStore: fileSearchStores/r-oop-store-bkd207kkacou ... s7_pdf_url <- \"https://cran.r-project.org/web/packages/S7/S7.pdf\"  doc2 <- google_gemini$add_file_to_store(     file_path = s7_pdf_url,     store_name = store$name,     file_name = \"S7 Package Documentation\",     custom_metadata = list(         package = \"S7\",         version = \"0.2.0\",         year = 2024     ) ) #> ✔ [Google] File uploaded to store: fileSearchStores/r-oop-store-bkd207kkacou/documents/fileSearchStores/r-oop-store-bkd207kkacou/documents/s7-package-documentation-x7o9goo6eufz  google_gemini$chat(     \"What are the main features of S7?\",     model = \"gemini-2.5-flash\",     tools = list(         list(             type = \"file_search\",             store_names = list(store$name),             metadata_filter = \"package=S7\"         )     ) ) doc3 <- google_gemini$add_file_to_store(     file_path = s7_pdf_url,     store_name = store$name,     file_name = \"S7 Package Documentation 2\",     chunking_config = list(         max_tokens_per_chunk = 400,         max_overlap_tokens = 50     ) ) #> ✔ [Google] File uploaded to store: fileSearchStores/r-oop-store-bkd207kkacou/documents/s7-package-documentation-2-sgob116kg3ts google_gemini$query_file(     file_name = doc$name,     query = \"What are active bindings?\",     results_count = 5 ) google_gemini$list_stores() google_gemini$list_files_in_store(store$name) google_gemini$read_file_from_store(doc$name) google_gemini$delete_file_from_store(doc$name) google_gemini$delete_store(store$name, force = TRUE)"},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"creating-a-file-search-store","dir":"Articles","previous_headings":"","what":"Creating a File Search Store","title":"Google (Gemini)","text":"First, create file search store hold files:","code":"store <- google_gemini$create_store(name = \"R OOP Store\") #> ✔ [Google] File search store created: fileSearchStores/r-oop-store-bkd207kkacou"},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"uploading-files-to-store","dir":"Articles","previous_headings":"","what":"Uploading Files to Store","title":"Google (Gemini)","text":"can either upload files directly import existing File API files: Direct upload: Import File API: Another way upload files use upload_file(), import uploaded file store. result .","code":"r6_pdf_url <- \"https://cran.r-project.org/web/packages/R6/R6.pdf\"  doc <- google_gemini$add_file_to_store(     file_path = r6_pdf_url,     store_name = store$name,     file_name = \"R6 Package Documentation\" ) #> ✔ [Google] File uploaded to store: fileSearchStores/r-oop-store-bkd207kkacou/documents/r6-package-documentation-9uwwjohjvjj1 file_metadata <- google_gemini$upload_file(r6_pdf_url, name = \"R6 Package Documentation 2\") #> ✔ [Google] File uploaded: files/ie6e5jfpuwpf  doc <- google_gemini$import_file_to_store(     file_name = file_metadata$name,     store_name = store$name ) #> ✔ [Google] File imported: fileSearchStores/r-oop-store-bkd207kkacou/documents/ie6e5jfpuwpf-3z73sypazx1o"},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"using-file-search-in-chat","dir":"Articles","previous_headings":"","what":"Using File Search in Chat","title":"Google (Gemini)","text":"files uploaded, use file_search tool chat requests: Extract grounding metadata citations:","code":"google_gemini$chat(     \"What are the key features of R6 classes compared to reference classes? In three sentences.\",     model = \"gemini-2.5-flash\",     tools = list(         list(type = \"file_search\", store_names = list(store$name))     ) ) cat(yaml::as.yaml(google_gemini$get_supplementary())) grounding_metadata:   groundingChunks:   - retrievedContext:       title: je792h7nxtr4       text: |-         above) of the object.          The default print method of R6 objects can be redefined, by supplying a public print method.          (print members that are not functions are ignored.) This method is automatically called when-          ever the object is printed, e.g. when the object's name is typed at the command prompt, or when          print(obj) is called. It can also be called directly via obj\\$print(). All extra arguments from a          print(obj,...) call are passed on to the obj\\$print(...) method.          Portable and non-portable classes          When R6 classes are portable (the default), they can be inherited across packages without compli-          cation. However, when in portable mode, members must be accessed with self and private, as in          self\\$x and private\\$y.          --- PAGE 5 ---          R6Class          5          When used in non-portable mode, R6 classes behave more like reference classes: inheritance across          packages will not work well, and self and private are not necessary for accessing fields.          Cloning objects          R6 objects have a method named clone by default. To disable this, use cloneable=FALSE. Having          the clone method present will slightly increase the memory footprint of R6 objects, but since the          method will be shared across all R6 objects, the memory use will be negligible.          By default, calling x\\$clone() on an R6 object will result in a shallow       fileSearchStore: fileSearchStores/r-oop-store-bkd207kkacou ..."},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"using-metadata-filtering","dir":"Articles","previous_headings":"","what":"Using Metadata Filtering","title":"Google (Gemini)","text":"Add custom metadata files filtering:","code":"s7_pdf_url <- \"https://cran.r-project.org/web/packages/S7/S7.pdf\"  doc2 <- google_gemini$add_file_to_store(     file_path = s7_pdf_url,     store_name = store$name,     file_name = \"S7 Package Documentation\",     custom_metadata = list(         package = \"S7\",         version = \"0.2.0\",         year = 2024     ) ) #> ✔ [Google] File uploaded to store: fileSearchStores/r-oop-store-bkd207kkacou/documents/fileSearchStores/r-oop-store-bkd207kkacou/documents/s7-package-documentation-x7o9goo6eufz  google_gemini$chat(     \"What are the main features of S7?\",     model = \"gemini-2.5-flash\",     tools = list(         list(             type = \"file_search\",             store_names = list(store$name),             metadata_filter = \"package=S7\"         )     ) )"},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"chunking-configuration","dir":"Articles","previous_headings":"","what":"Chunking Configuration","title":"Google (Gemini)","text":"Control files chunked better retrieval.","code":"doc3 <- google_gemini$add_file_to_store(     file_path = s7_pdf_url,     store_name = store$name,     file_name = \"S7 Package Documentation 2\",     chunking_config = list(         max_tokens_per_chunk = 400,         max_overlap_tokens = 50     ) ) #> ✔ [Google] File uploaded to store: fileSearchStores/r-oop-store-bkd207kkacou/documents/s7-package-documentation-2-sgob116kg3ts"},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"querying-files-directly","dir":"Articles","previous_headings":"","what":"Querying Files Directly","title":"Google (Gemini)","text":"can also query specific files without using chat interface:","code":"google_gemini$query_file(     file_name = doc$name,     query = \"What are active bindings?\",     results_count = 5 )"},{"path":"https://ma-riviere.github.io/argent/articles/google-gemini.html","id":"managing-stores-and-files","dir":"Articles","previous_headings":"","what":"Managing Stores and Files","title":"Google (Gemini)","text":"List stores: List files store: Get file details: Delete file: Delete store: Note Pricing & Limits: - File search currently preview pricing may change - Maximum file size: 2GB - Supported formats: PDF, TXT, HTML, DOCX, ","code":"google_gemini$list_stores() google_gemini$list_files_in_store(store$name) google_gemini$read_file_from_store(doc$name) google_gemini$delete_file_from_store(doc$name) google_gemini$delete_store(store$name, force = TRUE)"},{"path":"https://ma-riviere.github.io/argent/articles/local-llm.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Local LLMs","text":"article covers using local LLM servers argent. LocalLLM provider works OpenAI-compatible local server, including llama.cpp, Ollama, vLLM, others.","code":""},{"path":"https://ma-riviere.github.io/argent/articles/local-llm.html","id":"supported-servers","dir":"Articles","previous_headings":"","what":"Supported Servers","title":"Local LLMs","text":"llama.cpp (llama-server) Ollama vLLM LM Studio text-generation-webui OpenAI-compatible server","code":""},{"path":"https://ma-riviere.github.io/argent/articles/local-llm.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"Local LLMs","text":"initialization, argent automatically detect available model server set default model future chat() calls. can check current default model get_default_model_id().","code":"llama.cpp/build/bin/llama-server \\   --host 127.0.0.1 \\   --port 5000 \\   --flash-attn 1 \\   --n-gpu-layers 999 --n-gpu-layers-draft 999 \\   --model gemma-3-27b-it-IQ4_XS.gguf \\   --model-draft gemma-3-4b-it-IQ4_XS.gguf \\   --cache-type-k q8_0 --cache-type-v q8_0 \\   --ctx-size 64000 --ctx-size-draft 64000 \\   --draft-max 8 --draft-min 4 \\   --jinja \\   --reasoning-format deepseek \\   --reasoning-budget -1 \\   --mmproj mmproj-BF16.gguf local_llama <- LocalLLM$new(     base_url = \"http://localhost:5000\",     default_model = \"gemma-3-27b-it-IQ4_XS.gguf\" ) mirai::daemons(4) # To enable parallel tool calling"},{"path":"https://ma-riviere.github.io/argent/articles/local-llm.html","id":"starting-a-local-server","dir":"Articles","previous_headings":"","what":"Starting a Local Server","title":"Local LLMs","text":"","code":"llama.cpp/build/bin/llama-server \\   --host 127.0.0.1 \\   --port 5000 \\   --flash-attn 1 \\   --n-gpu-layers 999 --n-gpu-layers-draft 999 \\   --model gemma-3-27b-it-IQ4_XS.gguf \\   --model-draft gemma-3-4b-it-IQ4_XS.gguf \\   --cache-type-k q8_0 --cache-type-v q8_0 \\   --ctx-size 64000 --ctx-size-draft 64000 \\   --draft-max 8 --draft-min 4 \\   --jinja \\   --reasoning-format deepseek \\   --reasoning-budget -1 \\   --mmproj mmproj-BF16.gguf"},{"path":"https://ma-riviere.github.io/argent/articles/local-llm.html","id":"connecting-to-the-server","dir":"Articles","previous_headings":"","what":"Connecting to the Server","title":"Local LLMs","text":"initialization, argent automatically detect available model server set default model future chat() calls. can check current default model get_default_model_id().","code":"local_llama <- LocalLLM$new(     base_url = \"http://localhost:5000\",     default_model = \"gemma-3-27b-it-IQ4_XS.gguf\" ) mirai::daemons(4) # To enable parallel tool calling"},{"path":"https://ma-riviere.github.io/argent/articles/local-llm.html","id":"basic-completion","dir":"Articles","previous_headings":"","what":"Basic Completion","title":"Local LLMs","text":"","code":"local_llama$chat(\"What's the R programming language? Answer in three sentences.\")"},{"path":"https://ma-riviere.github.io/argent/articles/local-llm.html","id":"tool-calling-structured-output","dir":"Articles","previous_headings":"","what":"Tool Calling + Structured Output","title":"Local LLMs","text":"First, define web-related tools (search, crawl, fetch, general-use tool) bundled web_tools list: Search: Fetch: , let’s define JSON schema structured output using schema(): Wrong answer, used tools returned structured output. bad small local model. Note Structured output works model supports tool calling, even don’t support structured outputs response formats natively. argent handles “forced tool call” mechanism. Note Depending local machine’s speed, may need increase request timeout let server complete task. can setting argent.timeout option:","code":"web_search <- function(query) {     #' @description Search the web for information using Tavily API. Returns a JSON array of search results with titles, URLs, and content snippets. Use this when you need current information, facts, news, or any data not in your training data.     #' @param query:string* The search query string. Be specific and use keywords that will yield the most relevant results.          return(web_search_tavily(query)) }  web_search_tavily <- function(query) {     res <- httr2::request(\"https://api.tavily.com/search\") |>          httr2::req_body_json(list(             query = query,             search_depth = \"basic\",             include_answer = FALSE,             max_results = 10,             api_key = Sys.getenv(\"TAVILY_API_KEY\")         )) |>          httr2::req_error(is_error = \\(resp) FALSE) |>          httr2::req_throttle(rate = 20/60, realm = \"tavily\") |>          httr2::req_perform() |>          httr2::resp_body_json() |>          purrr::discard_at(c(\"response_time\", \"follow_up_questions\", \"images\"))      results <- purrr::map(res$results, \\(x) purrr::discard_at(x, \"raw_content\"))      return(jsonlite::toJSON(results, pretty = FALSE, auto_unbox = TRUE)) } web_fetch <- function(url) {     #' @description Fetch and extract the main text content from a web page as clean markdown. Returns the page content with formatting preserved, stripped of navigation, ads, and boilerplate. Use this to read articles, documentation, blog posts, or any web page content.     #' @param url:string* The complete URL of the web page to fetch (e.g., \"https://example.com/article\"). Must be a valid HTTP/HTTPS URL.          trafilatura_installed <- tryCatch({         system(\"which trafilatura\", intern = TRUE, ignore.stderr = TRUE)         return(TRUE)     },     warning = function(e) {         cli::cli_alert_warning(\"trafilatura is not installed. Install with: {.code pip install trafilatura}\")         return(FALSE)     })      if (trafilatura_installed) {         res <- web_fetch_trafilatura(url)          could_not_fetch <- c(             \"Impossible to fetch the contents of this web page\",             \"Please reload this page\",             \"There was an error while loading\",             \"404\"         )         if (is.null(res) || is.na(res) || nchar(res) == 0 ||             any(stringr::str_detect(res, stringr::fixed(could_not_fetch, ignore_case = TRUE)))) {             return(web_fetch_rvest(url))         }         return(res)     }      return(web_fetch_rvest(url)) }  web_fetch_trafilatura <- function(url) {     # pip install trafilatura     tryCatch({         res <- paste0(\"trafilatura -u \", url, \" --markdown --no-comments --links \") |>              system(intern = TRUE) |>              purrr::keep(nzchar) |>             paste0(collapse = \"\\n\")                  return(res)     },     error = function(e) {         return(\"Impossible to fetch the contents of this web page. It might not allow scraping\")     }) }  web_fetch_rvest <- function(url) {     tags_to_ignore <- c(         \"a\", \"script\", \"code\", \"img\", \"svg\", \"footer\", \"g\", \"path\", \"polygon\", \"label\", \"button\", \"form\", \"input\", \"select\",          \"style\", \"link\", \"meta\", \"noscript\", \"iframe\", \"embed\", \"object\", \"param\", \"video\", \"audio\", \"track\", \"source\",          \"canvas\", \"map\", \"area\", \"math\", \"col\", \"colgroup\", \"dl\", \"dt\", \"dd\", \"hr\", \"pre\", \"address\", \"figure\", \"figcaption\",         \"dfn\", \"em\", \"kbd\", \"samp\", \"var\", \"del\", \"ins\", \"mark\", \"circle\"     )      remove_tags <- function(xml, tags) {         purrr::walk(tags, \\(tag) purrr::walk(xml2::xml_find_all(xml, paste0(\".//\", tag)), \\(node) xml2::xml_remove(node)))         return(xml)     }      cleaned_contents <- tryCatch(         rvest::read_html(url)         |> rvest::html_element(\"body\")         |> remove_tags(tags_to_ignore)         |> rvest::html_children()         |> rvest::html_text2()         |> purrr::discard(\\(x) x == \"\")         |> paste0(collapse = \"\\n\\n\"),         error = \\(e) return(\"\")     )     return(cleaned_contents) } package_info_schema <- schema(     name = \"package_info\",     description = \"Information about an R package release\",     release_version = \"string* The release version of the package\",     release_date = \"string* The release date of the `release_version`\" ) local_llama$chat(     \"When was the first release of the R 'ellmer' package on GitHub?\",     tools = list(as_tool(web_search), as_tool(web_fetch)),     output_schema = package_info_schema ) $release_version [1] \"0.2.0\"  $release_date [1] \"2025-05-01\" options(argent.timeout = 120)"},{"path":"https://ma-riviere.github.io/argent/articles/local-llm.html","id":"reasoningthinking","dir":"Articles","previous_headings":"","what":"Reasoning/Thinking","title":"Local LLMs","text":"available, can extract reasoning response using get_reasoning_text(): Alternatively, simply print(local_llama) see reasoning answers’ text console, turn turn. Note get_reasoning_text() get_content_text() use last API response (local_llama$get_last_response()) default. Tip Reasoning capabilities local models controlled server level, API parameters. llama.cpp, use flags starting server: --reasoning-format: Specify reasoning format (e.g., deepseek) --reasoning-budget: -1 unlimited budget, 0 budget","code":"cat(local_llama$get_reasoning_text())"},{"path":"https://ma-riviere.github.io/argent/articles/local-llm.html","id":"multimodal-inputs","dir":"Articles","previous_headings":"","what":"Multimodal Inputs","title":"Local LLMs","text":"types media can send depend model using. minimum, can send type text content (files/objects automatically converted text). models support vision, can send images possibly PDFs. models need convert PDF image first (can passing PDF path as_image_content()). Sending local image: providing path local image, automatically converted base64 sent server. say ! Sending image URL: Note models don’t support PDF processing natively. can either convert PDF text using as_text_content() (use pdftools::pdf_convert()), convert PDF image using as_image_content() hope model better image comprehension capabilities tool used convert PDF text. Let’s send two PDF URLs use as_text_content() helper pdftools::pdf_convert() parse PDFs pass text content model instead passing base64. can pass R object chat() : Note R object passed chat() automatically converted JSON (text JSON conversion fails), added information like name object classes. Warning Remote file references via as_file_content() supported local servers. Use local file paths URLs instead.","code":"bsg04_cast_image_url <- \"https://upload.wikimedia.org/wikipedia/en/1/1a/Battlestar_Galactica_%282004%29_cast.jpg\" bsg04_cast_image_path <- download_temp_file(bsg04_cast_image_url) local_llama$chat(         \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_path ) Based on the image, the characters are from the science fiction television series *Battlestar Galactica*.   From left to right, the characters appear to be:  *   **William Adama** (Edward James Olmos) *   **Laura Roslin** (Mary McDonnell) *   **Lee Adama** (Jamie Bamber) *   **Kara \"Starbuck\" Thrace** (Katee Sackhoff) *   **Number Six** (Tricia Helfer) *   **Daniel \"Crashdown\" Tyrol** (Aaron D'Alessandro) *   **Sharon \"Boomer\" Valerii/Sharon \"Athena\" Agathon** (Grace Park)  I am confident in this identification, as the image is a promotional photo associated with the show and its primary cast. local_llama$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_url ) r6_pdf_url <- \"https://cran.r-project.org/web/packages/R6/R6.pdf\" s7_pdf_url <- \"https://cran.r-project.org/web/packages/S7/S7.pdf\"  multimodal_prompt <- list(     \"Give a 3 sentences summary of the advantages of S7 over R6\",     as_text_content(r6_pdf_url),     as_text_content(s7_pdf_url),     \"And give me the current versions of both packages\" # To make sure you actually read the PDFs )  local_llama$chat(!!!multimodal_prompt) Here's a 3-sentence summary of the advantages of S7 over R6, based on the provided documentation:  S7 offers a more formal and robust object-oriented programming system compared to R6, designed as a successor to both S3 and S4, and incorporates formal class and method specification with a limited form of multiple dispatch. Unlike R6's reliance on environments, S7 aims to provide a more structured approach with explicit class definitions and inheritance.  S7 also allows for integration with existing S3 and S4 systems, providing flexibility and a pathway for migrating existing code.  ---  According to the provided documentation:  *   **S7 version:** 0.2.0 (Published 2024-11-07) *   **R6 version:** 2.6.1 (Published 2025-02-15) lm_obj <- lm(body_mass ~ species + sex, data = datasets::penguins)  local_llama$chat(     \"What can we deduct from this regression model?\",     lm_obj ) Here's what we can deduce from the provided regression model summary:  1.  **Model Type:** The model is a linear regression (`lm` object). 2.  **Predictors:** The model predicts `body_mass` based on `species` and `sex`. 3.  **Coefficients:**     *   The intercept is 3372.4.     *   The coefficient for `species` is 26.9. This suggests that for each unit change in species (relative to the baseline species), body mass increases by 26.9 units, holding sex constant.     *   The coefficient for `species` is 1377.9.      *   The coefficient for `sex` is 667.6. This suggests that males have, on average, a body mass 667.6 units higher than females, holding species constant.  It's important to remember that these deductions are based solely on the provided output and do not include information about statistical significance (p-values), R-squared, or residual analysis, which would provide a more comprehensive understanding of the model's fit and reliability."},{"path":"https://ma-riviere.github.io/argent/articles/local-llm.html","id":"image-comprehension","dir":"Articles","previous_headings":"","what":"Image Comprehension","title":"Local LLMs","text":"Sending local image: providing path local image, automatically converted base64 sent server. say ! Sending image URL:","code":"bsg04_cast_image_url <- \"https://upload.wikimedia.org/wikipedia/en/1/1a/Battlestar_Galactica_%282004%29_cast.jpg\" bsg04_cast_image_path <- download_temp_file(bsg04_cast_image_url) local_llama$chat(         \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_path ) Based on the image, the characters are from the science fiction television series *Battlestar Galactica*.   From left to right, the characters appear to be:  *   **William Adama** (Edward James Olmos) *   **Laura Roslin** (Mary McDonnell) *   **Lee Adama** (Jamie Bamber) *   **Kara \"Starbuck\" Thrace** (Katee Sackhoff) *   **Number Six** (Tricia Helfer) *   **Daniel \"Crashdown\" Tyrol** (Aaron D'Alessandro) *   **Sharon \"Boomer\" Valerii/Sharon \"Athena\" Agathon** (Grace Park)  I am confident in this identification, as the image is a promotional photo associated with the show and its primary cast. local_llama$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_url )"},{"path":"https://ma-riviere.github.io/argent/articles/local-llm.html","id":"pdf-comprehension","dir":"Articles","previous_headings":"","what":"PDF Comprehension","title":"Local LLMs","text":"Note models don’t support PDF processing natively. can either convert PDF text using as_text_content() (use pdftools::pdf_convert()), convert PDF image using as_image_content() hope model better image comprehension capabilities tool used convert PDF text. Let’s send two PDF URLs use as_text_content() helper pdftools::pdf_convert() parse PDFs pass text content model instead passing base64.","code":"r6_pdf_url <- \"https://cran.r-project.org/web/packages/R6/R6.pdf\" s7_pdf_url <- \"https://cran.r-project.org/web/packages/S7/S7.pdf\"  multimodal_prompt <- list(     \"Give a 3 sentences summary of the advantages of S7 over R6\",     as_text_content(r6_pdf_url),     as_text_content(s7_pdf_url),     \"And give me the current versions of both packages\" # To make sure you actually read the PDFs )  local_llama$chat(!!!multimodal_prompt) Here's a 3-sentence summary of the advantages of S7 over R6, based on the provided documentation:  S7 offers a more formal and robust object-oriented programming system compared to R6, designed as a successor to both S3 and S4, and incorporates formal class and method specification with a limited form of multiple dispatch. Unlike R6's reliance on environments, S7 aims to provide a more structured approach with explicit class definitions and inheritance.  S7 also allows for integration with existing S3 and S4 systems, providing flexibility and a pathway for migrating existing code.  ---  According to the provided documentation:  *   **S7 version:** 0.2.0 (Published 2024-11-07) *   **R6 version:** 2.6.1 (Published 2025-02-15)"},{"path":"https://ma-riviere.github.io/argent/articles/local-llm.html","id":"passing-r-objects","dir":"Articles","previous_headings":"","what":"Passing R Objects","title":"Local LLMs","text":"can pass R object chat() : Note R object passed chat() automatically converted JSON (text JSON conversion fails), added information like name object classes.","code":"lm_obj <- lm(body_mass ~ species + sex, data = datasets::penguins)  local_llama$chat(     \"What can we deduct from this regression model?\",     lm_obj ) Here's what we can deduce from the provided regression model summary:  1.  **Model Type:** The model is a linear regression (`lm` object). 2.  **Predictors:** The model predicts `body_mass` based on `species` and `sex`. 3.  **Coefficients:**     *   The intercept is 3372.4.     *   The coefficient for `species` is 26.9. This suggests that for each unit change in species (relative to the baseline species), body mass increases by 26.9 units, holding sex constant.     *   The coefficient for `species` is 1377.9.      *   The coefficient for `sex` is 667.6. This suggests that males have, on average, a body mass 667.6 units higher than females, holding species constant.  It's important to remember that these deductions are based solely on the provided output and do not include information about statistical significance (p-values), R-squared, or residual analysis, which would provide a more comprehensive understanding of the model's fit and reliability."},{"path":"https://ma-riviere.github.io/argent/articles/local-llm.html","id":"file-references","dir":"Articles","previous_headings":"","what":"File References","title":"Local LLMs","text":"Warning Remote file references via as_file_content() supported local servers. Use local file paths URLs instead.","code":""},{"path":"https://ma-riviere.github.io/argent/articles/openai-assistants.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"OpenAI - Assistants API","text":"Assistants API manages conversation state server-side threads provides server-side tools like file_search code_interpreter.","code":""},{"path":"https://ma-riviere.github.io/argent/articles/openai-assistants.html","id":"key-differences-with-openais-other-apis","dir":"Articles","previous_headings":"","what":"Key Differences with OpenAI’s other APIs","title":"OpenAI - Assistants API","text":"Server-side state: Conversation history stored threads OpenAI servers Thread-based management: Conversations persist threads GPT-5 support: Limited GPT-4 earlier models reasoning: support reasoning_effort","code":""},{"path":"https://ma-riviere.github.io/argent/articles/openai-assistants.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"OpenAI - Assistants API","text":"Note create assistant, creates instance OpenAI_Assistant class. OpenAI Assistants, need specifically create assistant create_assistant() able call chat() method. assistant configuration (model, temperature, tools, system) set creation applies subsequent chat() calls. Note can check assistant configuration get_assistant(): can also load existing assistant load_assistant():","code":"openai_assistant <- OpenAI_Assistant$new(api_key = Sys.getenv(\"OPENAI_API_KEY\")) mirai::daemons(4) # To enable parallel tool calling openai_assistant$create_assistant(name = \"My Assistant\", model = \"gpt-4o-mini\") openai_assistant$get_assistant() # Find assistant by name assistant_id <- openai_assistant$find_assistants(name = \"My Assistant\") |>     purrr::pluck(\"id\", 1)  # Load in a new client instance existing_assistant <- OpenAI_Assistant$new()$load_assistant(id = assistant_id)"},{"path":"https://ma-riviere.github.io/argent/articles/openai-assistants.html","id":"discovering-models","dir":"Articles","previous_headings":"","what":"Discovering Models","title":"OpenAI - Assistants API","text":"Note Assistants API support GPT-5 models. Use GPT-4 earlier.","code":"openai_assistant$list_models() |>     dplyr::filter(stringr::str_detect(id, \"-4o-|-4.1-\"))"},{"path":"https://ma-riviere.github.io/argent/articles/openai-assistants.html","id":"basic-completion","dir":"Articles","previous_headings":"","what":"Basic Completion","title":"OpenAI - Assistants API","text":"Assistants API manages conversation state threads: First message: Creates new thread automatically Subsequent messages: Continue thread default New thread: Set in_new_thread = TRUE create fresh conversation Persistence: Threads persist OpenAI’s servers explicitly deleted Continue thread: Start new thread: Note Unlike Responses API uses previous_response_id, Assistants API maintains state persistent threads. Threads continue automatically unless explicitly create new one, e.g. in_new_thread = TRUE. Access latest thread messages get_thread_msgs() (see current ‘chat history’): , simply print openai_assistant object see current chat history:","code":"openai_assistant$chat(\"What's the R programming language? Answer in three sentences.\") #> ✔ [OpenAI Assistant] Thread created: thread_r4WVfgfBmPZ66WUpy4diT2gE openai_assistant$chat(\"Tell me more about its history\") openai_assistant$chat(\"What were we just talking about?\", in_new_thread = TRUE) #> ✔ [OpenAI Assistant] Thread created: thread_eVLlsNW3xGR2a0cUQSaoMX4G openai_assistant$get_chat_history() print(openai_assistant, show_tools = TRUE)"},{"path":"https://ma-riviere.github.io/argent/articles/openai-assistants.html","id":"tool-calling-structured-output","dir":"Articles","previous_headings":"","what":"Tool Calling + Structured Output","title":"OpenAI - Assistants API","text":"First, define web-related tools bundled web_tools list: Search: Fetch: , define JSON schema structured output using schema(): Create assistant client-side tools: Ask question structured output: Hum. quite right … Warning assistant uses server-side tools (like file_search code_interpreter) provide output_schema, API forces second call schema (tools) assistant done using tools, bypass fact can’t use server-side tools structured outputs. happens automatically.","code":"web_search <- function(query) {     #' @description Search the web for information using Tavily API. Returns a JSON array of search results with titles, URLs, and content snippets. Use this when you need current information, facts, news, or any data not in your training data.     #' @param query:string* The search query string. Be specific and use keywords that will yield the most relevant results.          return(web_search_tavily(query)) }  web_search_tavily <- function(query) {     res <- httr2::request(\"https://api.tavily.com/search\") |>          httr2::req_body_json(list(             query = query,             search_depth = \"basic\",             include_answer = FALSE,             max_results = 10,             api_key = Sys.getenv(\"TAVILY_API_KEY\")         )) |>          httr2::req_error(is_error = \\(resp) FALSE) |>          httr2::req_throttle(rate = 20/60, realm = \"tavily\") |>          httr2::req_perform() |>          httr2::resp_body_json() |>          purrr::discard_at(c(\"response_time\", \"follow_up_questions\", \"images\"))      results <- purrr::map(res$results, \\(x) purrr::discard_at(x, \"raw_content\"))      return(jsonlite::toJSON(results, pretty = FALSE, auto_unbox = TRUE)) } web_fetch <- function(url) {     #' @description Fetch and extract the main text content from a web page as clean markdown. Returns the page content with formatting preserved, stripped of navigation, ads, and boilerplate. Use this to read articles, documentation, blog posts, or any web page content.     #' @param url:string* The complete URL of the web page to fetch (e.g., \"https://example.com/article\"). Must be a valid HTTP/HTTPS URL.          trafilatura_installed <- tryCatch({         system(\"which trafilatura\", intern = TRUE, ignore.stderr = TRUE)         return(TRUE)     },     warning = function(e) {         cli::cli_alert_warning(\"trafilatura is not installed. Install with: {.code pip install trafilatura}\")         return(FALSE)     })      if (trafilatura_installed) {         res <- web_fetch_trafilatura(url)          could_not_fetch <- c(             \"Impossible to fetch the contents of this web page\",             \"Please reload this page\",             \"There was an error while loading\",             \"404\"         )         if (is.null(res) || is.na(res) || nchar(res) == 0 ||             any(stringr::str_detect(res, stringr::fixed(could_not_fetch, ignore_case = TRUE)))) {             return(web_fetch_rvest(url))         }         return(res)     }      return(web_fetch_rvest(url)) }  web_fetch_trafilatura <- function(url) {     # pip install trafilatura     tryCatch({         res <- paste0(\"trafilatura -u \", url, \" --markdown --no-comments --links \") |>              system(intern = TRUE) |>              purrr::keep(nzchar) |>             paste0(collapse = \"\\n\")                  return(res)     },     error = function(e) {         return(\"Impossible to fetch the contents of this web page. It might not allow scraping\")     }) }  web_fetch_rvest <- function(url) {     tags_to_ignore <- c(         \"a\", \"script\", \"code\", \"img\", \"svg\", \"footer\", \"g\", \"path\", \"polygon\", \"label\", \"button\", \"form\", \"input\", \"select\",          \"style\", \"link\", \"meta\", \"noscript\", \"iframe\", \"embed\", \"object\", \"param\", \"video\", \"audio\", \"track\", \"source\",          \"canvas\", \"map\", \"area\", \"math\", \"col\", \"colgroup\", \"dl\", \"dt\", \"dd\", \"hr\", \"pre\", \"address\", \"figure\", \"figcaption\",         \"dfn\", \"em\", \"kbd\", \"samp\", \"var\", \"del\", \"ins\", \"mark\", \"circle\"     )      remove_tags <- function(xml, tags) {         purrr::walk(tags, \\(tag) purrr::walk(xml2::xml_find_all(xml, paste0(\".//\", tag)), \\(node) xml2::xml_remove(node)))         return(xml)     }      cleaned_contents <- tryCatch(         rvest::read_html(url)         |> rvest::html_element(\"body\")         |> remove_tags(tags_to_ignore)         |> rvest::html_children()         |> rvest::html_text2()         |> purrr::discard(\\(x) x == \"\")         |> paste0(collapse = \"\\n\\n\"),         error = \\(e) return(\"\")     )     return(cleaned_contents) } package_info_schema <- schema(     name = \"package_info\",     description = \"Information about an R package release\",     release_version = \"string* The release version of the package\",     release_date = \"string* The release date of the `release_version`\" ) openai_assistant <- OpenAI_Assistant$new()$create_assistant(     name = \"My Assistant\",     model = \"gpt-4.1\",     tools = list(as_tool(web_search), as_tool(web_fetch)) ) openai_assistant$chat(     \"When was the first release of the R 'ellmer' package on GitHub?\",     output_schema = package_info_schema ) 6#> ✔ [OpenAI Assistant] Thread created: thread_leVxekjFQnO8vUES62HBJuLc $release_version [1] \"0.1.0\"  $release_date [1] \"2024-03-20\""},{"path":"https://ma-riviere.github.io/argent/articles/openai-assistants.html","id":"server-side-tools","dir":"Articles","previous_headings":"","what":"Server-side Tools","title":"OpenAI - Assistants API","text":"Server-side tools tools can call without define . run provider’s server. OpenAI Assistants API supports two server-side tools: file_search - Search uploaded files using vector stores (.e. server-side RAG) code_interpreter - Execute Python code sandboxed containers Assistants API provides code_interpreter executing Python code. Unlike Responses API uses containers, Assistants API attaches files directly tool. Example: Analyzing Penguins Dataset Download generated files: Continue asking questions thread: Incorrect year, incorrect sex, even accounting , value incorrect … Note Responses API uses containers code execution dedicated management methods (list_containers(), delete_container()). Assistants API attaches files directly maintains context automatically within threads.","code":"penguins_url <- \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/refs/heads/main/inst/extdata/penguins.csv\" penguins_file_metadata <- openai_assistant$upload_file(penguins_url, purpose = \"assistants\")  data_analyst <- OpenAI_Assistant$new()$create_assistant(     name = \"Data Analyst\",     model = \"gpt-4.1\",     tools = list(list(type = \"code_interpreter\", file_ids = list(penguins_file_metadata$id))) )  data_analyst$chat(     \"Create a summary table showing average body_mass grouped by species, sex, and year. Save as CSV.\" ) downloaded_paths <- data_analyst$download_generated_files(dest_path = \"data\") read.csv(downloaded_paths[1], na.strings = c(\"\", \"NA\")) species    sex year body_mass_g 1     Adelie female 2007    3389.773 2     Adelie female 2008    3386.000 3     Adelie female 2009    3334.615 4     Adelie   male 2007    4038.636 5     Adelie   male 2008    4098.000 6     Adelie   male 2009    3995.192 7  Chinstrap female 2007    3569.231 8  Chinstrap female 2008    3472.222 9  Chinstrap female 2009    3522.917 10 Chinstrap   male 2007    3819.231 11 Chinstrap   male 2008    4127.778 12 Chinstrap   male 2009    3927.083 13    Gentoo female 2007    4618.750 14    Gentoo female 2008    4627.273 15    Gentoo female 2009    4786.250 16    Gentoo   male 2007    5552.941 17    Gentoo   male 2008    5410.870 18    Gentoo   male 2009    5510.714 penguin_output_schema <- schema(     name = \"penguin_output\",     description = \"Schema for the penguin output\",     average_body_mass = \"number* The average body_mass\",     species = \"string* The species\",     sex = \"string* The sex\",     year = \"integer* The year\" )  data_analyst$chat(     \"What's the average body_mass for the Adelie females in 2009? Use the code_interpreter tool to compute the answer.\",     output_schema = penguin_output_schema ) $average_body_mass [1] 3317.073  $species [1] \"Adelie\"  $sex [1] \"male\"  $year [1] 2007"},{"path":"https://ma-riviere.github.io/argent/articles/openai-assistants.html","id":"server-side-code-interpreter","dir":"Articles","previous_headings":"","what":"Server-side: Code Interpreter","title":"OpenAI - Assistants API","text":"Assistants API provides code_interpreter executing Python code. Unlike Responses API uses containers, Assistants API attaches files directly tool. Example: Analyzing Penguins Dataset Download generated files: Continue asking questions thread: Incorrect year, incorrect sex, even accounting , value incorrect … Note Responses API uses containers code execution dedicated management methods (list_containers(), delete_container()). Assistants API attaches files directly maintains context automatically within threads.","code":"penguins_url <- \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/refs/heads/main/inst/extdata/penguins.csv\" penguins_file_metadata <- openai_assistant$upload_file(penguins_url, purpose = \"assistants\")  data_analyst <- OpenAI_Assistant$new()$create_assistant(     name = \"Data Analyst\",     model = \"gpt-4.1\",     tools = list(list(type = \"code_interpreter\", file_ids = list(penguins_file_metadata$id))) )  data_analyst$chat(     \"Create a summary table showing average body_mass grouped by species, sex, and year. Save as CSV.\" ) downloaded_paths <- data_analyst$download_generated_files(dest_path = \"data\") read.csv(downloaded_paths[1], na.strings = c(\"\", \"NA\")) species    sex year body_mass_g 1     Adelie female 2007    3389.773 2     Adelie female 2008    3386.000 3     Adelie female 2009    3334.615 4     Adelie   male 2007    4038.636 5     Adelie   male 2008    4098.000 6     Adelie   male 2009    3995.192 7  Chinstrap female 2007    3569.231 8  Chinstrap female 2008    3472.222 9  Chinstrap female 2009    3522.917 10 Chinstrap   male 2007    3819.231 11 Chinstrap   male 2008    4127.778 12 Chinstrap   male 2009    3927.083 13    Gentoo female 2007    4618.750 14    Gentoo female 2008    4627.273 15    Gentoo female 2009    4786.250 16    Gentoo   male 2007    5552.941 17    Gentoo   male 2008    5410.870 18    Gentoo   male 2009    5510.714 penguin_output_schema <- schema(     name = \"penguin_output\",     description = \"Schema for the penguin output\",     average_body_mass = \"number* The average body_mass\",     species = \"string* The species\",     sex = \"string* The sex\",     year = \"integer* The year\" )  data_analyst$chat(     \"What's the average body_mass for the Adelie females in 2009? Use the code_interpreter tool to compute the answer.\",     output_schema = penguin_output_schema ) $average_body_mass [1] 3317.073  $species [1] \"Adelie\"  $sex [1] \"male\"  $year [1] 2007"},{"path":"https://ma-riviere.github.io/argent/articles/openai-assistants.html","id":"multimodal-inputs","dir":"Articles","previous_headings":"","what":"Multimodal Inputs","title":"OpenAI - Assistants API","text":"OpenAI Assistants API supports sending: Images: URLs (-, base64), files (base64) PDFs: URLs (-, base64, text content), files (base64, text content) Remote files (as_file_content()) Plain text & code files Text-based data files (csv, tsv, json, ..) R objects Sending local image: provide local image path, image automatically uploaded server file_id passed model. Warning Behind scenes, uploading image server (.e. using $upload_file()) passing file_id model. Don’t forget delete file use $delete_file(). say ! Sending image URL: OpenAI supports sending image URLs -: Note OpenAI supports optional detail parameter (low/high/auto) control image processing detail level image sent URL. default value \"auto\". specify parameter, pass image URL as_image_content(url, .provider_options = list(detail = \"low\")). also use as_image_content() resize image sending .resize argument. OpenAI Assistants API also supports sending file_id (.e. image as_file_content(file_id) uploading openai_assistant$upload_file()). PDFs, need give assistant file_search tool able search PDF’s contents, let’s create assistant tool: Warning Behind scenes, uploading PDF server (.e. using $upload_file()) passing file_id model. Don’t forget delete file use $delete_file(). Sending local PDF: Damn right! Sending PDF URLs: OpenAI Assistants, default, PDF URLs (files) uploaded server file_id passed model. However, can use as_text_content() helper pdftools::pdf_convert() parse PDFs pass text contents model instead. can pass R object chat() : Note R object passed chat() automatically converted JSON (text JSON conversion fails), added information like name object classes. can also send files directly model. , also need give assistant file_search tool able search files’ contents, let’s create assistant tool: Uploading Files Listing Files Using Uploaded Files Use as_file_content() reference uploaded files: Note Assistants, thing simply giving local file path inputs $chat(), like “PDF Comprehension” section. Downloading Files Note Files purpose ‘assistants’ downloaded. Deleting Files","code":"bsg04_cast_image_url <- \"https://upload.wikimedia.org/wikipedia/en/1/1a/Battlestar_Galactica_%282004%29_cast.jpg\" bsg04_cast_image_path <- download_temp_file(bsg04_cast_image_url) openai_assistant$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_path,     in_new_thread = TRUE ) The characters in the image are from the television series **Battlestar Galactica**, which aired from 2004 to 2009. Here are the main characters shown:  1. **William Adama** (played by Edward James Olmos) - Commanding Officer of the Galactica. 2. **Laura Roslin** (played by Mary McDonnell) - President of the Twelve Colonies and a key leader. 3. **Lee \"Apollo\" Adama** (played by Jamie Bamber) - A captain in the Colonial Fleet and son of Bill Adama. 4. **Kara \"Starbuck\" Thrace** (played by Katee Sackhoff) - A talented pilot known for her rebellious nature. 5. **Gaius Baltar** (played by James Callis) - A scientist with a complex role in the series’ storyline. 6. **Number Six** (played by Tricia Helfer) - A Cylon who plays a significant role in the narrative. 7. **Baltar's Number Six** (also played by Tricia Helfer) - A manifestation that interacts with Gaius Baltar.  These characters are central to the themes of survival, morality, and human identity explored in the series. openai_assistant$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_url,     in_new_thread = TRUE ) my_cv_url <- \"https://ma-riviere.com/res/cv.pdf\" my_cv_pdf_path <- download_temp_file(my_cv_url) pdf_assistant <- OpenAI_Assistant$new()$create_assistant(     name = \"PDF Assistant\",     model = \"gpt-4o-mini\",     tools = list(\"file_search\") ) pdf_assistant$chat(     \"What's my favorite programming language?\",     my_cv_pdf_path ) Your favorite programming language appears to be **R**, as indicated by your extensive experience and focus on R and Shiny in your work and projects【4:0†source】. r6_pdf_url <- \"https://cran.r-project.org/web/packages/R6/R6.pdf\" s7_pdf_url <- \"https://cran.r-project.org/web/packages/S7/S7.pdf\"  multimodal_prompt <- list(     \"Give a 3 sentences summary of the advantages of S7 over R6\",     as_text_content(r6_pdf_url),     as_text_content(s7_pdf_url),     \"And give me the current versions of both packages\" )  openai_assistant$chat(!!!multimodal_prompt, in_new_thread = TRUE) **Summary of the Advantages of S7 over R6:**  S7 is designed to be a more robust object-oriented programming system than R6, offering formal class and method specifications along with a limited form of multiple dispatch, which enhances its flexibility. It also integrates both S3 and S4 systems seamlessly, allowing for extensibility with the addition of a clear inheritance model and property validation, thereby promoting cleaner and more maintainable code. Furthermore, S7 facilitates better interoperability with base R types and modern R programming practices, making it more suitable for complex applications  .  **Current Versions of Both Packages:** - **R6:** Version 2.6.1 - **S7:** Version 0.2.0 lm_obj <- lm(body_mass ~ species + sex, data = datasets::penguins)  openai_assistant$chat(     \"What can we deduct from this regression model?\",     lm_obj,     in_new_thread = TRUE ) file_assistant <- OpenAI_Assistant$new()$create_assistant(     name = \"File Assistant\",     model = \"gpt-4o-mini\",     tools = list(\"file_search\") ) file_metadata <- file_assistant$upload_file(my_cv_url, purpose = \"assistants\") file_assistant$list_files() # A tibble: 1 × 9    object id                          purpose    filename                bytes created_at          expires_at status    status_details    <chr>  <chr>                       <chr>      <chr>                   <int> <dttm>              <lgl>      <chr>     <lgl>           1 file   file-B1MRQat6f1sRArT3KhFkGu assistants file1e612f9544482.pdf  431965 2025-11-14 09:03:52 NA         processed NA file_assistant$chat(     \"What are my two favorite frameworks/tools ?\",     as_file_content(file_metadata$id) ) Your two favorite frameworks/tools are **Shiny** and **Quarto**【4:0†source】. file_assistant$delete_file(file_metadata$id) #> ✔ [OpenAI] File deleted: file-B1MRQat6f1sRArT3KhFkGu"},{"path":"https://ma-riviere.github.io/argent/articles/openai-assistants.html","id":"image-comprehension","dir":"Articles","previous_headings":"","what":"Image Comprehension","title":"OpenAI - Assistants API","text":"Sending local image: provide local image path, image automatically uploaded server file_id passed model. Warning Behind scenes, uploading image server (.e. using $upload_file()) passing file_id model. Don’t forget delete file use $delete_file(). say ! Sending image URL: OpenAI supports sending image URLs -: Note OpenAI supports optional detail parameter (low/high/auto) control image processing detail level image sent URL. default value \"auto\". specify parameter, pass image URL as_image_content(url, .provider_options = list(detail = \"low\")). also use as_image_content() resize image sending .resize argument. OpenAI Assistants API also supports sending file_id (.e. image as_file_content(file_id) uploading openai_assistant$upload_file()).","code":"bsg04_cast_image_url <- \"https://upload.wikimedia.org/wikipedia/en/1/1a/Battlestar_Galactica_%282004%29_cast.jpg\" bsg04_cast_image_path <- download_temp_file(bsg04_cast_image_url) openai_assistant$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_path,     in_new_thread = TRUE ) The characters in the image are from the television series **Battlestar Galactica**, which aired from 2004 to 2009. Here are the main characters shown:  1. **William Adama** (played by Edward James Olmos) - Commanding Officer of the Galactica. 2. **Laura Roslin** (played by Mary McDonnell) - President of the Twelve Colonies and a key leader. 3. **Lee \"Apollo\" Adama** (played by Jamie Bamber) - A captain in the Colonial Fleet and son of Bill Adama. 4. **Kara \"Starbuck\" Thrace** (played by Katee Sackhoff) - A talented pilot known for her rebellious nature. 5. **Gaius Baltar** (played by James Callis) - A scientist with a complex role in the series’ storyline. 6. **Number Six** (played by Tricia Helfer) - A Cylon who plays a significant role in the narrative. 7. **Baltar's Number Six** (also played by Tricia Helfer) - A manifestation that interacts with Gaius Baltar.  These characters are central to the themes of survival, morality, and human identity explored in the series. openai_assistant$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_url,     in_new_thread = TRUE )"},{"path":"https://ma-riviere.github.io/argent/articles/openai-assistants.html","id":"pdf-comprehension","dir":"Articles","previous_headings":"","what":"PDF Comprehension","title":"OpenAI - Assistants API","text":"PDFs, need give assistant file_search tool able search PDF’s contents, let’s create assistant tool: Warning Behind scenes, uploading PDF server (.e. using $upload_file()) passing file_id model. Don’t forget delete file use $delete_file(). Sending local PDF: Damn right! Sending PDF URLs: OpenAI Assistants, default, PDF URLs (files) uploaded server file_id passed model. However, can use as_text_content() helper pdftools::pdf_convert() parse PDFs pass text contents model instead.","code":"my_cv_url <- \"https://ma-riviere.com/res/cv.pdf\" my_cv_pdf_path <- download_temp_file(my_cv_url) pdf_assistant <- OpenAI_Assistant$new()$create_assistant(     name = \"PDF Assistant\",     model = \"gpt-4o-mini\",     tools = list(\"file_search\") ) pdf_assistant$chat(     \"What's my favorite programming language?\",     my_cv_pdf_path ) Your favorite programming language appears to be **R**, as indicated by your extensive experience and focus on R and Shiny in your work and projects【4:0†source】. r6_pdf_url <- \"https://cran.r-project.org/web/packages/R6/R6.pdf\" s7_pdf_url <- \"https://cran.r-project.org/web/packages/S7/S7.pdf\"  multimodal_prompt <- list(     \"Give a 3 sentences summary of the advantages of S7 over R6\",     as_text_content(r6_pdf_url),     as_text_content(s7_pdf_url),     \"And give me the current versions of both packages\" )  openai_assistant$chat(!!!multimodal_prompt, in_new_thread = TRUE) **Summary of the Advantages of S7 over R6:**  S7 is designed to be a more robust object-oriented programming system than R6, offering formal class and method specifications along with a limited form of multiple dispatch, which enhances its flexibility. It also integrates both S3 and S4 systems seamlessly, allowing for extensibility with the addition of a clear inheritance model and property validation, thereby promoting cleaner and more maintainable code. Furthermore, S7 facilitates better interoperability with base R types and modern R programming practices, making it more suitable for complex applications  .  **Current Versions of Both Packages:** - **R6:** Version 2.6.1 - **S7:** Version 0.2.0"},{"path":"https://ma-riviere.github.io/argent/articles/openai-assistants.html","id":"passing-r-objects","dir":"Articles","previous_headings":"","what":"Passing R Objects","title":"OpenAI - Assistants API","text":"can pass R object chat() : Note R object passed chat() automatically converted JSON (text JSON conversion fails), added information like name object classes.","code":"lm_obj <- lm(body_mass ~ species + sex, data = datasets::penguins)  openai_assistant$chat(     \"What can we deduct from this regression model?\",     lm_obj,     in_new_thread = TRUE )"},{"path":"https://ma-riviere.github.io/argent/articles/openai-assistants.html","id":"sending-files","dir":"Articles","previous_headings":"","what":"Sending files","title":"OpenAI - Assistants API","text":"can also send files directly model. , also need give assistant file_search tool able search files’ contents, let’s create assistant tool: Uploading Files Listing Files Using Uploaded Files Use as_file_content() reference uploaded files: Note Assistants, thing simply giving local file path inputs $chat(), like “PDF Comprehension” section. Downloading Files Note Files purpose ‘assistants’ downloaded. Deleting Files","code":"file_assistant <- OpenAI_Assistant$new()$create_assistant(     name = \"File Assistant\",     model = \"gpt-4o-mini\",     tools = list(\"file_search\") ) file_metadata <- file_assistant$upload_file(my_cv_url, purpose = \"assistants\") file_assistant$list_files() # A tibble: 1 × 9    object id                          purpose    filename                bytes created_at          expires_at status    status_details    <chr>  <chr>                       <chr>      <chr>                   <int> <dttm>              <lgl>      <chr>     <lgl>           1 file   file-B1MRQat6f1sRArT3KhFkGu assistants file1e612f9544482.pdf  431965 2025-11-14 09:03:52 NA         processed NA file_assistant$chat(     \"What are my two favorite frameworks/tools ?\",     as_file_content(file_metadata$id) ) Your two favorite frameworks/tools are **Shiny** and **Quarto**【4:0†source】. file_assistant$delete_file(file_metadata$id) #> ✔ [OpenAI] File deleted: file-B1MRQat6f1sRArT3KhFkGu"},{"path":"https://ma-riviere.github.io/argent/articles/openai-assistants.html","id":"server-side-rag","dir":"Articles","previous_headings":"","what":"Server-Side RAG","title":"OpenAI - Assistants API","text":"Instead just passing single files Assistant, can also add vector store (files chunked indexed) use file_search tool search store retrieve relevant chunks, effectively turning server-side RAG application. useful files large pass directly model. First, quick overview available methods manage vector stores: Creating Vector Store Listing Vector Stores Adding Files Vector Store Listing Files Vector Store Deleting Vector Store Note Files vector stores shared across OpenAI APIs (Chat Completions, Responses, Assistants). file uploaded via one API can used another. First, let’s upload files want search create vector store : , define output schema response: Create assistant file_search use structured output: Note Unlike Responses API can specify vector store per-call, Assistants API attaches vector store assistant creation. store available chat calls across threads. Note Assistants API includes citation markers like 【35†source】 [3:0†source] responses using file_search. Set remove_citations = TRUE chat() automatically remove markers response text. R6 uses ‘active bindings’ define properties compute values dynamically accessed, whereas S7 uses active class methods achieve similar behavior. View citations supplementary information: Cleaning Tip deleted everything (assistant, stores used assistant, files used stores) :","code":"store <- file_assistant$create_store(     name = \"my_docs\",     file_ids = list(\"file-123\", \"file-456\") ) openai_assistant$list_stores() openai_assistant$add_file_to_store(store$id, \"file-789\") openai_assistant$list_files_in_store(store$id) openai_assistant$delete_store(store$id) r6_file_metadata <- openai_assistant$upload_file(r6_pdf_url, purpose = \"assistants\") s7_file_metadata <- openai_assistant$upload_file(s7_pdf_url, purpose = \"assistants\")  r_oop_store <- openai_assistant$create_store(     name = \"r_oop_store\",     file_ids = list(r6_file_metadata$id, s7_file_metadata$id) ) oop_output_schema <- schema(     name = \"oop_output\",     description = \"Explanation of the R6 and S7 active bindings mechanism\",     short_answer = \"string* The short answer to the question, in plain text. One sentence.\",     r6_code_example = \"string* R6 active bindings code example. Formatted as code block (```{r} ... ```).\",     s7_code_example = \"string* S7 active bindings 'equivalent' code example. Formatted as code block (```{r} ... ```).\" ) rag_assistant <- OpenAI_Assistant$new(rate_limit = 3 / 60)$create_assistant(     name = \"R OOP Expert\",     model = \"gpt-4.1\",     tools = list(         list(type = \"file_search\", store_ids = list(r_oop_store$id)),         as_tool(web_search),         as_tool(web_fetch)     ) ) res <- rag_assistant$chat(     \"What's the active bindings' R6 mechanism equivalent in S7?\",     \"Important: use both the file_search and web_search & web_fetch tools to find the information.\",     output_schema = oop_output_schema,     remove_citations = TRUE ) purrr::walk(res, \\(x) cat(x, \"\\n\\n\", sep = \"\")) library(R6) Person <- R6Class(\"Person\",   private = list(age = 30),   active = list(     age = function(value) {       if (missing(value)) private$age      # Getter       else private$age <- value            # Setter     }   ) )  p <- Person$new() p$age           # get age p$age <- 35     # set age p$age           # get new age library(S7) Person <- new_class(\"Person\",   properties = list(age = class_double),   methods = list(     age = function(self) {       self@age     # Getter equivalent     },     set_age = function(self, value) {       self@age <- value   # Setter equivalent     }   ) )  p <- Person(age = 30) p$age()          # get age p$set_age(35)    # set age p$age()          # get new age cat(yaml::as.yaml(rag_assistant$get_supplementary()), \"\\n\") citations: - type: file_citation   text: 【6:18†file216bb87f788d7f.pdf】   start_index: 1264   end_index: 1293   file_citation:     file_id: file-N3CEQUR9ttxiRpo2qm3nqS - type: file_citation   text: 【6:13†file216bb87f788d7f.pdf】   start_index: 1293   end_index: 1322   file_citation:     file_id: file-N3CEQUR9ttxiRpo2qm3nqS - type: file_citation   text: 【6:0†file216bb87f788d7f.pdf】   start_index: 1497   end_index: 1525   file_citation:     file_id: file-N3CEQUR9ttxiRpo2qm3nqS rag_assistant$delete_store(r_oop_store$id) rag_assistant$delete_file(r6_file_metadata$id) rag_assistant$delete_file(s7_file_metadata$id) rag_assistant$delete_assistant() rag_assistant$delete_assistant_and_contents()"},{"path":"https://ma-riviere.github.io/argent/articles/openai-assistants.html","id":"vector-store-management","dir":"Articles","previous_headings":"","what":"Vector Store Management","title":"OpenAI - Assistants API","text":"First, quick overview available methods manage vector stores: Creating Vector Store Listing Vector Stores Adding Files Vector Store Listing Files Vector Store Deleting Vector Store Note Files vector stores shared across OpenAI APIs (Chat Completions, Responses, Assistants). file uploaded via one API can used another.","code":"store <- file_assistant$create_store(     name = \"my_docs\",     file_ids = list(\"file-123\", \"file-456\") ) openai_assistant$list_stores() openai_assistant$add_file_to_store(store$id, \"file-789\") openai_assistant$list_files_in_store(store$id) openai_assistant$delete_store(store$id)"},{"path":"https://ma-riviere.github.io/argent/articles/openai-assistants.html","id":"basic-server-side-rag-example","dir":"Articles","previous_headings":"","what":"Basic Server-Side RAG Example","title":"OpenAI - Assistants API","text":"First, let’s upload files want search create vector store : , define output schema response: Create assistant file_search use structured output: Note Unlike Responses API can specify vector store per-call, Assistants API attaches vector store assistant creation. store available chat calls across threads. Note Assistants API includes citation markers like 【35†source】 [3:0†source] responses using file_search. Set remove_citations = TRUE chat() automatically remove markers response text. R6 uses ‘active bindings’ define properties compute values dynamically accessed, whereas S7 uses active class methods achieve similar behavior. View citations supplementary information: Cleaning Tip deleted everything (assistant, stores used assistant, files used stores) :","code":"r6_file_metadata <- openai_assistant$upload_file(r6_pdf_url, purpose = \"assistants\") s7_file_metadata <- openai_assistant$upload_file(s7_pdf_url, purpose = \"assistants\")  r_oop_store <- openai_assistant$create_store(     name = \"r_oop_store\",     file_ids = list(r6_file_metadata$id, s7_file_metadata$id) ) oop_output_schema <- schema(     name = \"oop_output\",     description = \"Explanation of the R6 and S7 active bindings mechanism\",     short_answer = \"string* The short answer to the question, in plain text. One sentence.\",     r6_code_example = \"string* R6 active bindings code example. Formatted as code block (```{r} ... ```).\",     s7_code_example = \"string* S7 active bindings 'equivalent' code example. Formatted as code block (```{r} ... ```).\" ) rag_assistant <- OpenAI_Assistant$new(rate_limit = 3 / 60)$create_assistant(     name = \"R OOP Expert\",     model = \"gpt-4.1\",     tools = list(         list(type = \"file_search\", store_ids = list(r_oop_store$id)),         as_tool(web_search),         as_tool(web_fetch)     ) ) res <- rag_assistant$chat(     \"What's the active bindings' R6 mechanism equivalent in S7?\",     \"Important: use both the file_search and web_search & web_fetch tools to find the information.\",     output_schema = oop_output_schema,     remove_citations = TRUE ) purrr::walk(res, \\(x) cat(x, \"\\n\\n\", sep = \"\")) library(R6) Person <- R6Class(\"Person\",   private = list(age = 30),   active = list(     age = function(value) {       if (missing(value)) private$age      # Getter       else private$age <- value            # Setter     }   ) )  p <- Person$new() p$age           # get age p$age <- 35     # set age p$age           # get new age library(S7) Person <- new_class(\"Person\",   properties = list(age = class_double),   methods = list(     age = function(self) {       self@age     # Getter equivalent     },     set_age = function(self, value) {       self@age <- value   # Setter equivalent     }   ) )  p <- Person(age = 30) p$age()          # get age p$set_age(35)    # set age p$age()          # get new age cat(yaml::as.yaml(rag_assistant$get_supplementary()), \"\\n\") citations: - type: file_citation   text: 【6:18†file216bb87f788d7f.pdf】   start_index: 1264   end_index: 1293   file_citation:     file_id: file-N3CEQUR9ttxiRpo2qm3nqS - type: file_citation   text: 【6:13†file216bb87f788d7f.pdf】   start_index: 1293   end_index: 1322   file_citation:     file_id: file-N3CEQUR9ttxiRpo2qm3nqS - type: file_citation   text: 【6:0†file216bb87f788d7f.pdf】   start_index: 1497   end_index: 1525   file_citation:     file_id: file-N3CEQUR9ttxiRpo2qm3nqS rag_assistant$delete_store(r_oop_store$id) rag_assistant$delete_file(r6_file_metadata$id) rag_assistant$delete_file(s7_file_metadata$id) rag_assistant$delete_assistant() rag_assistant$delete_assistant_and_contents()"},{"path":"https://ma-riviere.github.io/argent/articles/openai-completions.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"OpenAI - Chat Completions API","text":"article covers using OpenAI’s Chat Completions API argent. standard OpenAI API supports tool calling, structured outputs, reasoning o1/GPT-5 models.","code":""},{"path":"https://ma-riviere.github.io/argent/articles/openai-completions.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"OpenAI - Chat Completions API","text":"","code":"openai_chat <- OpenAI_Chat$new(api_key = Sys.getenv(\"OPENAI_API_KEY\")) mirai::daemons(4) # To enable parallel tool calling"},{"path":"https://ma-riviere.github.io/argent/articles/openai-completions.html","id":"discovering-models","dir":"Articles","previous_headings":"","what":"Discovering Models","title":"OpenAI - Chat Completions API","text":"","code":"openai_chat$list_models() |>     dplyr::filter(stringr::str_detect(id, \"-5-\"))"},{"path":"https://ma-riviere.github.io/argent/articles/openai-completions.html","id":"basic-completion","dir":"Articles","previous_headings":"","what":"Basic Completion","title":"OpenAI - Chat Completions API","text":"","code":"openai_chat$chat(     \"What's the R programming language? Answer in three sentences.\",     model = \"gpt-5-mini\" )"},{"path":"https://ma-riviere.github.io/argent/articles/openai-completions.html","id":"tool-calling-structured-output-reasoning","dir":"Articles","previous_headings":"","what":"Tool Calling + Structured Output + Reasoning","title":"OpenAI - Chat Completions API","text":"First, define web-related tools (search, crawl, fetch, general-use tool) bundled web_tools list: Search: Fetch: , let’s define JSON schema structured output using schema(): Note Chat Completions API support extracting reasoning content.","code":"web_search <- function(query) {     #' @description Search the web for information using Tavily API. Returns a JSON array of search results with titles, URLs, and content snippets. Use this when you need current information, facts, news, or any data not in your training data.     #' @param query:string* The search query string. Be specific and use keywords that will yield the most relevant results.          return(web_search_tavily(query)) }  web_search_tavily <- function(query) {     res <- httr2::request(\"https://api.tavily.com/search\") |>          httr2::req_body_json(list(             query = query,             search_depth = \"basic\",             include_answer = FALSE,             max_results = 10,             api_key = Sys.getenv(\"TAVILY_API_KEY\")         )) |>          httr2::req_error(is_error = \\(resp) FALSE) |>          httr2::req_throttle(rate = 20/60, realm = \"tavily\") |>          httr2::req_perform() |>          httr2::resp_body_json() |>          purrr::discard_at(c(\"response_time\", \"follow_up_questions\", \"images\"))      results <- purrr::map(res$results, \\(x) purrr::discard_at(x, \"raw_content\"))      return(jsonlite::toJSON(results, pretty = FALSE, auto_unbox = TRUE)) } web_fetch <- function(url) {     #' @description Fetch and extract the main text content from a web page as clean markdown. Returns the page content with formatting preserved, stripped of navigation, ads, and boilerplate. Use this to read articles, documentation, blog posts, or any web page content.     #' @param url:string* The complete URL of the web page to fetch (e.g., \"https://example.com/article\"). Must be a valid HTTP/HTTPS URL.          trafilatura_installed <- tryCatch({         system(\"which trafilatura\", intern = TRUE, ignore.stderr = TRUE)         return(TRUE)     },     warning = function(e) {         cli::cli_alert_warning(\"trafilatura is not installed. Install with: {.code pip install trafilatura}\")         return(FALSE)     })      if (trafilatura_installed) {         res <- web_fetch_trafilatura(url)          could_not_fetch <- c(             \"Impossible to fetch the contents of this web page\",             \"Please reload this page\",             \"There was an error while loading\",             \"404\"         )         if (is.null(res) || is.na(res) || nchar(res) == 0 ||             any(stringr::str_detect(res, stringr::fixed(could_not_fetch, ignore_case = TRUE)))) {             return(web_fetch_rvest(url))         }         return(res)     }      return(web_fetch_rvest(url)) }  web_fetch_trafilatura <- function(url) {     # pip install trafilatura     tryCatch({         res <- paste0(\"trafilatura -u \", url, \" --markdown --no-comments --links \") |>              system(intern = TRUE) |>              purrr::keep(nzchar) |>             paste0(collapse = \"\\n\")                  return(res)     },     error = function(e) {         return(\"Impossible to fetch the contents of this web page. It might not allow scraping\")     }) }  web_fetch_rvest <- function(url) {     tags_to_ignore <- c(         \"a\", \"script\", \"code\", \"img\", \"svg\", \"footer\", \"g\", \"path\", \"polygon\", \"label\", \"button\", \"form\", \"input\", \"select\",          \"style\", \"link\", \"meta\", \"noscript\", \"iframe\", \"embed\", \"object\", \"param\", \"video\", \"audio\", \"track\", \"source\",          \"canvas\", \"map\", \"area\", \"math\", \"col\", \"colgroup\", \"dl\", \"dt\", \"dd\", \"hr\", \"pre\", \"address\", \"figure\", \"figcaption\",         \"dfn\", \"em\", \"kbd\", \"samp\", \"var\", \"del\", \"ins\", \"mark\", \"circle\"     )      remove_tags <- function(xml, tags) {         purrr::walk(tags, \\(tag) purrr::walk(xml2::xml_find_all(xml, paste0(\".//\", tag)), \\(node) xml2::xml_remove(node)))         return(xml)     }      cleaned_contents <- tryCatch(         rvest::read_html(url)         |> rvest::html_element(\"body\")         |> remove_tags(tags_to_ignore)         |> rvest::html_children()         |> rvest::html_text2()         |> purrr::discard(\\(x) x == \"\")         |> paste0(collapse = \"\\n\\n\"),         error = \\(e) return(\"\")     )     return(cleaned_contents) } package_info_schema <- schema(     name = \"package_info\",     description = \"Information about an R package release\",     release_version = \"string* The release version of the package\",     release_date = \"string* The release date of the `release_version`\" ) openai_chat$chat(     \"When was the first release of the R 'ellmer' package on GitHub?\",     model = \"gpt-5-mini\",     reasoning_effort = \"low\",     tools = list(as_tool(web_search), as_tool(web_fetch)),     output_schema = package_info_schema ) $release_version [1] \"v0.1.0\"  $release_date [1] \"2025-01-09T18:06:27Z\""},{"path":"https://ma-riviere.github.io/argent/articles/openai-completions.html","id":"server-side-tools","dir":"Articles","previous_headings":"","what":"Server-side Tools","title":"OpenAI - Chat Completions API","text":"Server-side tools tools can call without define . run provider’s server. OpenAI Chat Completions API supports one server-side tool: web_search OpenAI provides specialized search models web search capabilities Chat Completions API: gpt-5-search-api gpt-4o-search-preview gpt-4o-mini-search-preview Basic usage: search-preview models automatically use web search Advanced usage: can pass additional options web search tool Cost: Web searches incur $25.00/1k calls plus token costs. Note Search models support standard sampling parameters (temperature, top_p, frequency_penalty, presence_penalty, n). automatically omitted using search models. Extracting annotations: can extract annotations openai_chat$get_supplementary() print(openai_chat, show_supplementary = TRUE). flexible server-side tools, use Responses API instead.","code":"openai_chat$chat(     \"What's the latest version of the R 'ellmer' package?\",     model = \"gpt-4o-mini-search-preview\" ) openai_chat$chat(     \"What are the best restaurants near me?\",     model = \"gpt-4o-mini-search-preview\",     tools = list(list(         type = \"web_search\",         user_location = list(             type = \"approximate\",             approximate = list(                 country = \"NO\",                 city = \"Trondheim\",                 timezone = \"Europe/Oslo\"             )         ),         search_context_size = \"medium\"  # \"low\", \"medium\", or \"high\"     )) ) cat(yaml::as.yaml(openai_chat$get_supplementary())) annotations: - type: url_citation   url_citation:     end_index: 284     start_index: 196     title: Fagn     url: https://www.google.com/maps/search/Fagn%2C+Trondheim%2C+Norway?utm_source=openai - type: url_citation   url_citation:     end_index: 584     start_index: 484     title: Speilsalen     url: https://www.google.com/maps/search/Speilsalen%2C+Trondheim%2C+Norway?utm_source=openai - type: url_citation   url_citation:     end_index: 909     start_index: 819     title: Credo     url: https://www.google.com/maps/search/Credo%2C+Trondheim%2C+Norway?utm_source=openai - type: url_citation   url_citation:     end_index: 1241     start_index: 1122     title: To Rom og Kjøkken     url: https://www.google.com/maps/search/To+Rom+og+Kj%C3%B8kken%2C+Trondheim%2C+Norway?utm_source=openai - type: url_citation   url_citation:     end_index: 1562     start_index: 1427     title: Havfruen Sjømatrestaurant     url: https://www.google.com/maps/search/Havfruen+Sj%C3%B8matrestaurant%2C+Trondheim%2C+Norway?utm_source=openai - type: url_citation   url_citation:     end_index: 1837     start_index: 1729     title: Bula Neobistro     url: https://www.google.com/maps/search/Bula+Neobistro%2C+Trondheim%2C+Norway?utm_source=openai - type: url_citation   url_citation:     end_index: 2111     start_index: 1989     title: Vertshuset Grenaderen     url: https://www.google.com/maps/search/Vertshuset+Grenaderen%2C+Trondheim%2C+Norway?utm_source=openai - type: url_citation   url_citation:     end_index: 2805     start_index: 2686     title: Top 10 Best Restaurants to Visit in Trondheim | Norway     url: https://www.youtube.com/watch?v=0J6xfdlb3I4&utm_source=openai"},{"path":"https://ma-riviere.github.io/argent/articles/openai-completions.html","id":"server-side-web-search","dir":"Articles","previous_headings":"","what":"Server-side: Web Search","title":"OpenAI - Chat Completions API","text":"OpenAI provides specialized search models web search capabilities Chat Completions API: gpt-5-search-api gpt-4o-search-preview gpt-4o-mini-search-preview Basic usage: search-preview models automatically use web search Advanced usage: can pass additional options web search tool Cost: Web searches incur $25.00/1k calls plus token costs. Note Search models support standard sampling parameters (temperature, top_p, frequency_penalty, presence_penalty, n). automatically omitted using search models. Extracting annotations: can extract annotations openai_chat$get_supplementary() print(openai_chat, show_supplementary = TRUE). flexible server-side tools, use Responses API instead.","code":"openai_chat$chat(     \"What's the latest version of the R 'ellmer' package?\",     model = \"gpt-4o-mini-search-preview\" ) openai_chat$chat(     \"What are the best restaurants near me?\",     model = \"gpt-4o-mini-search-preview\",     tools = list(list(         type = \"web_search\",         user_location = list(             type = \"approximate\",             approximate = list(                 country = \"NO\",                 city = \"Trondheim\",                 timezone = \"Europe/Oslo\"             )         ),         search_context_size = \"medium\"  # \"low\", \"medium\", or \"high\"     )) ) cat(yaml::as.yaml(openai_chat$get_supplementary())) annotations: - type: url_citation   url_citation:     end_index: 284     start_index: 196     title: Fagn     url: https://www.google.com/maps/search/Fagn%2C+Trondheim%2C+Norway?utm_source=openai - type: url_citation   url_citation:     end_index: 584     start_index: 484     title: Speilsalen     url: https://www.google.com/maps/search/Speilsalen%2C+Trondheim%2C+Norway?utm_source=openai - type: url_citation   url_citation:     end_index: 909     start_index: 819     title: Credo     url: https://www.google.com/maps/search/Credo%2C+Trondheim%2C+Norway?utm_source=openai - type: url_citation   url_citation:     end_index: 1241     start_index: 1122     title: To Rom og Kjøkken     url: https://www.google.com/maps/search/To+Rom+og+Kj%C3%B8kken%2C+Trondheim%2C+Norway?utm_source=openai - type: url_citation   url_citation:     end_index: 1562     start_index: 1427     title: Havfruen Sjømatrestaurant     url: https://www.google.com/maps/search/Havfruen+Sj%C3%B8matrestaurant%2C+Trondheim%2C+Norway?utm_source=openai - type: url_citation   url_citation:     end_index: 1837     start_index: 1729     title: Bula Neobistro     url: https://www.google.com/maps/search/Bula+Neobistro%2C+Trondheim%2C+Norway?utm_source=openai - type: url_citation   url_citation:     end_index: 2111     start_index: 1989     title: Vertshuset Grenaderen     url: https://www.google.com/maps/search/Vertshuset+Grenaderen%2C+Trondheim%2C+Norway?utm_source=openai - type: url_citation   url_citation:     end_index: 2805     start_index: 2686     title: Top 10 Best Restaurants to Visit in Trondheim | Norway     url: https://www.youtube.com/watch?v=0J6xfdlb3I4&utm_source=openai"},{"path":"https://ma-riviere.github.io/argent/articles/openai-completions.html","id":"multimodal-inputs","dir":"Articles","previous_headings":"","what":"Multimodal Inputs","title":"OpenAI - Chat Completions API","text":"OpenAI Responses API supports sending: Images: URLs (-, base64), files (base64) PDFs: URLs files (base64, text content) Remote files (as_file_content()) Plain text & code files Text-based data files (csv, tsv, json, ..) R objects Sending local image: providing path local image, automatically converted base64 sent server. say ! Sending image URL: Note OpenAI supports optional detail parameter (low/high/auto) control image processing detail level image sent URL. default value \"auto\". specify parameter, need pass image URL as_image_content(url, .provider_options = list(detail = \"low\")). used as_image_content() resize image sending .resize argument. OpenAI Chat Completions API also supports sending file_id (.e. image as_file_content(file_id) uploading openai_chat$upload_file()). Sending local PDF: Damn right! Sending PDF URLs: OpenAI Chat Completions, default, PDF URLs sent base64 server. However, can use as_text_content() helper pdftools::pdf_convert() parse PDFs pass text contents model instead. can pass R object chat() : can also send files directly model. Uploading Files Listing Files Using Uploaded Files Use as_file_content() reference uploaded files: Downloading Files Deleting Files","code":"bsg04_cast_image_url <- \"https://upload.wikimedia.org/wikipedia/en/1/1a/Battlestar_Galactica_%282004%29_cast.jpg\" bsg04_cast_image_path <- download_temp_file(bsg04_cast_image_url) openai_chat$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_path,     model = \"gpt-5-mini\" ) This is a promotional shot of the main characters from the reimagined TV series Battlestar Galactica (2004). From left to right the characters are:  - Admiral William Adama (commander of the battlestar) - President Laura Roslin - Captain Lee \"Apollo\" Adama - Kara \"Starbuck\" Thrace - Number Six (the Cylon model) - Dr. Gaius Baltar (with Number Six) - Sharon \"Boomer\" Valerii (a Cylon sleeper/Raptor pilot)  (If you want, I can point out who plays each character.) openai_chat$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_url,     model = \"gpt-5-mini\" ) my_cv_url <- \"https://ma-riviere.com/res/cv.pdf\" my_cv_pdf_path <- download_temp_file(my_cv_url) openai_chat$chat(     \"What's my favorite programming language?\",     my_cv_pdf_path,     model = \"gpt-5-mini\" ) Based on the resume you provided (page 1), your favorite programming language appears to be R — the header says \"R Programming | Data Science | Neuroscience,\" you list R first under Programming Skills, and you have a strong focus on R & Shiny in your work experience. If that’s not right, tell me which source I should check or confirm your actual preference. r6_pdf_url <- \"https://cran.r-project.org/web/packages/R6/R6.pdf\" s7_pdf_url <- \"https://cran.r-project.org/web/packages/S7/S7.pdf\"  multimodal_prompt <- list(     \"Give a 3 sentences summary of the advantages of S7 over R6\",     as_text_content(r6_pdf_url),     as_text_content(s7_pdf_url),     \"And give me the current versions of both packages\" # To make sure it actually reads the PDFs )  openai_chat$chat(!!!multimodal_prompt, model = \"gpt-5-mini\") lm_obj <- lm(body_mass ~ species + sex, data = datasets::penguins)  openai_chat$chat(     \"What can we deduct from this regression model?\",     lm_obj,     model = \"gpt-5-mini\" ) 1) Estimated effects (relative to the reference group Adelie & female):      - Intercept = 3372.4 g → predicted body mass for an Adelie female ≈ 3372 g.      - speciesChinstrap = +26.9 g → Chinstrap females ≈ 27 g heavier than Adelie females (practically negligible).      - speciesGentoo = +1377.9 g → Gentoo females ≈ 1378 g heavier than Adelie females (large difference).      - sexmale = +667.6 g → males ≈ 668 g heavier than females (same additive effect across species).  2) Predicted group means (by combination):      - Adelie female ≈ 3372 g; Adelie male ≈ 3372.4 + 667.6 ≈ 4040 g.      - Chinstrap female ≈ 3372.4 + 26.9 ≈ 3399 g; Chinstrap male ≈ 3399 + 667.6 ≈ 4067 g.      - Gentoo female ≈ 3372.4 + 1377.9 ≈ 4750 g; Gentoo male ≈ 4750 + 667.6 ≈ 5418 g.  3) Model structure and caveats:      - This is an additive linear model (body_mass ~ species + sex) using treatment contrasts (Adelie & female as baselines); sex effect is assumed equal across species (no interaction included).      - There are 333 observations (11 rows omitted), rank = 4, df.residual = 329.      - The output quoted does not show standard errors, p-values, R² or diagnostic statistics, so you cannot formally assess statistical significance or overall fit from what’s shown — run summary(lm_obj) and diagnostic plots to check significance and model assumptions. file_metadata <- openai_chat$upload_file(my_cv_url) openai_chat$list_files() # A tibble: 1 × 9   object id                          purpose   filename               bytes created_at          expires_at status    status_details   <chr>  <chr>                       <chr>     <chr>                  <int> <dttm>              <lgl>      <chr>     <lgl>          1 file   file-5LtQyoh6ZdtK71TpZdTxSn user_data file1cc931188f797.pdf 431965 2025-11-11 21:49:14 NA         processed NA openai_chat$chat(     \"What are my two favorite frameworks/tools ?\",     as_file_content(file_metadata$id),     model = \"gpt-5-mini\" ) On your CV (Frameworks & Tools section, right column on page 1) the top two listed are: - Shiny - Scientific publishing tools — Quarto / R Markdown  So your two favorite frameworks/tools appear to be Shiny and Quarto (R Markdown). openai_chat$download_file(file_metadata$id, dest_path = \"data\")  # Downloads to data/ by default #> ✔ [OpenAI] File downloaded to: data/file1cc931188f797.pdf openai_chat$delete_file(file_metadata$id) #> ✔ [OpenAI] File deleted: file-5LtQyoh6ZdtK71TpZdTxSn"},{"path":"https://ma-riviere.github.io/argent/articles/openai-completions.html","id":"image-comprehension","dir":"Articles","previous_headings":"","what":"Image Comprehension","title":"OpenAI - Chat Completions API","text":"Sending local image: providing path local image, automatically converted base64 sent server. say ! Sending image URL: Note OpenAI supports optional detail parameter (low/high/auto) control image processing detail level image sent URL. default value \"auto\". specify parameter, need pass image URL as_image_content(url, .provider_options = list(detail = \"low\")). used as_image_content() resize image sending .resize argument. OpenAI Chat Completions API also supports sending file_id (.e. image as_file_content(file_id) uploading openai_chat$upload_file()).","code":"bsg04_cast_image_url <- \"https://upload.wikimedia.org/wikipedia/en/1/1a/Battlestar_Galactica_%282004%29_cast.jpg\" bsg04_cast_image_path <- download_temp_file(bsg04_cast_image_url) openai_chat$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_path,     model = \"gpt-5-mini\" ) This is a promotional shot of the main characters from the reimagined TV series Battlestar Galactica (2004). From left to right the characters are:  - Admiral William Adama (commander of the battlestar) - President Laura Roslin - Captain Lee \"Apollo\" Adama - Kara \"Starbuck\" Thrace - Number Six (the Cylon model) - Dr. Gaius Baltar (with Number Six) - Sharon \"Boomer\" Valerii (a Cylon sleeper/Raptor pilot)  (If you want, I can point out who plays each character.) openai_chat$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_url,     model = \"gpt-5-mini\" )"},{"path":"https://ma-riviere.github.io/argent/articles/openai-completions.html","id":"pdf-comprehension","dir":"Articles","previous_headings":"","what":"PDF Comprehension","title":"OpenAI - Chat Completions API","text":"Sending local PDF: Damn right! Sending PDF URLs: OpenAI Chat Completions, default, PDF URLs sent base64 server. However, can use as_text_content() helper pdftools::pdf_convert() parse PDFs pass text contents model instead.","code":"my_cv_url <- \"https://ma-riviere.com/res/cv.pdf\" my_cv_pdf_path <- download_temp_file(my_cv_url) openai_chat$chat(     \"What's my favorite programming language?\",     my_cv_pdf_path,     model = \"gpt-5-mini\" ) Based on the resume you provided (page 1), your favorite programming language appears to be R — the header says \"R Programming | Data Science | Neuroscience,\" you list R first under Programming Skills, and you have a strong focus on R & Shiny in your work experience. If that’s not right, tell me which source I should check or confirm your actual preference. r6_pdf_url <- \"https://cran.r-project.org/web/packages/R6/R6.pdf\" s7_pdf_url <- \"https://cran.r-project.org/web/packages/S7/S7.pdf\"  multimodal_prompt <- list(     \"Give a 3 sentences summary of the advantages of S7 over R6\",     as_text_content(r6_pdf_url),     as_text_content(s7_pdf_url),     \"And give me the current versions of both packages\" # To make sure it actually reads the PDFs )  openai_chat$chat(!!!multimodal_prompt, model = \"gpt-5-mini\")"},{"path":"https://ma-riviere.github.io/argent/articles/openai-completions.html","id":"passing-r-objects","dir":"Articles","previous_headings":"","what":"Passing R Objects","title":"OpenAI - Chat Completions API","text":"can pass R object chat() :","code":"lm_obj <- lm(body_mass ~ species + sex, data = datasets::penguins)  openai_chat$chat(     \"What can we deduct from this regression model?\",     lm_obj,     model = \"gpt-5-mini\" ) 1) Estimated effects (relative to the reference group Adelie & female):      - Intercept = 3372.4 g → predicted body mass for an Adelie female ≈ 3372 g.      - speciesChinstrap = +26.9 g → Chinstrap females ≈ 27 g heavier than Adelie females (practically negligible).      - speciesGentoo = +1377.9 g → Gentoo females ≈ 1378 g heavier than Adelie females (large difference).      - sexmale = +667.6 g → males ≈ 668 g heavier than females (same additive effect across species).  2) Predicted group means (by combination):      - Adelie female ≈ 3372 g; Adelie male ≈ 3372.4 + 667.6 ≈ 4040 g.      - Chinstrap female ≈ 3372.4 + 26.9 ≈ 3399 g; Chinstrap male ≈ 3399 + 667.6 ≈ 4067 g.      - Gentoo female ≈ 3372.4 + 1377.9 ≈ 4750 g; Gentoo male ≈ 4750 + 667.6 ≈ 5418 g.  3) Model structure and caveats:      - This is an additive linear model (body_mass ~ species + sex) using treatment contrasts (Adelie & female as baselines); sex effect is assumed equal across species (no interaction included).      - There are 333 observations (11 rows omitted), rank = 4, df.residual = 329.      - The output quoted does not show standard errors, p-values, R² or diagnostic statistics, so you cannot formally assess statistical significance or overall fit from what’s shown — run summary(lm_obj) and diagnostic plots to check significance and model assumptions."},{"path":"https://ma-riviere.github.io/argent/articles/openai-completions.html","id":"sending-files","dir":"Articles","previous_headings":"","what":"Sending files","title":"OpenAI - Chat Completions API","text":"can also send files directly model. Uploading Files Listing Files Using Uploaded Files Use as_file_content() reference uploaded files: Downloading Files Deleting Files","code":"file_metadata <- openai_chat$upload_file(my_cv_url) openai_chat$list_files() # A tibble: 1 × 9   object id                          purpose   filename               bytes created_at          expires_at status    status_details   <chr>  <chr>                       <chr>     <chr>                  <int> <dttm>              <lgl>      <chr>     <lgl>          1 file   file-5LtQyoh6ZdtK71TpZdTxSn user_data file1cc931188f797.pdf 431965 2025-11-11 21:49:14 NA         processed NA openai_chat$chat(     \"What are my two favorite frameworks/tools ?\",     as_file_content(file_metadata$id),     model = \"gpt-5-mini\" ) On your CV (Frameworks & Tools section, right column on page 1) the top two listed are: - Shiny - Scientific publishing tools — Quarto / R Markdown  So your two favorite frameworks/tools appear to be Shiny and Quarto (R Markdown). openai_chat$download_file(file_metadata$id, dest_path = \"data\")  # Downloads to data/ by default #> ✔ [OpenAI] File downloaded to: data/file1cc931188f797.pdf openai_chat$delete_file(file_metadata$id) #> ✔ [OpenAI] File deleted: file-5LtQyoh6ZdtK71TpZdTxSn"},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"OpenAI - Responses API","text":"article covers using OpenAI’s Responses API argent. OpenAI’s newest advanced API combines strengths Chat Completions Assistants APIs single streamlined interface building agentic applications.","code":""},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"OpenAI - Responses API","text":"","code":"openai_responses <- OpenAI_Responses$new(api_key = Sys.getenv(\"OPENAI_API_KEY\")) mirai::daemons(4) # To enable parallel tool calling"},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"discovering-models","dir":"Articles","previous_headings":"","what":"Discovering Models","title":"OpenAI - Responses API","text":"","code":"openai_responses$list_models() |>     dplyr::filter(stringr::str_detect(id, \"-5|5.1-\"))"},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"basic-completion","dir":"Articles","previous_headings":"","what":"Basic Completion","title":"OpenAI - Responses API","text":"","code":"openai_responses$chat(     \"What's the R programming language? Answer in three sentences.\",     model = \"gpt-5-mini\" )"},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"tool-calling-structured-output-reasoning","dir":"Articles","previous_headings":"","what":"Tool Calling + Structured Output + Reasoning","title":"OpenAI - Responses API","text":"First, define web-related tools (search, crawl, fetch, general-use tool) bundled web_tools list: Search: Fetch: , let’s define JSON schema structured output using schema(): , run agent: model keep calling tools enough information answer question. Unlike Chat Completions, Responses API supports extracting reasoning content: Alternatively, simply print(openai_responses) see reasoning answers’ text console, turn turn. Note get_reasoning_text() get_content_text() use last API response (get_last_response()) default. Tip get model’s reasoning responses API, need set reasoning_effort reasoning_summary parameters. However, reasoning_summary requires verify identity organization OpenAI.","code":"web_search <- function(query) {     #' @description Search the web for information using Tavily API. Returns a JSON array of search results with titles, URLs, and content snippets. Use this when you need current information, facts, news, or any data not in your training data.     #' @param query:string* The search query string. Be specific and use keywords that will yield the most relevant results.          return(web_search_tavily(query)) }  web_search_tavily <- function(query) {     res <- httr2::request(\"https://api.tavily.com/search\") |>          httr2::req_body_json(list(             query = query,             search_depth = \"basic\",             include_answer = FALSE,             max_results = 10,             api_key = Sys.getenv(\"TAVILY_API_KEY\")         )) |>          httr2::req_error(is_error = \\(resp) FALSE) |>          httr2::req_throttle(rate = 20/60, realm = \"tavily\") |>          httr2::req_perform() |>          httr2::resp_body_json() |>          purrr::discard_at(c(\"response_time\", \"follow_up_questions\", \"images\"))      results <- purrr::map(res$results, \\(x) purrr::discard_at(x, \"raw_content\"))      return(jsonlite::toJSON(results, pretty = FALSE, auto_unbox = TRUE)) } web_fetch <- function(url) {     #' @description Fetch and extract the main text content from a web page as clean markdown. Returns the page content with formatting preserved, stripped of navigation, ads, and boilerplate. Use this to read articles, documentation, blog posts, or any web page content.     #' @param url:string* The complete URL of the web page to fetch (e.g., \"https://example.com/article\"). Must be a valid HTTP/HTTPS URL.          trafilatura_installed <- tryCatch({         system(\"which trafilatura\", intern = TRUE, ignore.stderr = TRUE)         return(TRUE)     },     warning = function(e) {         cli::cli_alert_warning(\"trafilatura is not installed. Install with: {.code pip install trafilatura}\")         return(FALSE)     })      if (trafilatura_installed) {         res <- web_fetch_trafilatura(url)          could_not_fetch <- c(             \"Impossible to fetch the contents of this web page\",             \"Please reload this page\",             \"There was an error while loading\",             \"404\"         )         if (is.null(res) || is.na(res) || nchar(res) == 0 ||             any(stringr::str_detect(res, stringr::fixed(could_not_fetch, ignore_case = TRUE)))) {             return(web_fetch_rvest(url))         }         return(res)     }      return(web_fetch_rvest(url)) }  web_fetch_trafilatura <- function(url) {     # pip install trafilatura     tryCatch({         res <- paste0(\"trafilatura -u \", url, \" --markdown --no-comments --links \") |>              system(intern = TRUE) |>              purrr::keep(nzchar) |>             paste0(collapse = \"\\n\")                  return(res)     },     error = function(e) {         return(\"Impossible to fetch the contents of this web page. It might not allow scraping\")     }) }  web_fetch_rvest <- function(url) {     tags_to_ignore <- c(         \"a\", \"script\", \"code\", \"img\", \"svg\", \"footer\", \"g\", \"path\", \"polygon\", \"label\", \"button\", \"form\", \"input\", \"select\",          \"style\", \"link\", \"meta\", \"noscript\", \"iframe\", \"embed\", \"object\", \"param\", \"video\", \"audio\", \"track\", \"source\",          \"canvas\", \"map\", \"area\", \"math\", \"col\", \"colgroup\", \"dl\", \"dt\", \"dd\", \"hr\", \"pre\", \"address\", \"figure\", \"figcaption\",         \"dfn\", \"em\", \"kbd\", \"samp\", \"var\", \"del\", \"ins\", \"mark\", \"circle\"     )      remove_tags <- function(xml, tags) {         purrr::walk(tags, \\(tag) purrr::walk(xml2::xml_find_all(xml, paste0(\".//\", tag)), \\(node) xml2::xml_remove(node)))         return(xml)     }      cleaned_contents <- tryCatch(         rvest::read_html(url)         |> rvest::html_element(\"body\")         |> remove_tags(tags_to_ignore)         |> rvest::html_children()         |> rvest::html_text2()         |> purrr::discard(\\(x) x == \"\")         |> paste0(collapse = \"\\n\\n\"),         error = \\(e) return(\"\")     )     return(cleaned_contents) } package_info_schema <- schema(     name = \"package_info\",     description = \"Information about an R package release\",     release_version = \"string* The release version of the package\",     release_date = \"string* The release date of the `release_version`\" ) openai_responses$chat(     \"When was the first release of the R 'ellmer' package on GitHub?\",     model = \"gpt-5-mini\",     reasoning_effort = \"medium\",     tools = list(as_tool(web_search), as_tool(web_fetch)),     output_schema = package_info_schema ) $release_version [1] \"0.1.0\"  $release_date [1] \"2025-01-09T18:06:27Z\" cat(openai_responses$get_reasoning_text())"},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"extracting-reasoning","dir":"Articles","previous_headings":"","what":"Extracting Reasoning","title":"OpenAI - Responses API","text":"Unlike Chat Completions, Responses API supports extracting reasoning content: Alternatively, simply print(openai_responses) see reasoning answers’ text console, turn turn. Note get_reasoning_text() get_content_text() use last API response (get_last_response()) default. Tip get model’s reasoning responses API, need set reasoning_effort reasoning_summary parameters. However, reasoning_summary requires verify identity organization OpenAI.","code":"cat(openai_responses$get_reasoning_text())"},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"server-side-tools","dir":"Articles","previous_headings":"","what":"Server-side Tools","title":"OpenAI - Responses API","text":"Server-side tools tools can call without define . run provider’s server. OpenAI Responses API supports three server-side tools: web_search - Web search/fetch capabilities (citations) file_search - Search uploaded files using vector stores (.e. server-side RAG) code_interpreter - Execute Python code sandboxed containers Docs: https://platform.openai.com/docs/guides/tools-web-search?api-mode=responses Note can used model, unlike equivalent Chat Completions API. can check searches made openai_responses$get_supplementary() print(openai_responses, show_supplementary = TRUE). See RAG section complete example using file_search tool. code_interpreter tool provides sandboxed Python execution containers. Containers sandboxed virtual machines code_interpreter can execute Python code. Cost: $0.03 per container Duration: 1 hour active, 20 minute idle timeout Auto-creation: Containers created automatically using tool Example: Analyzing Penguins Dataset Inspect generated code: Download generated files: Continue asking questions context: List containers: Get container information: List files container: Download specific file container: Delete container:","code":"openai_responses$chat(     \"What's the latest version of the R 'ellmer' package?\",     model = \"gpt-5-mini\",     tools = list(\"web_search\"),     output_schema = package_info_schema ) penguins_url <- \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/refs/heads/main/inst/extdata/penguins.csv\" penguins_file_metadata <- openai_responses$upload_file(penguins_url, purpose = \"assistants\")  openai_responses$chat(     \"Create a summary table showing average body_mass grouped by species, sex, and year. Save as CSV.\",     model = \"gpt-5-mini\",     tools = list(list(type = \"code_interpreter\", file_ids = list(penguins_file_metadata$id))) ) cat(openai_responses$get_generated_code(langs = c(\"python\"), as_chunks = TRUE)) # Read the uploaded CSV, compute average body_mass grouped by species, sex, and year, # then save the result to a CSV file and display the first few rows.  import pandas as pd  input_path = \"/mnt/data/file-KPrDs2i6sSr4afRoahCyyF-file541142818f949.csv\" output_path = \"/mnt/data/summary_body_mass_by_species_sex_year.csv\"  # Read CSV (infer encoding and separators) df = pd.read_csv(input_path)  # Inspect columns print(\"Columns in input file:\", list(df.columns))  # Normalize column names to lower for matching common names cols_lower = [c.lower() for c in df.columns] col_map = dict(zip(df.columns, cols_lower))  # Determine likely column names for species, sex, year, body_mass def find_col(possible_names):     for name in possible_names:         for col in df.columns:             if col.lower() == name.lower():                 return col     return None  species_col = find_col([\"species\", \"species_name\", \"speciesid\"]) sex_col = find_col([\"sex\", \"gender\"]) year_col = find_col([\"year\", \"time\", \"year_observed\"]) body_col = find_col([\"body_mass\", \"bodymass\", \"mass\", \"weight\", \"body_weight\"])  # If any not found, try fuzzy match by substring if not species_col:     for col in df.columns:         if \"species\" in col.lower():             species_col = col if not sex_col:     for col in df.columns:         if \"sex\" in col.lower():             sex_col = col if not year_col:     for col in df.columns:         if \"year\" in col.lower():             year_col = col if not body_col:     for col in df.columns:         if \"body\" in col.lower() and \"mass\" in col.lower():             body_col = col  # If still not found, raise an informative error missing = [] if not species_col: missing.append(\"species\") if not sex_col: missing.append(\"sex\") if not year_col: missing.append(\"year\") if not body_col: missing.append(\"body_mass\")  if missing:     raise ValueError(f\"Could not find columns for: {', '.join(missing)}. Columns present: {list(df.columns)}\")  # Create a working DataFrame with selected columns work = df[[species_col, sex_col, year_col, body_col]].copy() work.columns = [\"species\", \"sex\", \"year\", \"body_mass\"]  # Convert body_mass to numeric (coerce errors), drop NA body_mass work[\"body_mass\"] = pd.to_numeric(work[\"body_mass\"], errors=\"coerce\") before_drop = len(work) work = work.dropna(subset=[\"body_mass\"]) dropped = before_drop - len(work)  # Group and compute mean summary = (     work.groupby([\"species\", \"sex\", \"year\"], dropna=False)     .agg(avg_body_mass=(\"body_mass\", \"mean\"), count=(\"body_mass\",\"size\"))     .reset_index() )  # Round average body mass to 3 decimal places summary[\"avg_body_mass\"] = summary[\"avg_body_mass\"].round(3)  # Save to CSV summary.to_csv(output_path, index=False)  # Display info and first rows print(f\"Dropped {dropped} rows with missing/non-numeric body_mass.\") print(f\"Saved summary to: {output_path}\") summary.head(20) downloaded_path <- openai_responses$download_generated_files(dest_path = \"data\") #> ✔ [OpenAI Responses] Downloaded file to: data/summary_body_mass_by_species_sex_year.csv read.csv(downloaded_path, na.strings = c(\"\", \"NA\")) species    sex year avg_body_mass count 1     Adelie female 2007      3389.773    22 2     Adelie female 2008      3386.000    25 3     Adelie female 2009      3334.615    26 4     Adelie   male 2007      4038.636    22 5     Adelie   male 2008      4098.000    25 6     Adelie   male 2009      3995.192    26 7     Adelie   <NA> 2007      3540.000     5 8  Chinstrap female 2007      3569.231    13 9  Chinstrap female 2008      3472.222     9 10 Chinstrap female 2009      3522.917    12 11 Chinstrap   male 2007      3819.231    13 12 Chinstrap   male 2008      4127.778     9 13 Chinstrap   male 2009      3927.083    12 14    Gentoo female 2007      4618.750    16 15    Gentoo female 2008      4627.273    22 16    Gentoo female 2009      4786.250    20 17    Gentoo   male 2007      5552.941    17 18    Gentoo   male 2008      5410.870    23 19    Gentoo   male 2009      5510.714    21 20    Gentoo   <NA> 2007      4100.000     1 21    Gentoo   <NA> 2008      4650.000     1 22    Gentoo   <NA> 2009      4800.000     2 penguin_output_schema <- schema(     name = \"penguin_output\",     description = \"Schema for the penguin output\",     average_body_mass = \"number* The average body_mass\",     species = \"string* The species\",     sex = \"string* The sex\",     year = \"integer* The year\" )  openai_responses$chat(     \"What's the average body_mass for the Adelie females in 2009?\",     tools = list(\"code_interpreter\"), # We need to specify the tool again to continue in the same context     output_schema = penguin_output_schema ) $average_body_mass [1] 3334.615  $species [1] \"Adelie\"  $sex [1] \"female\"  $year [1] 2009 openai_responses$list_containers() container <- openai_responses$get_container(\"cntr_690fb998d170819089fc6176eaa19ab90f300882b8d201ca\") openai_responses$list_container_files(\"cntr_690fb998d170819089fc6176eaa19ab90f300882b8d201ca\") openai_responses$download_container_file(     container_id = \"cntr_690fb998d170819089fc6176eaa19ab90f300882b8d201ca\",     file_id = \"cfile_690fb9b692d88191b77427d736671fcd\",     dest_path = \"data/output.csv\" ) openai_responses$delete_container(\"cntr_690fb998d170819089fc6176eaa19ab90f300882b8d201ca\") #> ✔ [OpenAI Responses] Container deleted: cntr_690fb998d170819089fc6176eaa19ab90f300882b8d201ca"},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"server-side-web-search","dir":"Articles","previous_headings":"","what":"Server-side: Web Search","title":"OpenAI - Responses API","text":"Docs: https://platform.openai.com/docs/guides/tools-web-search?api-mode=responses Note can used model, unlike equivalent Chat Completions API. can check searches made openai_responses$get_supplementary() print(openai_responses, show_supplementary = TRUE).","code":"openai_responses$chat(     \"What's the latest version of the R 'ellmer' package?\",     model = \"gpt-5-mini\",     tools = list(\"web_search\"),     output_schema = package_info_schema )"},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"server-side-file-search","dir":"Articles","previous_headings":"","what":"Server-side: File Search","title":"OpenAI - Responses API","text":"See RAG section complete example using file_search tool.","code":""},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"server-side-code-interpreter","dir":"Articles","previous_headings":"","what":"Server-side: Code Interpreter","title":"OpenAI - Responses API","text":"code_interpreter tool provides sandboxed Python execution containers. Containers sandboxed virtual machines code_interpreter can execute Python code. Cost: $0.03 per container Duration: 1 hour active, 20 minute idle timeout Auto-creation: Containers created automatically using tool Example: Analyzing Penguins Dataset Inspect generated code: Download generated files: Continue asking questions context: List containers: Get container information: List files container: Download specific file container: Delete container:","code":"penguins_url <- \"https://raw.githubusercontent.com/allisonhorst/palmerpenguins/refs/heads/main/inst/extdata/penguins.csv\" penguins_file_metadata <- openai_responses$upload_file(penguins_url, purpose = \"assistants\")  openai_responses$chat(     \"Create a summary table showing average body_mass grouped by species, sex, and year. Save as CSV.\",     model = \"gpt-5-mini\",     tools = list(list(type = \"code_interpreter\", file_ids = list(penguins_file_metadata$id))) ) cat(openai_responses$get_generated_code(langs = c(\"python\"), as_chunks = TRUE)) # Read the uploaded CSV, compute average body_mass grouped by species, sex, and year, # then save the result to a CSV file and display the first few rows.  import pandas as pd  input_path = \"/mnt/data/file-KPrDs2i6sSr4afRoahCyyF-file541142818f949.csv\" output_path = \"/mnt/data/summary_body_mass_by_species_sex_year.csv\"  # Read CSV (infer encoding and separators) df = pd.read_csv(input_path)  # Inspect columns print(\"Columns in input file:\", list(df.columns))  # Normalize column names to lower for matching common names cols_lower = [c.lower() for c in df.columns] col_map = dict(zip(df.columns, cols_lower))  # Determine likely column names for species, sex, year, body_mass def find_col(possible_names):     for name in possible_names:         for col in df.columns:             if col.lower() == name.lower():                 return col     return None  species_col = find_col([\"species\", \"species_name\", \"speciesid\"]) sex_col = find_col([\"sex\", \"gender\"]) year_col = find_col([\"year\", \"time\", \"year_observed\"]) body_col = find_col([\"body_mass\", \"bodymass\", \"mass\", \"weight\", \"body_weight\"])  # If any not found, try fuzzy match by substring if not species_col:     for col in df.columns:         if \"species\" in col.lower():             species_col = col if not sex_col:     for col in df.columns:         if \"sex\" in col.lower():             sex_col = col if not year_col:     for col in df.columns:         if \"year\" in col.lower():             year_col = col if not body_col:     for col in df.columns:         if \"body\" in col.lower() and \"mass\" in col.lower():             body_col = col  # If still not found, raise an informative error missing = [] if not species_col: missing.append(\"species\") if not sex_col: missing.append(\"sex\") if not year_col: missing.append(\"year\") if not body_col: missing.append(\"body_mass\")  if missing:     raise ValueError(f\"Could not find columns for: {', '.join(missing)}. Columns present: {list(df.columns)}\")  # Create a working DataFrame with selected columns work = df[[species_col, sex_col, year_col, body_col]].copy() work.columns = [\"species\", \"sex\", \"year\", \"body_mass\"]  # Convert body_mass to numeric (coerce errors), drop NA body_mass work[\"body_mass\"] = pd.to_numeric(work[\"body_mass\"], errors=\"coerce\") before_drop = len(work) work = work.dropna(subset=[\"body_mass\"]) dropped = before_drop - len(work)  # Group and compute mean summary = (     work.groupby([\"species\", \"sex\", \"year\"], dropna=False)     .agg(avg_body_mass=(\"body_mass\", \"mean\"), count=(\"body_mass\",\"size\"))     .reset_index() )  # Round average body mass to 3 decimal places summary[\"avg_body_mass\"] = summary[\"avg_body_mass\"].round(3)  # Save to CSV summary.to_csv(output_path, index=False)  # Display info and first rows print(f\"Dropped {dropped} rows with missing/non-numeric body_mass.\") print(f\"Saved summary to: {output_path}\") summary.head(20) downloaded_path <- openai_responses$download_generated_files(dest_path = \"data\") #> ✔ [OpenAI Responses] Downloaded file to: data/summary_body_mass_by_species_sex_year.csv read.csv(downloaded_path, na.strings = c(\"\", \"NA\")) species    sex year avg_body_mass count 1     Adelie female 2007      3389.773    22 2     Adelie female 2008      3386.000    25 3     Adelie female 2009      3334.615    26 4     Adelie   male 2007      4038.636    22 5     Adelie   male 2008      4098.000    25 6     Adelie   male 2009      3995.192    26 7     Adelie   <NA> 2007      3540.000     5 8  Chinstrap female 2007      3569.231    13 9  Chinstrap female 2008      3472.222     9 10 Chinstrap female 2009      3522.917    12 11 Chinstrap   male 2007      3819.231    13 12 Chinstrap   male 2008      4127.778     9 13 Chinstrap   male 2009      3927.083    12 14    Gentoo female 2007      4618.750    16 15    Gentoo female 2008      4627.273    22 16    Gentoo female 2009      4786.250    20 17    Gentoo   male 2007      5552.941    17 18    Gentoo   male 2008      5410.870    23 19    Gentoo   male 2009      5510.714    21 20    Gentoo   <NA> 2007      4100.000     1 21    Gentoo   <NA> 2008      4650.000     1 22    Gentoo   <NA> 2009      4800.000     2 penguin_output_schema <- schema(     name = \"penguin_output\",     description = \"Schema for the penguin output\",     average_body_mass = \"number* The average body_mass\",     species = \"string* The species\",     sex = \"string* The sex\",     year = \"integer* The year\" )  openai_responses$chat(     \"What's the average body_mass for the Adelie females in 2009?\",     tools = list(\"code_interpreter\"), # We need to specify the tool again to continue in the same context     output_schema = penguin_output_schema ) $average_body_mass [1] 3334.615  $species [1] \"Adelie\"  $sex [1] \"female\"  $year [1] 2009 openai_responses$list_containers() container <- openai_responses$get_container(\"cntr_690fb998d170819089fc6176eaa19ab90f300882b8d201ca\") openai_responses$list_container_files(\"cntr_690fb998d170819089fc6176eaa19ab90f300882b8d201ca\") openai_responses$download_container_file(     container_id = \"cntr_690fb998d170819089fc6176eaa19ab90f300882b8d201ca\",     file_id = \"cfile_690fb9b692d88191b77427d736671fcd\",     dest_path = \"data/output.csv\" ) openai_responses$delete_container(\"cntr_690fb998d170819089fc6176eaa19ab90f300882b8d201ca\") #> ✔ [OpenAI Responses] Container deleted: cntr_690fb998d170819089fc6176eaa19ab90f300882b8d201ca"},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"container-management","dir":"Articles","previous_headings":"Server-side Tools","what":"Container Management","title":"OpenAI - Responses API","text":"List containers: Get container information: List files container: Download specific file container: Delete container:","code":"openai_responses$list_containers() container <- openai_responses$get_container(\"cntr_690fb998d170819089fc6176eaa19ab90f300882b8d201ca\") openai_responses$list_container_files(\"cntr_690fb998d170819089fc6176eaa19ab90f300882b8d201ca\") openai_responses$download_container_file(     container_id = \"cntr_690fb998d170819089fc6176eaa19ab90f300882b8d201ca\",     file_id = \"cfile_690fb9b692d88191b77427d736671fcd\",     dest_path = \"data/output.csv\" ) openai_responses$delete_container(\"cntr_690fb998d170819089fc6176eaa19ab90f300882b8d201ca\") #> ✔ [OpenAI Responses] Container deleted: cntr_690fb998d170819089fc6176eaa19ab90f300882b8d201ca"},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"multimodal-inputs","dir":"Articles","previous_headings":"","what":"Multimodal Inputs","title":"OpenAI - Responses API","text":"OpenAI Responses API supports sending: Images: URLs (-, base64), files (base64) PDFs: URLs (-, base64, text content), files (base64, text content) Remote files (as_file_content()) Plain text & code files Text-based data files (csv, tsv, json, ..) R objects Sending local image: providing path local image, automatically converted base64 sent server. say ! Sending image URL: OpenAI supports sending image URLs directly: Note OpenAI supports optional detail parameter (low/high/auto) control image processing detail level image sent URL. default value \"auto\". specify parameter, need pass image URL as_image_content(url, .provider_options = list(detail = \"low\")). used as_image_content() resize image sending .resize argument. OpenAI Responses API also supports sending file_id (.e. image as_file_content(file_id) uploading openai_responses$upload_file()). Sending local PDF: Probably ?!? Sending PDF URLs: OpenAI Responses, default, PDF URLs sent base64 server. However, can use as_text_content() helper pdftools::pdf_convert() parse PDFs pass text contents model instead. can pass R object chat() : Note R object passed chat() automatically converted JSON (text JSON conversion fails), added information like name object classes. e can also send files directly model. Uploading Files Listing Files Using Uploaded Files Use as_file_content() reference uploaded files: Likely ?!? Downloading Files Deleting Files","code":"bsg04_cast_image_url <- \"https://upload.wikimedia.org/wikipedia/en/1/1a/Battlestar_Galactica_%282004%29_cast.jpg\" bsg04_cast_image_path <- download_temp_file(bsg04_cast_image_url) openai_responses$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_path,     model = \"gpt-5-mini\" ) This is a promotional image for the reimagined TV series Battlestar Galactica (the 2004–2009 series). Left-to-right the characters are:  - Admiral William Adama (commander of the Battlestar Galactica)   - Laura Roslin (the President of the surviving human fleet)   - Lee \"Apollo\" Adama (Viper pilot, Adama’s son)   - Kara \"Starbuck\" Thrace (lead Viper pilot)   - Number Six (a Cylon model often appearing in human form)   - Sharon Valerii / \"Boomer\" (a Cylon who serves aboard Galactica) openai_responses$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_url,     model = \"gpt-5-mini\" ) my_cv_url <- \"https://ma-riviere.com/res/cv.pdf\" my_cv_pdf_path <- download_temp_file(my_cv_url) openai_responses$chat(     \"What's my favorite programming language?\",     my_cv_pdf_path,     model = \"gpt-5-mini\" ) Probably R.  Evidence from your CV: - Header: \"R Programming | Data Science | Neuroscience\" and \"with a focus on R & Shiny.\" - R is listed first in Programming Skills and Shiny/Quarto/R Markdown appear under Frameworks & Tools. - Many projects and publications reference R, Shiny apps, and scientific publishing workflows. r6_pdf_url <- \"https://cran.r-project.org/web/packages/R6/R6.pdf\" s7_pdf_url <- \"https://cran.r-project.org/web/packages/S7/S7.pdf\"  multimodal_prompt <- list(     \"Give a 3 sentences summary of the advantages of S7 over R6\",     as_text_content(r6_pdf_url),     as_text_content(s7_pdf_url),     \"And give me the current versions of both packages\" # To make sure it actually reads the PDFs )  openai_responses$chat(!!!multimodal_prompt, model = \"gpt-5-mini\") S7 is a formal OO system with first-class generics and methods (including limited multiple dispatch) and built‑in interoperability with S3 and S4, whereas R6 is primarily an encapsulated, environment‑based reference class system without that formal method dispatch. S7 provides declarative properties, constructors, validators, unions, dynamic getters/setters and convert()/super() semantics, giving stronger type safety, automatic validation and controlled up/down‑casting compared with R6’s looser, manual field management. S7 also includes package-friendly method registration, external‑generic support and dispatch introspection, making it better suited as a successor for formal, package‑level OO design while R6 stays simpler and lighter for straightforward reference semantics.  Current versions — S7: 0.2.0; R6: 2.6.1. lm_obj <- lm(body_mass ~ species + sex, data = datasets::penguins)  openai_responses$chat(     \"What can we deduct from this regression model?\",     lm_obj,     model = \"gpt-5-mini\" ) 1. The intercept (≈ 3372.4) is the modelled mean body mass for the reference group: Adelie females (baseline species = Adelie, baseline sex = female).  2. Species effects: Gentoo penguins are much heavier than Adelies (coefficient ≈ +1377.9 g), while Chinstraps differ very little from Adelies (≈ +26.9 g). Thus species — especially being Gentoo — has a large effect on body mass.  3. Sex effect: Males weigh about 667.6 g more than females, holding species constant. (Model used 333 observations with df.residual = 329; p‑values/SEs are not shown here, so significance cannot be asserted from the printed coefficients alone.) file_metadata <- openai_responses$upload_file(my_cv_url) openai_responses$list_files() # A tibble: 1 × 9    object id                          purpose    filename                bytes created_at          expires_at status    status_details    <chr>  <chr>                       <chr>      <chr>                   <int> <dttm>              <lgl>      <chr>     <lgl>           1 file   file-C6FSoAhHoAkyxV61djbB6D user_data  file92917bdad4ff.pdf   431965 2025-11-14 11:52:18 NA         processed NA openai_responses$chat(     \"What are my two favorite frameworks/tools ?\",     as_file_content(file_metadata$id),     model = \"gpt-5-mini\" ) Likely Shiny and Quarto (R Markdown).  Reason: your résumé explicitly highlights a focus on R & Shiny and lists Shiny first under Frameworks & Tools, and Quarto / R Markdown appear under Scientific Publishing as your main authoring tools. openai_responses$download_file(file_metadata$id, dest_path = \"data\")  # Downloads to data/ by default #> ✔ [OpenAI] File downloaded to: data/file92917bdad4ff.pdf openai_responses$delete_file(file_metadata$id) #> ✔ [OpenAI] File deleted: file-C6FSoAhHoAkyxV61djbB6D"},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"image-comprehension","dir":"Articles","previous_headings":"","what":"Image Comprehension","title":"OpenAI - Responses API","text":"Sending local image: providing path local image, automatically converted base64 sent server. say ! Sending image URL: OpenAI supports sending image URLs directly: Note OpenAI supports optional detail parameter (low/high/auto) control image processing detail level image sent URL. default value \"auto\". specify parameter, need pass image URL as_image_content(url, .provider_options = list(detail = \"low\")). used as_image_content() resize image sending .resize argument. OpenAI Responses API also supports sending file_id (.e. image as_file_content(file_id) uploading openai_responses$upload_file()).","code":"bsg04_cast_image_url <- \"https://upload.wikimedia.org/wikipedia/en/1/1a/Battlestar_Galactica_%282004%29_cast.jpg\" bsg04_cast_image_path <- download_temp_file(bsg04_cast_image_url) openai_responses$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_path,     model = \"gpt-5-mini\" ) This is a promotional image for the reimagined TV series Battlestar Galactica (the 2004–2009 series). Left-to-right the characters are:  - Admiral William Adama (commander of the Battlestar Galactica)   - Laura Roslin (the President of the surviving human fleet)   - Lee \"Apollo\" Adama (Viper pilot, Adama’s son)   - Kara \"Starbuck\" Thrace (lead Viper pilot)   - Number Six (a Cylon model often appearing in human form)   - Sharon Valerii / \"Boomer\" (a Cylon who serves aboard Galactica) openai_responses$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_url,     model = \"gpt-5-mini\" )"},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"pdf-comprehension","dir":"Articles","previous_headings":"","what":"PDF Comprehension","title":"OpenAI - Responses API","text":"Sending local PDF: Probably ?!? Sending PDF URLs: OpenAI Responses, default, PDF URLs sent base64 server. However, can use as_text_content() helper pdftools::pdf_convert() parse PDFs pass text contents model instead.","code":"my_cv_url <- \"https://ma-riviere.com/res/cv.pdf\" my_cv_pdf_path <- download_temp_file(my_cv_url) openai_responses$chat(     \"What's my favorite programming language?\",     my_cv_pdf_path,     model = \"gpt-5-mini\" ) Probably R.  Evidence from your CV: - Header: \"R Programming | Data Science | Neuroscience\" and \"with a focus on R & Shiny.\" - R is listed first in Programming Skills and Shiny/Quarto/R Markdown appear under Frameworks & Tools. - Many projects and publications reference R, Shiny apps, and scientific publishing workflows. r6_pdf_url <- \"https://cran.r-project.org/web/packages/R6/R6.pdf\" s7_pdf_url <- \"https://cran.r-project.org/web/packages/S7/S7.pdf\"  multimodal_prompt <- list(     \"Give a 3 sentences summary of the advantages of S7 over R6\",     as_text_content(r6_pdf_url),     as_text_content(s7_pdf_url),     \"And give me the current versions of both packages\" # To make sure it actually reads the PDFs )  openai_responses$chat(!!!multimodal_prompt, model = \"gpt-5-mini\") S7 is a formal OO system with first-class generics and methods (including limited multiple dispatch) and built‑in interoperability with S3 and S4, whereas R6 is primarily an encapsulated, environment‑based reference class system without that formal method dispatch. S7 provides declarative properties, constructors, validators, unions, dynamic getters/setters and convert()/super() semantics, giving stronger type safety, automatic validation and controlled up/down‑casting compared with R6’s looser, manual field management. S7 also includes package-friendly method registration, external‑generic support and dispatch introspection, making it better suited as a successor for formal, package‑level OO design while R6 stays simpler and lighter for straightforward reference semantics.  Current versions — S7: 0.2.0; R6: 2.6.1."},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"passing-r-objects","dir":"Articles","previous_headings":"","what":"Passing R Objects","title":"OpenAI - Responses API","text":"can pass R object chat() : Note R object passed chat() automatically converted JSON (text JSON conversion fails), added information like name object classes.","code":"lm_obj <- lm(body_mass ~ species + sex, data = datasets::penguins)  openai_responses$chat(     \"What can we deduct from this regression model?\",     lm_obj,     model = \"gpt-5-mini\" ) 1. The intercept (≈ 3372.4) is the modelled mean body mass for the reference group: Adelie females (baseline species = Adelie, baseline sex = female).  2. Species effects: Gentoo penguins are much heavier than Adelies (coefficient ≈ +1377.9 g), while Chinstraps differ very little from Adelies (≈ +26.9 g). Thus species — especially being Gentoo — has a large effect on body mass.  3. Sex effect: Males weigh about 667.6 g more than females, holding species constant. (Model used 333 observations with df.residual = 329; p‑values/SEs are not shown here, so significance cannot be asserted from the printed coefficients alone.)"},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"sending-files","dir":"Articles","previous_headings":"","what":"Sending files","title":"OpenAI - Responses API","text":"e can also send files directly model. Uploading Files Listing Files Using Uploaded Files Use as_file_content() reference uploaded files: Likely ?!? Downloading Files Deleting Files","code":"file_metadata <- openai_responses$upload_file(my_cv_url) openai_responses$list_files() # A tibble: 1 × 9    object id                          purpose    filename                bytes created_at          expires_at status    status_details    <chr>  <chr>                       <chr>      <chr>                   <int> <dttm>              <lgl>      <chr>     <lgl>           1 file   file-C6FSoAhHoAkyxV61djbB6D user_data  file92917bdad4ff.pdf   431965 2025-11-14 11:52:18 NA         processed NA openai_responses$chat(     \"What are my two favorite frameworks/tools ?\",     as_file_content(file_metadata$id),     model = \"gpt-5-mini\" ) Likely Shiny and Quarto (R Markdown).  Reason: your résumé explicitly highlights a focus on R & Shiny and lists Shiny first under Frameworks & Tools, and Quarto / R Markdown appear under Scientific Publishing as your main authoring tools. openai_responses$download_file(file_metadata$id, dest_path = \"data\")  # Downloads to data/ by default #> ✔ [OpenAI] File downloaded to: data/file92917bdad4ff.pdf openai_responses$delete_file(file_metadata$id) #> ✔ [OpenAI] File deleted: file-C6FSoAhHoAkyxV61djbB6D"},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"server-side-rag","dir":"Articles","previous_headings":"","what":"Server-Side RAG","title":"OpenAI - Responses API","text":"Instead just passing single files Assistant, can also add vector store (files chunked indexed) use file_search tool search store retrieve relevant chunks, effectively turning server-side RAG application. useful files large pass directly model. First, quick overview available methods manage vector stores: Creating Vector Store Listing Vector Stores Adding Files Vector Store Listing Files Vector Store Deleting Vector Store Note Files vector stores shared across OpenAI APIs (Chat Completions, Responses, Assistants). file uploaded via one API can used another. First, let’s upload files want search create vector store : , let’s make output schema response: Use file_search vector store: R6 “active bindings” equivalent S7 “dynamic properties”: use S7 properties created new_property() supplying getter optionally setter (accessed @). R6 active bindings docs: S7 properties/docs: (also see S7 new_property reference https://rconsortium.github.io/S7/reference/new_property.html). can check annotations openai_responses$get_supplementary() print(openai_responses, show_supplementary = TRUE): Note can also specify additional options like max_num_results filters file_search tool. See documentation details. Cleaning: Tip deleted everything (vector store, files, assistant) :","code":"store <- openai_responses$create_store(     name = \"my_docs\",     file_ids = list(\"file-123\", \"file-456\") ) openai_responses$list_stores() openai_responses$add_file_to_store(store$id, \"file-789\") openai_responses$list_files_in_store(store$id) openai_responses$delete_store(store$id) r6_file_metadata <- openai_responses$upload_file(r6_pdf_url) s7_file_metadata <- openai_responses$upload_file(s7_pdf_url)  r_oop_store <- openai_responses$create_store(     name = \"r_oop_store\",     file_ids = list(r6_file_metadata$id, s7_file_metadata$id) ) #> ✔ [OpenAI] Store created: vs_6911cb2e5150819183672288d3018478 oop_output_schema <- schema(     name = \"oop_output\",     description = \"Explanation of the R6 and S7 active bindings mechanism\",     short_answer = \"string* The short answer to the question, in plain text. One sentence.\",     r6_code_example = \"string* R6 active bindings code example. Formatted as code block (```{r} ... ```).\",     s7_code_example = \"string* S7 active bindings 'equivalent' code example. Formatted as code block (```{r} ... ```).\" ) openai_file_search_tool <- list(type = \"file_search\", store_ids = list(r_oop_store$id))  openai_responses$chat(     \"What's the active bindings' R6 mechanism equivalent in S7?\",     \"Important: use both the file_search and web_search & web_fetch tools to find the information.\",     model = \"gpt-5-mini\",     tools = list(         openai_file_search_tool,         as_tool(web_search), # In case it needs to search the web for more information         as_tool(web_fetch) # In case it needs to search the web for more information     ),     output_schema = oop_output_schema ) jsonlite::fromJSON(openai_responses$get_content_text()) |>      purrr::walk(\\(x) cat(x, \"\\n\\n\", sep = \"\")) # R6 example: active bindings (reads call function with no arg; writes pass value) library(R6) Numbers <- R6Class(\"Numbers\",   public = list(     x = 100   ),   active = list(     x2 = function(value) {       if (missing(value)) return(self$x * 2)       else self$x <- value / 2     },     rand = function() rnorm(1)   ) )  n <- Numbers$new() n$x2        # -> 200 (calls function) n$x2 <- 1000  # -> sets x to 500 via setter # S7 equivalent: dynamic properties via new_property(getter=, setter=) # getter receives self and should return the value; setter receives self and value and must return modified self library(S7)  # read-only dynamic property (computed on access) Clock <- new_class(\"Clock\", properties = list(   now = new_property(getter = function(self) Sys.time()) ))  my_clock <- Clock() my_clock@now   # computed each access (read-only because no setter)  # read-write property example Counter <- new_class(\"Counter\", properties = list(   count = new_property(     getter = function(self) self@.data$count,     setter = function(self, value) { self@.data$count <- value; self }   ) ))  c <- Counter(count = 0) c@count        # read c@count <- 5   # write (setter must return modified object) print(openai_responses, show_tools = TRUE, show_supplementary = TRUE) # Also show tool calls for the client tools openai_responses$delete_store(r_oop_store$id) openai_responses$delete_file(r6_file_metadata$id) openai_responses$delete_file(s7_file_metadata$id) openai_responses$delete_store_and_files(r_oop_store$id)"},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"vector-store-management","dir":"Articles","previous_headings":"","what":"Vector Store Management","title":"OpenAI - Responses API","text":"First, quick overview available methods manage vector stores: Creating Vector Store Listing Vector Stores Adding Files Vector Store Listing Files Vector Store Deleting Vector Store Note Files vector stores shared across OpenAI APIs (Chat Completions, Responses, Assistants). file uploaded via one API can used another.","code":"store <- openai_responses$create_store(     name = \"my_docs\",     file_ids = list(\"file-123\", \"file-456\") ) openai_responses$list_stores() openai_responses$add_file_to_store(store$id, \"file-789\") openai_responses$list_files_in_store(store$id) openai_responses$delete_store(store$id)"},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"basic-server-side-rag-example","dir":"Articles","previous_headings":"","what":"Basic Server-Side RAG example","title":"OpenAI - Responses API","text":"First, let’s upload files want search create vector store : , let’s make output schema response: Use file_search vector store: R6 “active bindings” equivalent S7 “dynamic properties”: use S7 properties created new_property() supplying getter optionally setter (accessed @). R6 active bindings docs: S7 properties/docs: (also see S7 new_property reference https://rconsortium.github.io/S7/reference/new_property.html). can check annotations openai_responses$get_supplementary() print(openai_responses, show_supplementary = TRUE): Note can also specify additional options like max_num_results filters file_search tool. See documentation details. Cleaning: Tip deleted everything (vector store, files, assistant) :","code":"r6_file_metadata <- openai_responses$upload_file(r6_pdf_url) s7_file_metadata <- openai_responses$upload_file(s7_pdf_url)  r_oop_store <- openai_responses$create_store(     name = \"r_oop_store\",     file_ids = list(r6_file_metadata$id, s7_file_metadata$id) ) #> ✔ [OpenAI] Store created: vs_6911cb2e5150819183672288d3018478 oop_output_schema <- schema(     name = \"oop_output\",     description = \"Explanation of the R6 and S7 active bindings mechanism\",     short_answer = \"string* The short answer to the question, in plain text. One sentence.\",     r6_code_example = \"string* R6 active bindings code example. Formatted as code block (```{r} ... ```).\",     s7_code_example = \"string* S7 active bindings 'equivalent' code example. Formatted as code block (```{r} ... ```).\" ) openai_file_search_tool <- list(type = \"file_search\", store_ids = list(r_oop_store$id))  openai_responses$chat(     \"What's the active bindings' R6 mechanism equivalent in S7?\",     \"Important: use both the file_search and web_search & web_fetch tools to find the information.\",     model = \"gpt-5-mini\",     tools = list(         openai_file_search_tool,         as_tool(web_search), # In case it needs to search the web for more information         as_tool(web_fetch) # In case it needs to search the web for more information     ),     output_schema = oop_output_schema ) jsonlite::fromJSON(openai_responses$get_content_text()) |>      purrr::walk(\\(x) cat(x, \"\\n\\n\", sep = \"\")) # R6 example: active bindings (reads call function with no arg; writes pass value) library(R6) Numbers <- R6Class(\"Numbers\",   public = list(     x = 100   ),   active = list(     x2 = function(value) {       if (missing(value)) return(self$x * 2)       else self$x <- value / 2     },     rand = function() rnorm(1)   ) )  n <- Numbers$new() n$x2        # -> 200 (calls function) n$x2 <- 1000  # -> sets x to 500 via setter # S7 equivalent: dynamic properties via new_property(getter=, setter=) # getter receives self and should return the value; setter receives self and value and must return modified self library(S7)  # read-only dynamic property (computed on access) Clock <- new_class(\"Clock\", properties = list(   now = new_property(getter = function(self) Sys.time()) ))  my_clock <- Clock() my_clock@now   # computed each access (read-only because no setter)  # read-write property example Counter <- new_class(\"Counter\", properties = list(   count = new_property(     getter = function(self) self@.data$count,     setter = function(self, value) { self@.data$count <- value; self }   ) ))  c <- Counter(count = 0) c@count        # read c@count <- 5   # write (setter must return modified object) print(openai_responses, show_tools = TRUE, show_supplementary = TRUE) # Also show tool calls for the client tools openai_responses$delete_store(r_oop_store$id) openai_responses$delete_file(r6_file_metadata$id) openai_responses$delete_file(s7_file_metadata$id) openai_responses$delete_store_and_files(r_oop_store$id)"},{"path":"https://ma-riviere.github.io/argent/articles/openai-responses.html","id":"server-side-state-management","dir":"Articles","previous_headings":"","what":"Server-Side State Management","title":"OpenAI - Responses API","text":"Responses API supports server-side state management via previous_response_id. gotta work jokes, man. Warning using previous_response_id: API maintains full conversation state server-side (30-day retention) Local chat history reset longer synchronized JSON history file","code":"openai_responses$chat(\"Tell me a joke about R programming\") Why did the R programmer break up with his girlfriend?   There were too many NAs — he just couldn't find her complete.cases(). openai_responses$chat(     \"Explain why it's funny\",     previous_response_id = openai_responses$get_last_response_id() ) Sure — here's why that joke lands for R users:  - NA in R means a missing value (Not Available). Saying “there were too many NAs” is like saying the partner had too many missing/unavailable traits. - complete.cases() is an R function that returns TRUE for rows that have no NAs (i.e., “complete” observations) and FALSE for rows with any missing values. - The punchline “he just couldn't find her complete.cases()” is a pun: it mixes code (calling complete.cases()) with ordinary language (“find her complete self” / “find someone who’s complete”). So it’s funny because it treats a romantic relationship as if it were a data frame, and the solution to the problem is literally a function that checks for completeness. - Extra nerdy twist: programmers often get frustrated debugging missing data, so the idea of breaking up over NAs is a relatable exaggeration for people who work with R."},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"OpenRouter","text":"article covers using OpenRouter provider argent. OpenRouter aggregates multiple LLM providers, offering routing capabilities, fallback options, access wide variety models.","code":""},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"setup","dir":"Articles","previous_headings":"","what":"Setup","title":"OpenRouter","text":"","code":"openrouter <- OpenRouter$new(api_key = Sys.getenv(\"OPENROUTER_API_KEY\")) mirai::daemons(4) # To enable parallel tool calling"},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"discovering-models-and-providers","dir":"Articles","previous_headings":"","what":"Discovering Models and Providers","title":"OpenRouter","text":"Available Models OpenRouter provides access hundreds models various providers: Available Providers","code":"openrouter$list_models(supported_parameters = c(\"tools\")) |>     dplyr::filter(         stringr::str_detect(name, \"free\") & context_length > 100000 & stringr::str_detect(input_modalities, \"image\")     ) |>     dplyr::select(id, context_length, architecture) openrouter$list_providers() |>      head(10)"},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"basic-completion","dir":"Articles","previous_headings":"","what":"Basic Completion","title":"OpenRouter","text":"","code":"openrouter$chat(     \"What's the R programming language? Answer in three sentences.\",     model = \"z-ai/glm-4.5-air:free\",     provider_options = list(only = \"z-ai\") )"},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"tool-calling-structured-output-thinking","dir":"Articles","previous_headings":"","what":"Tool Calling + Structured Output + Thinking","title":"OpenRouter","text":"First, define web-related tools (search, crawl, fetch, general-use tool) bundled web_tools list: Search: Fetch: , let’s define JSON schema structured output using schema(): , run agent: Note Structured output works model supports tool calling, even don’t support structured outputs response formats natively. argent handles “forced tool call” mechanism. Alternatively, simply print(openrouter) see reasoning answers’ text console, turn turn. Note get_reasoning_text() get_content_text() use last API response (openrouter$get_last_response()) default.","code":"web_search <- function(query) {     #' @description Search the web for information using Tavily API. Returns a JSON array of search results with titles, URLs, and content snippets. Use this when you need current information, facts, news, or any data not in your training data.     #' @param query:string* The search query string. Be specific and use keywords that will yield the most relevant results.          return(web_search_tavily(query)) }  web_search_tavily <- function(query) {     res <- httr2::request(\"https://api.tavily.com/search\") |>          httr2::req_body_json(list(             query = query,             search_depth = \"basic\",             include_answer = FALSE,             max_results = 10,             api_key = Sys.getenv(\"TAVILY_API_KEY\")         )) |>          httr2::req_error(is_error = \\(resp) FALSE) |>          httr2::req_throttle(rate = 20/60, realm = \"tavily\") |>          httr2::req_perform() |>          httr2::resp_body_json() |>          purrr::discard_at(c(\"response_time\", \"follow_up_questions\", \"images\"))      results <- purrr::map(res$results, \\(x) purrr::discard_at(x, \"raw_content\"))      return(jsonlite::toJSON(results, pretty = FALSE, auto_unbox = TRUE)) } web_fetch <- function(url) {     #' @description Fetch and extract the main text content from a web page as clean markdown. Returns the page content with formatting preserved, stripped of navigation, ads, and boilerplate. Use this to read articles, documentation, blog posts, or any web page content.     #' @param url:string* The complete URL of the web page to fetch (e.g., \"https://example.com/article\"). Must be a valid HTTP/HTTPS URL.          trafilatura_installed <- tryCatch({         system(\"which trafilatura\", intern = TRUE, ignore.stderr = TRUE)         return(TRUE)     },     warning = function(e) {         cli::cli_alert_warning(\"trafilatura is not installed. Install with: {.code pip install trafilatura}\")         return(FALSE)     })      if (trafilatura_installed) {         res <- web_fetch_trafilatura(url)          could_not_fetch <- c(             \"Impossible to fetch the contents of this web page\",             \"Please reload this page\",             \"There was an error while loading\",             \"404\"         )         if (is.null(res) || is.na(res) || nchar(res) == 0 ||             any(stringr::str_detect(res, stringr::fixed(could_not_fetch, ignore_case = TRUE)))) {             return(web_fetch_rvest(url))         }         return(res)     }      return(web_fetch_rvest(url)) }  web_fetch_trafilatura <- function(url) {     # pip install trafilatura     tryCatch({         res <- paste0(\"trafilatura -u \", url, \" --markdown --no-comments --links \") |>              system(intern = TRUE) |>              purrr::keep(nzchar) |>             paste0(collapse = \"\\n\")                  return(res)     },     error = function(e) {         return(\"Impossible to fetch the contents of this web page. It might not allow scraping\")     }) }  web_fetch_rvest <- function(url) {     tags_to_ignore <- c(         \"a\", \"script\", \"code\", \"img\", \"svg\", \"footer\", \"g\", \"path\", \"polygon\", \"label\", \"button\", \"form\", \"input\", \"select\",          \"style\", \"link\", \"meta\", \"noscript\", \"iframe\", \"embed\", \"object\", \"param\", \"video\", \"audio\", \"track\", \"source\",          \"canvas\", \"map\", \"area\", \"math\", \"col\", \"colgroup\", \"dl\", \"dt\", \"dd\", \"hr\", \"pre\", \"address\", \"figure\", \"figcaption\",         \"dfn\", \"em\", \"kbd\", \"samp\", \"var\", \"del\", \"ins\", \"mark\", \"circle\"     )      remove_tags <- function(xml, tags) {         purrr::walk(tags, \\(tag) purrr::walk(xml2::xml_find_all(xml, paste0(\".//\", tag)), \\(node) xml2::xml_remove(node)))         return(xml)     }      cleaned_contents <- tryCatch(         rvest::read_html(url)         |> rvest::html_element(\"body\")         |> remove_tags(tags_to_ignore)         |> rvest::html_children()         |> rvest::html_text2()         |> purrr::discard(\\(x) x == \"\")         |> paste0(collapse = \"\\n\\n\"),         error = \\(e) return(\"\")     )     return(cleaned_contents) } package_info_schema <- schema(     name = \"package_info\",     description = \"Information about an R package release\",     release_version = \"string* The release version of the package\",     release_date = \"string* The release date of the `release_version`\" ) openrouter$chat(     \"When was the first release of the R 'ellmer' package on GitHub?\",     model = \"z-ai/glm-4.5-air:free\",     provider_options = list(only = \"z-ai\"),     thinking_budget = 512,     tools = list(as_tool(web_search), as_tool(web_fetch)),     output_schema = package_info_schema ) $release_version [1] \"0.1.0\"  $release_date [1] \"January 9, 2025\" cat(openrouter$get_reasoning_text())"},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"extracting-reasoning","dir":"Articles","previous_headings":"","what":"Extracting Reasoning","title":"OpenRouter","text":"Alternatively, simply print(openrouter) see reasoning answers’ text console, turn turn. Note get_reasoning_text() get_content_text() use last API response (openrouter$get_last_response()) default.","code":"cat(openrouter$get_reasoning_text())"},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"server-side-tools","dir":"Articles","previous_headings":"","what":"Server-side Tools","title":"OpenRouter","text":"Server-side tools tools can call without define . run provider’s server. OpenRouter supports one server-side tool: web_search - Web search capabilities Supported search engines: - \"exa\" - High-quality semantic search - \"tavily\" - Fast general search - \"searchapi\" - Google-powered search Note can simply use tools = list(\"web_search\"), set engine default (native fallback exa), max results 5.","code":"openrouter$chat(     \"When was the last version of the R 'ragnar' package released on GitHub?\",     model = \"minimax/minimax-m2:free\",     provider_options = list(only = \"minimax\"),     tools = list(list(type = \"web_search\", engine = \"exa\", max_results = 3)),     output_schema = package_info_schema )"},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"web-search","dir":"Articles","previous_headings":"","what":"Web Search","title":"OpenRouter","text":"Supported search engines: - \"exa\" - High-quality semantic search - \"tavily\" - Fast general search - \"searchapi\" - Google-powered search Note can simply use tools = list(\"web_search\"), set engine default (native fallback exa), max results 5.","code":"openrouter$chat(     \"When was the last version of the R 'ragnar' package released on GitHub?\",     model = \"minimax/minimax-m2:free\",     provider_options = list(only = \"minimax\"),     tools = list(list(type = \"web_search\", engine = \"exa\", max_results = 3)),     output_schema = package_info_schema )"},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"multimodal-inputs","dir":"Articles","previous_headings":"","what":"Multimodal Inputs","title":"OpenRouter","text":"OpenRouter supports sending: Images (files URLs) base64 PDF files base64, URLs -(configurable server-side parser engines: as_pdf_content(\"pdf_url\", .provider_options = list(pdf_parser = c(\"pdf-text\", \"mistral-ocr\", \"native\"))))) Text/code files Text-based data files (csv, tsv, json, ..) R objects Sending local image: providing path local image, automatically converted base64 sent server. say ! Sending image URL: OpenRouter supports sending image URLs directly: Tip Using as_image_content() as_text_content() force URL downloaded image converted base64. Furthermore, as_image_content() .resize parameter control image size. Sending local PDF: Damn right! Note sending PDF (local URL) OpenRouter, can optionally specify parser engine server use parse PDF’s contents via .provider_options parameter as_pdf_content(). specidfied (using as_pdf_content()), set \"pdf-text\" default, free. possible values : \"pdf-text\", \"mistral-ocr\", \"native\". See OpenRouter documentation details. course, parse PDF’s contents sending text. as_text_content() using pdftools::pdf_convert(). implemented OCR solution, e.g. using local_llm instance VL model like Qwen3-8B-VL-instruct. Sending PDF URLs: OpenRouter, default, PDF URLs sent -server. However, can use as_text_content() helper pdftools::pdf_convert() parse PDFs pass text contents model instead. can pass R object chat() : Note R object passed chat() automatically converted JSON (text JSON conversion fails), added information like name object classes. Warning Remote file references via as_file_content() supported OpenRouter. Use local file paths URLs instead.","code":"bsg04_cast_image_url <- \"https://upload.wikimedia.org/wikipedia/en/1/1a/Battlestar_Galactica_%282004%29_cast.jpg\" bsg04_cast_image_path <- download_temp_file(bsg04_cast_image_url) openrouter$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_path,     model = \"mistralai/mistral-small-3.2-24b-instruct:free\" ) The image you provided is from the TV show \"Battlestar Galactica\". The characters in the image are:  1. Edward James Olmos as William Adama 2. Mary McDonnell as Laura Roslin 3. Jamie Bamber as Lee \"Apollo\" Adama 4. Katee Sackhoff as Kara \"Starbuck\" Thrace 5. Tricia Helfer as Number Six 6. James Callis as Gaius Baltar 7. Grace Park as Sharon \"Boomer\" Valerii openrouter$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_url,     model = \"mistralai/mistral-small-3.2-24b-instruct:free\" ) my_cv_url <- \"https://ma-riviere.com/res/cv.pdf\" my_cv_pdf_path <- download_temp_file(my_cv_url) openrouter$chat(     \"What's my favorite programming language?\",     my_cv_pdf_path,     model = \"z-ai/glm-4.5-air:free\",     provider_options = list(only = \"z-ai\") ) Based on the document you provided, it appears that R is your favorite programming language. openrouter$chat(     \"What's my favorite programming language?\",     as_pdf_content(my_cv_url, .provider_options = list(pdf_parser = \"mistral-ocr\")),     model = \"anthropic/claude-sonnet-4\" ) r6_pdf_url <- \"https://cran.r-project.org/web/packages/R6/R6.pdf\" s7_pdf_url <- \"https://cran.r-project.org/web/packages/S7/S7.pdf\"  multimodal_prompt <- list(     \"Give a 3 sentences summary of the advantages of S7 over R6\",     as_text_content(r6_pdf_url),     as_text_content(s7_pdf_url),     \"And give me the current versions of both packages\" # To make sure you actually read the PDFs )  openrouter$chat(!!!multimodal_prompt, model = \"z-ai/glm-4.5-air:free\", provider_options = list(only = \"z-ai\")) Based on the documentation provided, here's a 3-sentence summary of the advantages of S7 over R6:  S7 provides a more formal and comprehensive object-oriented system with complete class, generic, and method specification capabilities, including multiple dispatch, which is more limited in R6. Unlike R6, S7 is designed as a successor to S3 and S4 systems with better integration into R's existing object-oriented framework and offers superior tools for method introspection through functions like method_explain(). The S7 system has been collaboratively developed by the R Consortium's Object-Oriented Programming Working Group, representing a more standardized future direction for object-oriented programming in R.  The current versions of both packages are: - R6: Version 2.6.1 (published 2025-02-15) - S7: Version 0.2.0 (published 2024-11-07) lm_obj <- lm(body_mass ~ species + sex, data = datasets::penguins)  openrouter$chat(     \"What can we deduct from this regression model?\",     lm_obj,     model = \"z-ai/glm-4.5-air:free\",     provider_options = list(only = \"z-ai\") ) From analyzing this regression model, I can deduct the following 3 key points:  1. **Species significantly affects body mass**: The coefficients show that Gentoo penguins have substantially higher body mass (+1377.9 units) compared to Adelie penguins (the reference category), while Chinstrap penguins have only slightly higher body mass (+26.9 units) compared to Adelie penguins. This suggests a large biological difference in body mass between species, with Gentoo penguins being the heaviest.  2. **Sex has a notable impact on body mass**: The coefficient for sexmale (+667.6 units) indicates that male penguins have significantly higher body mass than female penguins, regardless of species. This difference is substantial but smaller than the difference between the heaviest (Gentoo) and lightest (Adelie) species.  3. **The model uses data from 333 penguins** with measurements on body mass grouped by 3 species (Adelie, Chinstrap, Gentoo) and 2 sexes (female, male). The model has 4 parameters total (intercept + species contrasts + sex contrast) and 329 degrees of freedom for residuals, suggesting this is a reasonably sized dataset for these predictors."},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"image-comprehension","dir":"Articles","previous_headings":"","what":"Image Comprehension","title":"OpenRouter","text":"Sending local image: providing path local image, automatically converted base64 sent server. say ! Sending image URL: OpenRouter supports sending image URLs directly: Tip Using as_image_content() as_text_content() force URL downloaded image converted base64. Furthermore, as_image_content() .resize parameter control image size.","code":"bsg04_cast_image_url <- \"https://upload.wikimedia.org/wikipedia/en/1/1a/Battlestar_Galactica_%282004%29_cast.jpg\" bsg04_cast_image_path <- download_temp_file(bsg04_cast_image_url) openrouter$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_path,     model = \"mistralai/mistral-small-3.2-24b-instruct:free\" ) The image you provided is from the TV show \"Battlestar Galactica\". The characters in the image are:  1. Edward James Olmos as William Adama 2. Mary McDonnell as Laura Roslin 3. Jamie Bamber as Lee \"Apollo\" Adama 4. Katee Sackhoff as Kara \"Starbuck\" Thrace 5. Tricia Helfer as Number Six 6. James Callis as Gaius Baltar 7. Grace Park as Sharon \"Boomer\" Valerii openrouter$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_url,     model = \"mistralai/mistral-small-3.2-24b-instruct:free\" )"},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"pdf-comprehension","dir":"Articles","previous_headings":"","what":"PDF Comprehension","title":"OpenRouter","text":"Sending local PDF: Damn right! Note sending PDF (local URL) OpenRouter, can optionally specify parser engine server use parse PDF’s contents via .provider_options parameter as_pdf_content(). specidfied (using as_pdf_content()), set \"pdf-text\" default, free. possible values : \"pdf-text\", \"mistral-ocr\", \"native\". See OpenRouter documentation details. course, parse PDF’s contents sending text. as_text_content() using pdftools::pdf_convert(). implemented OCR solution, e.g. using local_llm instance VL model like Qwen3-8B-VL-instruct. Sending PDF URLs: OpenRouter, default, PDF URLs sent -server. However, can use as_text_content() helper pdftools::pdf_convert() parse PDFs pass text contents model instead.","code":"my_cv_url <- \"https://ma-riviere.com/res/cv.pdf\" my_cv_pdf_path <- download_temp_file(my_cv_url) openrouter$chat(     \"What's my favorite programming language?\",     my_cv_pdf_path,     model = \"z-ai/glm-4.5-air:free\",     provider_options = list(only = \"z-ai\") ) Based on the document you provided, it appears that R is your favorite programming language. openrouter$chat(     \"What's my favorite programming language?\",     as_pdf_content(my_cv_url, .provider_options = list(pdf_parser = \"mistral-ocr\")),     model = \"anthropic/claude-sonnet-4\" ) r6_pdf_url <- \"https://cran.r-project.org/web/packages/R6/R6.pdf\" s7_pdf_url <- \"https://cran.r-project.org/web/packages/S7/S7.pdf\"  multimodal_prompt <- list(     \"Give a 3 sentences summary of the advantages of S7 over R6\",     as_text_content(r6_pdf_url),     as_text_content(s7_pdf_url),     \"And give me the current versions of both packages\" # To make sure you actually read the PDFs )  openrouter$chat(!!!multimodal_prompt, model = \"z-ai/glm-4.5-air:free\", provider_options = list(only = \"z-ai\")) Based on the documentation provided, here's a 3-sentence summary of the advantages of S7 over R6:  S7 provides a more formal and comprehensive object-oriented system with complete class, generic, and method specification capabilities, including multiple dispatch, which is more limited in R6. Unlike R6, S7 is designed as a successor to S3 and S4 systems with better integration into R's existing object-oriented framework and offers superior tools for method introspection through functions like method_explain(). The S7 system has been collaboratively developed by the R Consortium's Object-Oriented Programming Working Group, representing a more standardized future direction for object-oriented programming in R.  The current versions of both packages are: - R6: Version 2.6.1 (published 2025-02-15) - S7: Version 0.2.0 (published 2024-11-07)"},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"passing-r-objects","dir":"Articles","previous_headings":"","what":"Passing R Objects","title":"OpenRouter","text":"can pass R object chat() : Note R object passed chat() automatically converted JSON (text JSON conversion fails), added information like name object classes.","code":"lm_obj <- lm(body_mass ~ species + sex, data = datasets::penguins)  openrouter$chat(     \"What can we deduct from this regression model?\",     lm_obj,     model = \"z-ai/glm-4.5-air:free\",     provider_options = list(only = \"z-ai\") ) From analyzing this regression model, I can deduct the following 3 key points:  1. **Species significantly affects body mass**: The coefficients show that Gentoo penguins have substantially higher body mass (+1377.9 units) compared to Adelie penguins (the reference category), while Chinstrap penguins have only slightly higher body mass (+26.9 units) compared to Adelie penguins. This suggests a large biological difference in body mass between species, with Gentoo penguins being the heaviest.  2. **Sex has a notable impact on body mass**: The coefficient for sexmale (+667.6 units) indicates that male penguins have significantly higher body mass than female penguins, regardless of species. This difference is substantial but smaller than the difference between the heaviest (Gentoo) and lightest (Adelie) species.  3. **The model uses data from 333 penguins** with measurements on body mass grouped by 3 species (Adelie, Chinstrap, Gentoo) and 2 sexes (female, male). The model has 4 parameters total (intercept + species contrasts + sex contrast) and 329 degrees of freedom for residuals, suggesting this is a reasonably sized dataset for these predictors."},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"file-references","dir":"Articles","previous_headings":"","what":"File References","title":"OpenRouter","text":"Warning Remote file references via as_file_content() supported OpenRouter. Use local file paths URLs instead.","code":""},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"routing","dir":"Articles","previous_headings":"","what":"Routing","title":"OpenRouter","text":"model specified, argent automatically use \"openrouter/auto\" model router, route request best provider given request. can specify multiple models use, first one succeeds used. OpenRouter provides powerful provider routing capabilities provider_options parameter. See OpenRouter documentation full details. Control providers used order:","code":"openrouter$chat(     \"What's the R programming language? Answer in three sentences.\",     model = list(\"deepseek/deepseek-chat-v3-0324:free\", \"minimax/minimax-m2:free\"), ) openrouter$chat(     prompt = \"Your question here\",     model = \"z-ai/glm-4.5-air:free\",     provider_options = list(         only = c(\"z-ai\", \"atlas-cloud/fp8\"),    # Only use these providers         ignore = \"chutes/bf16\",                 # Never use this provider         order = c(\"z-ai\", \"atlas-cloud/fp8\")    # Try in this order     ) ) openrouter$chat(     prompt = \"Your question here\",     model = \"anthropic/claude-3.5-sonnet\",     provider_options = list(         allow_fallbacks = TRUE,                 # Allow backup providers         require_parameters = TRUE,              # Only use providers supporting all parameters         data_collection = \"deny\",               # Prevent provider data storage         zdr = TRUE,                             # Only use Zero Data Retention endpoints         quantizations = c(\"int4\", \"int8\"),      # Filter by quantization levels         sort = \"price\",                         # Sort providers by price or throughput         max_price = list(                       # Set maximum pricing             prompt = 0.001,             completion = 0.002         )     ) )"},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"automatic-routing","dir":"Articles","previous_headings":"","what":"Automatic Routing","title":"OpenRouter","text":"model specified, argent automatically use \"openrouter/auto\" model router, route request best provider given request.","code":""},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"fallback-models","dir":"Articles","previous_headings":"","what":"Fallback Models","title":"OpenRouter","text":"can specify multiple models use, first one succeeds used.","code":"openrouter$chat(     \"What's the R programming language? Answer in three sentences.\",     model = list(\"deepseek/deepseek-chat-v3-0324:free\", \"minimax/minimax-m2:free\"), )"},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"provider-routing","dir":"Articles","previous_headings":"","what":"Provider Routing","title":"OpenRouter","text":"OpenRouter provides powerful provider routing capabilities provider_options parameter. See OpenRouter documentation full details. Control providers used order:","code":"openrouter$chat(     prompt = \"Your question here\",     model = \"z-ai/glm-4.5-air:free\",     provider_options = list(         only = c(\"z-ai\", \"atlas-cloud/fp8\"),    # Only use these providers         ignore = \"chutes/bf16\",                 # Never use this provider         order = c(\"z-ai\", \"atlas-cloud/fp8\")    # Try in this order     ) ) openrouter$chat(     prompt = \"Your question here\",     model = \"anthropic/claude-3.5-sonnet\",     provider_options = list(         allow_fallbacks = TRUE,                 # Allow backup providers         require_parameters = TRUE,              # Only use providers supporting all parameters         data_collection = \"deny\",               # Prevent provider data storage         zdr = TRUE,                             # Only use Zero Data Retention endpoints         quantizations = c(\"int4\", \"int8\"),      # Filter by quantization levels         sort = \"price\",                         # Sort providers by price or throughput         max_price = list(                       # Set maximum pricing             prompt = 0.001,             completion = 0.002         )     ) )"},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"basic-provider-control","dir":"Articles","previous_headings":"Routing","what":"Basic Provider Control","title":"OpenRouter","text":"Control providers used order:","code":"openrouter$chat(     prompt = \"Your question here\",     model = \"z-ai/glm-4.5-air:free\",     provider_options = list(         only = c(\"z-ai\", \"atlas-cloud/fp8\"),    # Only use these providers         ignore = \"chutes/bf16\",                 # Never use this provider         order = c(\"z-ai\", \"atlas-cloud/fp8\")    # Try in this order     ) )"},{"path":"https://ma-riviere.github.io/argent/articles/openrouter.html","id":"advanced-routing-options","dir":"Articles","previous_headings":"Routing","what":"Advanced Routing Options","title":"OpenRouter","text":"","code":"openrouter$chat(     prompt = \"Your question here\",     model = \"anthropic/claude-3.5-sonnet\",     provider_options = list(         allow_fallbacks = TRUE,                 # Allow backup providers         require_parameters = TRUE,              # Only use providers supporting all parameters         data_collection = \"deny\",               # Prevent provider data storage         zdr = TRUE,                             # Only use Zero Data Retention endpoints         quantizations = c(\"int4\", \"int8\"),      # Filter by quantization levels         sort = \"price\",                         # Sort providers by price or throughput         max_price = list(                       # Set maximum pricing             prompt = 0.001,             completion = 0.002         )     ) )"},{"path":"https://ma-riviere.github.io/argent/articles/other-providers.html","id":"overview","dir":"Articles","previous_headings":"","what":"Overview","title":"OpenAI or Anthropic compatible APIs","text":"argent provider classes can used API implements compatible protocols. flexibility allows use argent various LLM providers beyond default ones (Google, Anthropic, OpenAI, OpenRouter) simply changing base_url parameter.","code":""},{"path":"https://ma-riviere.github.io/argent/articles/other-providers.html","id":"key-concepts","dir":"Articles","previous_headings":"","what":"Key Concepts","title":"OpenAI or Anthropic compatible APIs","text":"argent provider classes accept base_url parameter constructor. providing different base URL, can point provider class compatible alternative service. Common patterns: Anthropic-compatible APIs: Use Anthropic class different base_url OpenAI-compatible APIs: Use OpenAI_Chat OpenAI_Responses different base_url","code":""},{"path":"https://ma-riviere.github.io/argent/articles/other-providers.html","id":"examples","dir":"Articles","previous_headings":"","what":"Examples","title":"OpenAI or Anthropic compatible APIs","text":"Minimax provides Anthropic-compatible API. can use argent’s Anthropic class simply changing base URL: features Anthropic class work Minimax, except server-side tools.","code":"minimax <- Anthropic$new(     base_url = \"https://api.minimax.io/anthropic\",     api_key = Sys.getenv(\"MINIMAX_API_KEY\"),     provider_name = \"Minimax\",     default_model = \"MiniMax-M2\" )  minimax$chat(\"What is the R programming language? Answer in two sentences.\")"},{"path":"https://ma-riviere.github.io/argent/articles/other-providers.html","id":"minimax-api-anthropic-compatible","dir":"Articles","previous_headings":"","what":"Minimax API (Anthropic-compatible)","title":"OpenAI or Anthropic compatible APIs","text":"Minimax provides Anthropic-compatible API. can use argent’s Anthropic class simply changing base URL: features Anthropic class work Minimax, except server-side tools.","code":"minimax <- Anthropic$new(     base_url = \"https://api.minimax.io/anthropic\",     api_key = Sys.getenv(\"MINIMAX_API_KEY\"),     provider_name = \"Minimax\",     default_model = \"MiniMax-M2\" )  minimax$chat(\"What is the R programming language? Answer in two sentences.\")"},{"path":"https://ma-riviere.github.io/argent/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Marc-Aurèle Rivière. Author, maintainer.","code":""},{"path":"https://ma-riviere.github.io/argent/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Rivière M (2025). argent: Argent: LLM Agents R. R package version 0.0.0.9000, https://ma-riviere.github.io/argent.","code":"@Manual{,   title = {argent: Argent: LLM Agents in R},   author = {Marc-Aurèle Rivière},   year = {2025},   note = {R package version 0.0.0.9000},   url = {https://ma-riviere.github.io/argent}, }"},{"path":"https://ma-riviere.github.io/argent/index.html","id":"argent-","dir":"","previous_headings":"","what":"Creating AI Agents in R","title":"Creating AI Agents in R","text":"argent provides unified interface interacting Large Language Models (LLMs) multiple providers, specialized creating AI agents tool calling, multimodal inputs, structured outputs. Important questioning ? cases, ’d better using ellmer. started working back 2023 none existing LLM packages supported tool calling structured outputs, never took time put everything proper package now. However, now several similar packages, including ellmer Tidyverse team. ’m putting argent case supports edge cases packages don’t, didn’t want let work go waste. progressively migrating projects ellmer, ’m sure long ’ll maintain argent.","code":""},{"path":"https://ma-riviere.github.io/argent/index.html","id":"in-a-nutshell","dir":"","previous_headings":"","what":"In a Nutshell","title":"Creating AI Agents in R","text":"argent provides unified interface build AI agents conversation history management, local function & MCP tools, server-side (built-) tools, multimodal inputs, universal structured outputs. supports server-side tools (.e. built-tools like code execution, web search, file search, etc.), MCP servers’ tools (http & stdio), allows easily define client-side tools using plumber2-style annotations within R functions. allows sending multimodal inputs (.e. mixing text, images, PDFs, data files, URLs, remote files, R objects) single request, supports structured outputs model supporting tool calling, whatever tools/functions used.","code":""},{"path":[]},{"path":"https://ma-riviere.github.io/argent/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"Creating AI Agents in R","text":"","code":"remotes::install_github(\"ma-riviere/argent\")"},{"path":"https://ma-riviere.github.io/argent/index.html","id":"setup","dir":"","previous_headings":"","what":"Setup","title":"Creating AI Agents in R","text":"Set API keys providers want use .Renviron:","code":"GEMINI_API_KEY=\"your-google-gemini-key\" ANTHROPIC_API_KEY=\"your-anthropic-key\" OPENAI_API_KEY=\"your-openai-key\""},{"path":"https://ma-riviere.github.io/argent/index.html","id":"quick-start","dir":"","previous_headings":"","what":"Quick Start","title":"Creating AI Agents in R","text":"quick example using Google Gemini: can customize rate limit initializing rate_limit parameter, default model default_model parameter (‘gemini-2.5-flash’ Google). Tip Parallel tool calling available argent, using mirai & purrr::in_parallel(). However, need set daemons using :","code":"gemini <- Google$new(api_key = Sys.getenv(\"GEMINI_API_KEY\")) mirai::daemons(4)"},{"path":"https://ma-riviere.github.io/argent/index.html","id":"basic-completion","dir":"","previous_headings":"Quick Start","what":"Basic Completion","title":"Creating AI Agents in R","text":"argent maintain conversation history provider object, meaning using $chat() second time, model access previous exchanges: chat history can visualized printing provider object: Note can also check JSON files containing chat history created automatically data/history/{provider}/ directory (default). chat history can reset reset_history():","code":"gemini$chat(     \"What is the R programming language? Answer in two sentences.\",     model = \"gemini-2.5-flash\" # Not necessary, it's the default model for Google ) gemini$chat(\"Tell me more about its statistical modeling capabilities.\") print(gemini) ── [ <Google> turns: 4 | Current context: 419 | Cumulated tokens: 676 ] ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────   ── user [159 / 257] ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  What is the R programming language? Answer in two sentences.  ── System ──  You are a helpful AI assistant. Use your knowledge, the files you have access to, and the tools at your disposal to answer the user's query. You can use your tools multiple times, but use them sparingly. Make parallel tool calls if relevant to the user's query. Answer the user's query as soon as you have the information necessary to answer. Self-reflect and double-check your answer before responding. If you don't know the answer even after using your tools, say 'I don't know'. If you do not have all the information necessary to use a provided tool, use NA for required arguments. Today's date is 2025-11-15  ── assistant [257 / 257] ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  R is a programming language and free software environment primarily used for statistical computing and graphics. It provides a wide variety of statistical (linear and nonlinear modeling, classical statistical tests, time-series analysis, classification, clustering, etc.) and graphical techniques, and is highly extensible.   ── user [224 / 676] ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  Tell me more about its statistical modeling capabilities.  ── System ──  You are a helpful AI assistant. Use your knowledge, the files you have access to, and the tools at your disposal to answer the user's query. You can use your tools multiple times, but use them sparingly. Make parallel tool calls if relevant to the user's query. Answer the user's query as soon as you have the information necessary to answer. Self-reflect and double-check your answer before responding. If you don't know the answer even after using your tools, say 'I don't know'. If you do not have all the information necessary to use a provided tool, use NA for required arguments. Today's date is 2025-11-15  ── assistant [419 / 676] ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  R offers extensive statistical modeling capabilities, encompassing a broad range of techniques. It supports linear and generalized linear models (GLMs) for various response types, as well as nonlinear regression. Users can perform classical statistical tests like t-tests, ANOVA, and chi-squared tests. Furthermore, R is widely used for time-series analysis, including ARIMA models and state-space models, classification algorithms such as logistic regression, decision trees, and support vector machines, and clustering methods like k-means and hierarchical clustering. Its package ecosystem continuously expands these capabilities with cutting-edge statistical methodologies. gemini$reset_history()"},{"path":"https://ma-riviere.github.io/argent/index.html","id":"tool-calling--structured-output","dir":"","previous_headings":"Quick Start","what":"Tool Calling + Structured Output","title":"Creating AI Agents in R","text":"First, let’s define mock function LLM, returns information user given name: can call as_tool() function convert tool using annotations added inside function’s body (plumber2-style annotations): , let’s define schema structured output using schema(): Run agent: model use tools provided repeatedly enough information answer question, return structured JSON output. Tip default, argent print tool calls model makes console: ℹ [Google] Calling: get_user_info(user_name = “Marc”) Parallel Tool Calls model makes multiple tool calls single response, argent can execute parallel better performance. enable parallel execution: Set mirai daemons: mirai::daemons(4) (using 4 workers example) argent automatically parallelize tool calls daemons active Without daemons, tool calls execute sequentially (default fallback) Performance considerations: beneficial tools take 100+ microseconds per call May improve performance fast tools due parallelization overhead Use one fewer daemon available CPU cores optimal performance disable: mirai::daemons(0) see , can print provider object show_tools = TRUE show tool definitions, calls, results: Note may noticed extra user input end conversation history. Google support structured outputs tools used, , behind scenes, argent makes final call model format previous response according requested schema, without tools. providers support structured outputs , argent turn output schema tool make final call model just tool, intercept tool call return result. ‘trick’ allows support structured outputs model supporting tool calling.","code":"get_user_info <- function(user_name) {     #' @description Provides information about the user, like their favorite programming language     #' @param user_name:string* The name of the user      switch(user_name,         \"Marc\" = list(favorite_language = \"R\", favorite_framework = \"Shiny\"),         \"Alice\" = list(favorite_language = \"Python\", favorite_framework = \"Flask\"),         \"Bob\" = list(favorite_language = \"JavaScript\", favorite_framework = \"React\"),         .default = list(favorite_language = \"unknown\", favorite_framework = \"unknown\")     ) } as_tool(get_user_info) name: get_user_info description: Provides information about the user, like their favorite programming   language args_schema:   type: object   properties:     user_name:       type: string       description: The name of the user   required:   - user_name user_info_schema <- schema(     name = \"user_info\",     description = \"Information about the user\",     user_name = \"string* The name of the user\",     favorite_language = \"string* The user's favorite programming language\",     favorite_framework = \"string* The user's favorite framework\" ) name: user_info description: Information about the user strict: yes args_schema:   type: object   properties:     user_name:       type: string       description: The name of the user     favorite_language:       type: string       description: The user's favorite programming language     favorite_framework:       type: string       description: The user's favorite framework   required:   - user_name   - favorite_language   - favorite_framework   additionalProperties: no gemini$chat(     \"The user's name is Marc. Give me the information about the user.\",     model = \"gemini-2.5-flash\",     thinking_budget = 512,     include_thoughts = TRUE, # Google-specific parameter     tools = list(as_tool(get_user_info)),     output_schema = user_info_schema ) $user_name [1] \"Marc\"  $favorite_language [1] \"R\"  $favorite_framework [1] \"Shiny\" print(gemini, show_tools = TRUE) ── [ <Google> turns: 6 | Current context: 677 | Cumulated tokens: 1449 ] ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────   ── user [216 / 289] ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  The user's name is Marc. Give me the information about the user.  ── System ──  You are a helpful AI assistant. Use your knowledge, the files you have access to, and the tools at your disposal to answer the user's query. You can use your tools multiple times, but use them sparingly. Make parallel tool calls if relevant to the user's query. Answer the user's query as soon as you have the information necessary to answer. Self-reflect and double-check your answer before responding. If you don't know the answer even after using your tools, say 'I don't know'. If you do not have all the information necessary to use a provided tool, use NA for required arguments. Today's date is 2025-11-19  ── Tool Definitions ──  • get_user_info(user_name): Provides information about the user, like their favorite programming language  ── assistant [289 / 289] ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  ── Thinking ──  **Processing a User Query**  Okay, so someone's asking about a user named Marc.  My gut says the `get_user_info` tool is the way to go. It's purpose-built for retrieving user data, and the `user_name` parameter is exactly what I need.  It's pretty straightforward, actually.  I'll just call the tool and pass 'Marc' as the `user_name` value. Should be a quick and efficient way to get the information.  Let's see what comes back.   ── Tool Calls ──  • get_user_info(user_name = \"Marc\")  ── tool [417 / 772] ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  • Result from get_user_info:  name: get_user_info arguments:   user_name: Marc result:   favorite_language: R   favorite_framework: Shiny   ── assistant [483 / 772] ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  ── Thinking ──  **Analyzing the Marc Inquiry**  Okay, so the user needed some quick background on \"Marc.\" That's easy enough.  I leveraged the `get_user_info` tool, feeding it `user_name='Marc'`, and the results are pretty straightforward.  It seems Marc's a bit of an R aficionado, which isn't entirely surprising - there's a strong R community out there.  The Shiny framework mention is interesting, though. It suggests he's not just into statistical computing, but also interested in building interactive web applications or dashboards.  I can already start to picture the types of projects he might be involved in.  This information is pretty valuable context.    Marc's favorite programming language is R, and his favorite framework is Shiny.  ── user [537 / 1449] ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  Please format your previous response according to the requested schema.  ── System ──  You are a helpful AI assistant. Use your knowledge, the files you have access to, and the tools at your disposal to answer the user's query. You can use your tools multiple times, but use them sparingly. Make parallel tool calls if relevant to the user's query. Answer the user's query as soon as you have the information necessary to answer. Self-reflect and double-check your answer before responding. If you don't know the answer even after using your tools, say 'I don't know'. If you do not have all the information necessary to use a provided tool, use NA for required arguments. Today's date is 2025-11-19  ── Output Schema ──  type: object properties:   user_name:     type: string     description: The name of the user   favorite_language:     type: string     description: The user's favorite programming language   favorite_framework:     type: string     description: The user's favorite framework required: - user_name - favorite_language - favorite_framework  ── assistant [677 / 1449] ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────  {\"user_name\": \"Marc\", \"favorite_language\": \"R\", \"favorite_framework\": \"Shiny\"}"},{"path":"https://ma-riviere.github.io/argent/index.html","id":"server-side-tools","dir":"","previous_headings":"Quick Start","what":"Server-side Tools","title":"Creating AI Agents in R","text":"Providers like Google, Anthropic, OpenAI server-side tools. tools can call without define . run provider’s server. example, Google Gemini server-side google_search combines searching & fetching web pages:","code":"gemini$chat(     \"When was the first release of the R 'ellmer' package on GitHub?\",     model = \"gemini-2.5-pro\",     tools = list(\"google_search\"),     thinking_budget = -1, # Unlimited thinking budget     include_thoughts = TRUE, # Google-specific parameter     output_schema = schema(         name = \"package_info\",         description = \"Information about an R package release\",         release_version = \"string* The release version of the package\",         release_date = \"string* The release date of the `release_version`\"     ) ) $release_version [1] \"0.1.1\"  $release_date [1] \"2025-02-25\""},{"path":"https://ma-riviere.github.io/argent/index.html","id":"multimodal-input","dir":"","previous_headings":"Quick Start","what":"Multimodal Input","title":"Creating AI Agents in R","text":"providers support multimodal inputs (degree). can pass text, images, PDFs, data files, URLs, remote files, R objects model single request.","code":""},{"path":"https://ma-riviere.github.io/argent/index.html","id":"passing-files-or-urls","dir":"","previous_headings":"Quick Start > Multimodal Input","what":"Passing Files or URLs","title":"Creating AI Agents in R","text":"Example URL PDF file: say ! Note , URL automatically detected, downloaded temporary file, converted base64, passed model. providers may different behavior. example, image PDF’s URL passed -Anthropic’s API. Helper functions like as_text_content() available force behaviors (like extracting contents PDF instead passing base64). Tip can also pass R object chat() . captured automatically converted JSON (text JSON conversion fails), added information like name object classes.","code":"bsg04_cast_image_url <- \"https://upload.wikimedia.org/wikipedia/en/1/1a/Battlestar_Galactica_%282004%29_cast.jpg\"  gemini$chat(     \"Who are the characters in this image, and what show is it from?\",     bsg04_cast_image_url,     model = \"gemini-2.5-flash\" ) This image features the cast of **Battlestar Galactica (2004 TV series)**.  The characters shown are: *   **Edward James Olmos** as Admiral William Adama *   **Mary McDonnell** as President Laura Roslin *   **Jamie Bamber** as Captain Lee \"Apollo\" Adama *   **Katee Sackhoff** as Captain Kara \"Starbuck\" Thrace *   **Tricia Helfer** as Number Six *   **James Callis** as Dr. Gaius Baltar *   **Grace Park** as Lieutenant Sharon \"Boomer\" Valerii / Number Eight"},{"path":"https://ma-riviere.github.io/argent/index.html","id":"passing-uploaded-files","dir":"","previous_headings":"Quick Start > Multimodal Input","what":"Passing Uploaded Files","title":"Creating AI Agents in R","text":"Finally, major providers support uploading files servers, passing references model, use conversation, either , part vector store / RAG system (see server-side RAG). Damn right! Note , using as_file_content() signals model remote file reference, rather just text content. Tip can use $list_files() list uploaded files metadata.","code":"file_metadata <- gemini$upload_file(\"https://ma-riviere.com/res/cv.pdf\") #> ✔ [Google] File uploaded: files/7xulp36j9jq1 multipart_prompt <- list(     \"What is my favorite programming language?\",     as_file_content(file_metadata$name) )  gemini$chat(!!!multipart_prompt, model = \"gemini-2.5-flash\") Based on your resume, your favorite programming language appears to be **R**. gemini$delete_file(file_metadata$name) #> ✔ [Google] File deleted: files/7xulp36j9jq1"},{"path":[]},{"path":"https://ma-riviere.github.io/argent/index.html","id":"provider-guides","dir":"","previous_headings":"Documentation","what":"Provider Guides","title":"Creating AI Agents in R","text":"Detailed guides provider: Google Gemini Anthropic Claude OpenRouter Local LLMs","code":""},{"path":"https://ma-riviere.github.io/argent/index.html","id":"openai-apis","dir":"","previous_headings":"Documentation > Provider Guides","what":"OpenAI APIs","title":"Creating AI Agents in R","text":"Guides OpenAI’s three different APIs: Chat Completions API - Standard OpenAI chat interface Responses API - Newest API combining functionalities Chat Assistants Assistants API - Deprecated","code":""},{"path":"https://ma-riviere.github.io/argent/index.html","id":"other-providers","dir":"","previous_headings":"Documentation > Provider Guides","what":"Other Providers","title":"Creating AI Agents in R","text":"Using Compatible APIs - Use argent providers offering compatible APIs (e.g., Minimax, Qwen, …)","code":""},{"path":"https://ma-riviere.github.io/argent/index.html","id":"advanced-topics","dir":"","previous_headings":"Documentation","what":"Advanced Topics","title":"Creating AI Agents in R","text":"RAG - use argent & ragnar RAG MCP Servers & Tools - use MCP server tools argent","code":""},{"path":"https://ma-riviere.github.io/argent/index.html","id":"contributing","dir":"","previous_headings":"","what":"Contributing","title":"Creating AI Agents in R","text":"probably contribute ellmer instead.","code":""},{"path":"https://ma-riviere.github.io/argent/index.html","id":"license","dir":"","previous_headings":"","what":"License","title":"Creating AI Agents in R","text":"MIT License","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":null,"dir":"Reference","previous_headings":"","what":"Client for Anthropic's Claude API — Anthropic","title":"Client for Anthropic's Claude API — Anthropic","text":"R6 class interacting Anthropic's Claude API.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"features","dir":"Reference","previous_headings":"","what":"Features","title":"Client for Anthropic's Claude API — Anthropic","text":"Client-side conversation state management Client-side tools Server-side tools Multimodal inputs (files, images, PDFs, R objects) File uploads management Prompt caching Extended thinking support Structured outputs","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"useful-links","dir":"Reference","previous_headings":"","what":"Useful links","title":"Client for Anthropic's Claude API — Anthropic","text":"API reference: https://docs.claude.com/en/api/overview API docs: https://docs.claude.com/en/docs/intro","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"main-entrypoints","dir":"Reference","previous_headings":"","what":"Main entrypoints","title":"Client for Anthropic's Claude API — Anthropic","text":"chat(): Multi-turn multimodal conversations tool use structured outputs.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"server-side-tools","dir":"Reference","previous_headings":"","what":"Server-side tools","title":"Client for Anthropic's Claude API — Anthropic","text":"\"code_execution\" bash commands file operations (pricing: $0.05/session-hour, 5-min min) \"web_search\" web search capabilities. Can also list search_options. \"web_fetch\" fetch content URL provided prompt.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"structured-outputs","dir":"Reference","previous_headings":"","what":"Structured outputs","title":"Client for Anthropic's Claude API — Anthropic","text":"Hybrid approach structured outputs: Native support Sonnet Opus models via JSON schema. Function-call trick Haiku models code_execution enabled: uses tool calling simulate structured outputs, requiring additional API query full chat history (incurs extra cost).","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Client for Anthropic's Claude API — Anthropic","text":"argent::Provider -> Anthropic","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Client for Anthropic's Claude API — Anthropic","text":"default_beta_features Character vector. Default beta features use API requests","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Client for Anthropic's Claude API — Anthropic","text":"argent::Provider$download_generated_files() argent::Provider$dump_history() argent::Provider$get_auto_save_history() argent::Provider$get_chat_history() argent::Provider$get_content_text() argent::Provider$get_generated_code() argent::Provider$get_generated_files() argent::Provider$get_history() argent::Provider$get_history_file_path() argent::Provider$get_last_response() argent::Provider$get_rate_limit() argent::Provider$get_reasoning_text() argent::Provider$get_session_cumulative_token_count() argent::Provider$get_session_history() argent::Provider$get_session_last_token_count() argent::Provider$get_supplementary() argent::Provider$load_history() argent::Provider$print() argent::Provider$reset_history() argent::Provider$set_auto_save_history() argent::Provider$set_history() argent::Provider$set_rate_limit()","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Client for Anthropic's Claude API — Anthropic","text":"Anthropic$new() Anthropic$list_models() Anthropic$get_model_info() Anthropic$upload_file() Anthropic$get_file_metadata() Anthropic$download_file() Anthropic$list_files() Anthropic$delete_file() Anthropic$embeddings() Anthropic$chat() Anthropic$clone()","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Client for Anthropic's Claude API — Anthropic","text":"Initialize new Anthropic client","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Anthropic's Claude API — Anthropic","text":"","code":"Anthropic$new(   base_url = \"https://api.anthropic.com\",   api_key = Sys.getenv(\"ANTHROPIC_API_KEY\"),   provider_name = \"Anthropic\",   rate_limit = 50/60,   server_tools = c(\"code_execution\", \"web_search\", \"web_fetch\"),   default_model = \"claude-haiku-4-5-20251001\",   auto_save_history = TRUE )"},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Anthropic's Claude API — Anthropic","text":"base_url Character. Base URL API (default: \"https://api.anthropic.com\") api_key Character. API key (default: ANTHROPIC_API_KEY env var) provider_name Character. Provider name (default: \"Anthropic\") rate_limit Numeric. Rate limit requests per second (default: 50/60) server_tools Character vector. Server-side tools available (default: c(\"code_execution\", \"web_search\", \"web_fetch\")) default_model Character. Default model use chat requests (default: \"claude-haiku-4-5-20251001\") auto_save_history Logical. Enable/disable automatic history sync (default: TRUE)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"method-list-models-","dir":"Reference","previous_headings":"","what":"Method list_models()","title":"Client for Anthropic's Claude API — Anthropic","text":"List available Anthropic models","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Anthropic's Claude API — Anthropic","text":"","code":"Anthropic$list_models()"},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Anthropic's Claude API — Anthropic","text":"Data frame. Available models specifications","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"method-get-model-info-","dir":"Reference","previous_headings":"","what":"Method get_model_info()","title":"Client for Anthropic's Claude API — Anthropic","text":"Get information specific Anthropic model","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Anthropic's Claude API — Anthropic","text":"","code":"Anthropic$get_model_info(model_id)"},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Anthropic's Claude API — Anthropic","text":"model_id Character. Model ID (e.g., \"claude-sonnet-4-5-20250929\")","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Anthropic's Claude API — Anthropic","text":"List. Model information","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"method-upload-file-","dir":"Reference","previous_headings":"","what":"Method upload_file()","title":"Client for Anthropic's Claude API — Anthropic","text":"Upload file Anthropic Files API","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Anthropic's Claude API — Anthropic","text":"","code":"Anthropic$upload_file(file_path, file_name = NULL)"},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Anthropic's Claude API — Anthropic","text":"file_path Character. Path file upload, URL remote file file_name Character. Optional custom name uploaded file. NULL (default), uses basename file_path.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Anthropic's Claude API — Anthropic","text":"List. File metadata NULL error","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"method-get-file-metadata-","dir":"Reference","previous_headings":"","what":"Method get_file_metadata()","title":"Client for Anthropic's Claude API — Anthropic","text":"Get metadata uploaded file","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Anthropic's Claude API — Anthropic","text":"","code":"Anthropic$get_file_metadata(file_id)"},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Anthropic's Claude API — Anthropic","text":"file_id Character. File ID (e.g., \"file-abc123\")","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Anthropic's Claude API — Anthropic","text":"List. File metadata NULL error","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"method-download-file-","dir":"Reference","previous_headings":"","what":"Method download_file()","title":"Client for Anthropic's Claude API — Anthropic","text":"Download file content. files created Anthropic's Code Execution tool can downloaded. Downloads file content saves specified path. dest_path directory, file saved original filename. dest_path file path, used complete destination path.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Anthropic's Claude API — Anthropic","text":"","code":"Anthropic$download_file(file_id, dest_path = \"data\", overwrite = TRUE)"},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Anthropic's Claude API — Anthropic","text":"file_id Character. File ID (e.g., \"file-abc123\") dest_path Character. Destination path (default: \"data\"). Can either directory path complete file path. NULL, returns content raw bytes without saving. Created exist. overwrite Logical. Whether overwrite existing files (default: TRUE).","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Anthropic's Claude API — Anthropic","text":"Raw vector (dest_path NULL) Character path downloaded file (invisibly)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for Anthropic's Claude API — Anthropic","text":"","code":"\\dontrun{ anthropic <- Anthropic$new()  # Download to a directory path <- anthropic$download_file(file_id = \"file-abc123\", dest_path = \"downloads\")  # Download with specific filename path <- anthropic$download_file(file_id = \"file-abc123\", dest_path = \"downloads/myfile.txt\")  # Get raw bytes without saving raw_content <- anthropic$download_file(file_id = \"file-abc123\", dest_path = NULL) }"},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"method-list-files-","dir":"Reference","previous_headings":"","what":"Method list_files()","title":"Client for Anthropic's Claude API — Anthropic","text":"List uploaded files","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Anthropic's Claude API — Anthropic","text":"","code":"Anthropic$list_files(   limit = 20,   before_id = NULL,   after_id = NULL,   as_df = TRUE )"},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Anthropic's Claude API — Anthropic","text":"limit Integer. Number files per page (default: 20, max: 1000) before_id Character. ID pagination (optional) after_id Character. ID pagination (optional) as_df Logical. Whether return files data frame (default: TRUE). FALSE, returns raw list.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Anthropic's Claude API — Anthropic","text":"Data frame (as_df = TRUE) list (as_df = FALSE). Files metadata pagination info.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"method-delete-file-","dir":"Reference","previous_headings":"","what":"Method delete_file()","title":"Client for Anthropic's Claude API — Anthropic","text":"Delete uploaded file","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Anthropic's Claude API — Anthropic","text":"","code":"Anthropic$delete_file(file_id)"},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Anthropic's Claude API — Anthropic","text":"file_id Character. File ID (e.g., \"file-abc123\")","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Anthropic's Claude API — Anthropic","text":"Logical. TRUE successful, FALSE otherwise","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"method-embeddings-","dir":"Reference","previous_headings":"","what":"Method embeddings()","title":"Client for Anthropic's Claude API — Anthropic","text":"Generate embeddings text input Note: Anthropic provide native embeddings API. embeddings, consider using: Voyage AI (recommended Anthropic) OpenAI (text-embedding-3-small, text-embedding-3-large) Google (text-embedding-004) embedding providers available OpenRouter","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Anthropic's Claude API — Anthropic","text":"","code":"Anthropic$embeddings(...)"},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Anthropic's Claude API — Anthropic","text":"... Arguments (used, included method signature consistency)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Anthropic's Claude API — Anthropic","text":"method always throws error","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"method-chat-","dir":"Reference","previous_headings":"","what":"Method chat()","title":"Client for Anthropic's Claude API — Anthropic","text":"Send chat completion request Anthropic","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Anthropic's Claude API — Anthropic","text":"","code":"Anthropic$chat(   ...,   cache_prompt = FALSE,   model = self$default_model,   system = .default_system_prompt,   cache_system = FALSE,   max_tokens = 4096,   temperature = 1,   top_p = NULL,   top_k = NULL,   tools = NULL,   tool_choice = list(type = \"auto\"),   cache_tools = FALSE,   output_schema = NULL,   thinking_budget = 0 )"},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Anthropic's Claude API — Anthropic","text":"... One inputs prompt. Can text strings, file paths, URLs, R objects, content wrapped as_*_content() functions. R objects (plain strings) include names structure context sent model. cache_prompt Logical. Cache prompt reuse (default: FALSE) model Character. Model use (default: \"claude-haiku-4-5-20251001\") system Character. System instructions (optional) cache_system Logical. Cache system instructions (default: FALSE) max_tokens Integer. Maximum tokens generate (default: 4096) temperature Numeric. Sampling temperature (default: 1). Range: 0.0-1.0 top_p Numeric. Nucleus sampling - use cumulative probability distribution. Range: 0.0-1.0 (optional). used instead temperature advanced use cases top_k Integer. Sample top K options . Minimum: 0 (optional). advanced use cases tools List. Tool definitions (server-side client-side functions). Server-side tools: \"code_execution\" bash commands file operations (pricing: $0.05/session-hour, 5-min min) list(type = \"code_execution\", container = container_id) reuse existing container \"web_search\" web search capabilities. Can also list search_options. \"web_fetch\" fetch content URL provided prompt. Client-side functions: created as_tool(fn) tool() helpers. Note: Code execution internet access, 5GB RAM/disk, 1 CPU. Containers expire 30 days creation. tool_choice List. Tool choice configuration: list(type = \"auto\"), list(type = \"\"), list(type = \"tool\", name = \"tool_name\"), NULL none (default: list(type = \"auto\")) cache_tools Logical. Cache tool definitions (default: FALSE) output_schema List. JSON schema structured output (optional) thinking_budget Integer. Thinking budget tokens: 0 (disabled), 1024-max_tokens (default: 0). Minimum 1024 tokens.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Anthropic's Claude API — Anthropic","text":"Character. Text response model.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Client for Anthropic's Claude API — Anthropic","text":"objects class cloneable method.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Anthropic's Claude API — Anthropic","text":"","code":"Anthropic$clone(deep = FALSE)"},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Anthropic's Claude API — Anthropic","text":"deep Whether make deep clone.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Anthropic.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for Anthropic's Claude API — Anthropic","text":"","code":"if (FALSE) { # \\dontrun{ # Initialize with API key from environment anthropic <- Anthropic$new()  # Or provide API key explicitly anthropic <- Anthropic$new(api_key = \"your-api-key\")  # Simple chat completion response <- anthropic$chat(   prompt = \"What is R programming?\",   model = \"claude-sonnet-4-5-20250929\" )  # With prompt caching response <- anthropic$chat(   prompt = \"Analyze this data\",   cache_prompt = TRUE,   cache_system = TRUE )  # With extended thinking (10k token budget) response <- anthropic$chat(   prompt = \"Solve this complex math problem: ...\",   thinking_budget = 10000 )  # With custom thinking budget response <- anthropic$chat(   prompt = \"Analyze this complex scenario: ...\",   thinking_budget = 16000 ) } # }  ## ------------------------------------------------ ## Method `Anthropic$download_file` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ anthropic <- Anthropic$new()  # Download to a directory path <- anthropic$download_file(file_id = \"file-abc123\", dest_path = \"downloads\")  # Download with specific filename path <- anthropic$download_file(file_id = \"file-abc123\", dest_path = \"downloads/myfile.txt\")  # Get raw bytes without saving raw_content <- anthropic$download_file(file_id = \"file-abc123\", dest_path = NULL) } # }"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":null,"dir":"Reference","previous_headings":"","what":"Client for Google's Gemini API — Google","title":"Client for Google's Gemini API — Google","text":"R6 class interacting Google's Gemini API.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"features","dir":"Reference","previous_headings":"","what":"Features","title":"Client for Google's Gemini API — Google","text":"Client-side conversation state management Client-side tools Server-side tools Multimodal inputs (files, images, PDFs, R objects) File uploads management Server-side RAG stores & file_search server tool Reasoning Structured outputs","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"useful-links","dir":"Reference","previous_headings":"","what":"Useful links","title":"Client for Google's Gemini API — Google","text":"API reference: https://ai.google.dev/api/generate-content API docs: https://ai.google.dev/gemini-api/docs","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"main-entrypoints","dir":"Reference","previous_headings":"","what":"Main entrypoints","title":"Client for Google's Gemini API — Google","text":"chat(): Multi-turn multimodal conversations tool use structured outputs. embeddings(): Vector embeddings text inputs.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"server-side-tools","dir":"Reference","previous_headings":"","what":"Server-side tools","title":"Client for Google's Gemini API — Google","text":"code_execution: Execute Python code google_search: Web search grounding url_context: Fetch process URLs google_maps: Location-aware data file_search: Search uploaded files","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"structured-outputs","dir":"Reference","previous_headings":"","what":"Structured outputs","title":"Client for Google's Gemini API — Google","text":"Hybrid approach structured outputs: Native support Sonnet Pro models via JSON schema. Function-call trick models: uses tool calling simulate structured outputs, requiring additional API query full chat history (incurs extra cost).","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Client for Google's Gemini API — Google","text":"argent::Provider -> Google","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Client for Google's Gemini API — Google","text":"argent::Provider$download_generated_files() argent::Provider$dump_history() argent::Provider$get_auto_save_history() argent::Provider$get_chat_history() argent::Provider$get_content_text() argent::Provider$get_generated_code() argent::Provider$get_generated_files() argent::Provider$get_history() argent::Provider$get_history_file_path() argent::Provider$get_last_response() argent::Provider$get_rate_limit() argent::Provider$get_reasoning_text() argent::Provider$get_session_cumulative_token_count() argent::Provider$get_session_history() argent::Provider$get_session_last_token_count() argent::Provider$get_supplementary() argent::Provider$load_history() argent::Provider$print() argent::Provider$reset_history() argent::Provider$set_auto_save_history() argent::Provider$set_history() argent::Provider$set_rate_limit()","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Client for Google's Gemini API — Google","text":"Google$new() Google$list_models() Google$get_model_info() Google$embeddings() Google$upload_file() Google$get_file_metadata() Google$list_files() Google$delete_file() Google$create_store() Google$list_stores() Google$read_store() Google$delete_store() Google$add_file_to_store() Google$import_file_to_store() Google$list_files_in_store() Google$read_file_from_store() Google$delete_file_from_store() Google$query_file() Google$get_operation() Google$chat() Google$clone()","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Client for Google's Gemini API — Google","text":"Initialize new Google Gemini client","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$new(   base_url = \"https://generativelanguage.googleapis.com\",   api_key = Sys.getenv(\"GEMINI_API_KEY\"),   provider_name = \"Google\",   rate_limit = 5/60,   server_tools = c(\"code_execution\", \"google_search\", \"url_context\", \"google_maps\",     \"file_search\"),   default_model = \"gemini-2.5-flash\",   auto_save_history = TRUE )"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"base_url Character. Base URL API (default: \"https://generativelanguage.googleapis.com\") api_key Character. API key (default: GEMINI_API_KEY env var) provider_name Character. Provider name (default: \"Google\") rate_limit Numeric. Rate limit requests per second (default: 5/60, free tier 2.5 Pro) server_tools Character vector. Server-side tools available (default: c(\"code_execution\", \"google_search\", \"url_context\", \"google_maps\", \"file_search\")) default_model Character. Default model use chat requests (default: \"gemini-2.5-flash\") auto_save_history Logical. Enable/disable automatic history sync (default: TRUE)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-list-models-","dir":"Reference","previous_headings":"","what":"Method list_models()","title":"Client for Google's Gemini API — Google","text":"List available Google models","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$list_models()"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"Data frame. Available models specifications","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-get-model-info-","dir":"Reference","previous_headings":"","what":"Method get_model_info()","title":"Client for Google's Gemini API — Google","text":"Get information specific Google model","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$get_model_info(model_id)"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"model_id Character. Model ID (e.g., \"gemini-2.5-pro\")","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"List. Model information","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-embeddings-","dir":"Reference","previous_headings":"","what":"Method embeddings()","title":"Client for Google's Gemini API — Google","text":"Generate embeddings text input using Google's embedding models","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$embeddings(   input,   model,   task_type = NULL,   output_dimensionality = NULL,   return_full_response = FALSE )"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"input Character vector. Text(s) embed model Character. Model use (e.g., \"text-embedding-004\", \"text-embedding-preview-0815\") task_type Character. Task type optimization (optional). One : \"RETRIEVAL_QUERY\": search queries \"RETRIEVAL_DOCUMENT\": documents search corpus \"SEMANTIC_SIMILARITY\": similarity comparison \"CLASSIFICATION\": text classification \"CLUSTERING\": clustering tasks output_dimensionality Integer. Number dimensions output (optional, 128-3072 supported models) return_full_response Logical. Return full API response (default: FALSE)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"Numeric matrix (List return_full_response = TRUE). Embeddings one row per input text","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for Google's Gemini API — Google","text":"","code":"\\dontrun{ google <- Google$new()  # Generate embeddings embeddings <- google$embeddings(   input = c(\"Hello world\", \"How are you?\"),   model = \"text-embedding-004\" )  # With task type for optimization embeddings <- google$embeddings(   input = \"Sample query\",   model = \"text-embedding-004\",   task_type = \"RETRIEVAL_QUERY\" )  # With dimension reduction embeddings <- google$embeddings(   input = \"Sample text\",   model = \"text-embedding-004\",   output_dimensionality = 256 ) }"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-upload-file-","dir":"Reference","previous_headings":"","what":"Method upload_file()","title":"Client for Google's Gemini API — Google","text":"Upload file Google Files API (uses resumable upload)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$upload_file(file_path, name = NULL, mime_type = NULL)"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"file_path Character. Path file upload, URL remote file name Character. Display name file (optional, uses filename NULL) mime_type Character. MIME type (optional, auto-detected NULL)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"List. File metadata NULL error","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-get-file-metadata-","dir":"Reference","previous_headings":"","what":"Method get_file_metadata()","title":"Client for Google's Gemini API — Google","text":"Get metadata uploaded file","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$get_file_metadata(file_name)"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"file_name Character. File name (e.g., \"files/abc123\")","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"List. File metadata NULL error","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-list-files-","dir":"Reference","previous_headings":"","what":"Method list_files()","title":"Client for Google's Gemini API — Google","text":"List uploaded files","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$list_files(page_size = 100, page_token = NULL, as_df = TRUE)"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"page_size Integer. Number files per page (default: 100) page_token Character. Page token pagination (optional) as_df Logical. Whether return files data frame (default: TRUE). FALSE, returns raw list.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"Data frame (as_df = TRUE) list otherwise. Files metadata next page token (list ).","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-delete-file-","dir":"Reference","previous_headings":"","what":"Method delete_file()","title":"Client for Google's Gemini API — Google","text":"Delete uploaded file","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$delete_file(file_name)"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"file_name Character. File name (e.g., \"files/abc123\")","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"Logical. TRUE successful, FALSE otherwise","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-create-store-","dir":"Reference","previous_headings":"","what":"Method create_store()","title":"Client for Google's Gemini API — Google","text":"Create new file search store RAG operations","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$create_store(name = NULL)"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"name Character. Display name store (optional)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"List. File search store metadata NULL error","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-list-stores-","dir":"Reference","previous_headings":"","what":"Method list_stores()","title":"Client for Google's Gemini API — Google","text":"List file search stores","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$list_stores(page_size = 10, page_token = NULL, as_df = TRUE)"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"page_size Integer. Number stores per page (default: 10, max: 100) page_token Character. Page token pagination (optional) as_df Logical. Whether return stores data frame (default: TRUE). FALSE, returns raw list.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"Data frame (as_df = TRUE) list otherwise. File search stores metadata.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-read-store-","dir":"Reference","previous_headings":"","what":"Method read_store()","title":"Client for Google's Gemini API — Google","text":"Get information specific file search store","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$read_store(name)"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"name Character. Store name (e.g., \"fileSearchStores/abc123\")","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"List. File search store metadata NULL error","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-delete-store-","dir":"Reference","previous_headings":"","what":"Method delete_store()","title":"Client for Google's Gemini API — Google","text":"Delete file search store","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$delete_store(name, force = FALSE)"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"name Character. Store name (e.g., \"fileSearchStores/abc123\") force Logical. TRUE, delete store even contains documents (default: FALSE)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns-10","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"Logical. TRUE successful, FALSE otherwise","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-add-file-to-store-","dir":"Reference","previous_headings":"","what":"Method add_file_to_store()","title":"Client for Google's Gemini API — Google","text":"Upload file directly file search store using resumable upload. Supports custom chunking configuration metadata.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$add_file_to_store(   file_path,   store_name,   file_name = NULL,   custom_metadata = NULL,   chunking_config = NULL )"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-11","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"file_path Character. Path file upload, URL remote file store_name Character. Store name (e.g., \"fileSearchStores/abc123\") file_name Character. Display name file (optional, uses filename NULL) custom_metadata List. Custom metadata key-value pairs (optional) chunking_config List. Chunking configuration max_tokens_per_chunk max_overlap_tokens (optional)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns-11","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"List. Document metadata NULL error","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-import-file-to-store-","dir":"Reference","previous_headings":"","what":"Method import_file_to_store()","title":"Client for Google's Gemini API — Google","text":"Import existing File API file file search store. Note: importFile endpoint support custom chunking configuration. Files chunked automatically Google's API.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$import_file_to_store(file_name, store_name, custom_metadata = NULL)"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-12","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"file_name Character. File name File API (e.g., \"files/abc123\") store_name Character. Store name (e.g., \"fileSearchStores/abc123\") custom_metadata List. Custom metadata key-value pairs (optional)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns-12","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"List. Document metadata NULL error","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-list-files-in-store-","dir":"Reference","previous_headings":"","what":"Method list_files_in_store()","title":"Client for Google's Gemini API — Google","text":"List files file search store","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$list_files_in_store(   store_name,   page_size = 10,   page_token = NULL,   as_df = TRUE )"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-13","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"store_name Character. Store name (e.g., \"fileSearchStores/abc123\") page_size Integer. Number files per page (default: 10, max: 100) page_token Character. Page token pagination (optional) as_df Logical. Whether return files data frame (default: TRUE). FALSE, returns raw list.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns-13","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"Data frame (as_df = TRUE) list otherwise. Files metadata.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-read-file-from-store-","dir":"Reference","previous_headings":"","what":"Method read_file_from_store()","title":"Client for Google's Gemini API — Google","text":"Get information specific file store","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$read_file_from_store(file_name)"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-14","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"file_name Character. File name (e.g., \"fileSearchStores/xyz/documents/abc123\")","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns-14","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"List. File metadata NULL error","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-delete-file-from-store-","dir":"Reference","previous_headings":"","what":"Method delete_file_from_store()","title":"Client for Google's Gemini API — Google","text":"Delete file file search store","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-16","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$delete_file_from_store(file_name, force = TRUE)"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-15","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"file_name Character. File name (e.g., \"fileSearchStores/xyz/documents/abc123\") force Logical. TRUE, force deletion non-empty documents (default: TRUE)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns-15","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"Logical. TRUE successful, FALSE otherwise","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-query-file-","dir":"Reference","previous_headings":"","what":"Method query_file()","title":"Client for Google's Gemini API — Google","text":"Query specific file file search store","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-17","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$query_file(   file_name,   query,   results_count = 10,   metadata_filters = NULL )"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-16","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"file_name Character. File name (e.g., \"fileSearchStores/xyz/documents/abc123\") query Character. Query string results_count Integer. Number results return (default: 10) metadata_filters Character. Metadata filter string (optional, e.g., \"author=John\")","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns-16","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"List. Query results NULL error","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-get-operation-","dir":"Reference","previous_headings":"","what":"Method get_operation()","title":"Client for Google's Gemini API — Google","text":"Get status long-running operation","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-18","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$get_operation(operation_name)"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-17","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"operation_name Character. Operation name (e.g., \"fileSearchStores/xyz/operations/abc123\")","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns-17","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"List. Operation status NULL error","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-chat-","dir":"Reference","previous_headings":"","what":"Method chat()","title":"Client for Google's Gemini API — Google","text":"Send chat completion request Google Note thinking function calling: thinking function calling enabled, model returns thought signatures response. encrypted representations model's thought process automatically included subsequent turns via chat_history, allowing model maintain thought context across multi-turn conversations. However, thought signatures increase input token costs sent back requests. See https://ai.google.dev/gemini-api/docs/thinking#signatures","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-19","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$chat(   ...,   model = self$default_model,   system = .default_system_prompt,   max_tokens = 8000,   temperature = 1,   top_p = NULL,   top_k = NULL,   stop_sequences = NULL,   tools = NULL,   tool_choice = list(mode = \"AUTO\"),   output_schema = NULL,   thinking_budget = 0,   include_thoughts = FALSE )"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-18","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"... One inputs prompt. Can text strings, file paths, URLs, R objects, content wrapped as_*_content() functions. R objects (plain strings) include names structure context sent model. model Character. Model use (default: \"gemini-2.5-flash\") system Character. System instructions (optional) max_tokens Integer. Maximum tokens generate (default: 8000) temperature Numeric. Sampling temperature 0-2 (default: 1) top_p Numeric. Nucleus sampling - cumulative probability cutoff. Range: 0.0-1.0 (optional) top_k Integer. Top-K sampling - sample top K options. Range: 1+ (optional) stop_sequences Character vector. Sequences stop generation encountered (optional) tools List. Function definitions tool calling (optional). Supports client-side functions server-side tools: Server-side tools: \"code_execution\" list(type = \"code_execution\", file_ids = list(\"files/xyz\")) - Execute Python code \"google_search\" - Web search grounding \"url_context\" - Fetch process URLs \"google_maps\" list(type = \"google_maps\", enable_widget = TRUE, location = list(latitude, longitude)) - Location-aware data \"file_search\" list(type = \"file_search\", store_names = list(\"fileSearchStores/xyz\"), metadata_filter = \"key=value\") - Search uploaded files Client-side functions: Use as_tool(fn) wrap R functions tool calling. tool_choice Character List. Controls model uses function declarations (default: list(mode = \"AUTO\")). applies client function tools, server tools. Modes: \"AUTO\" (default): Model decides whether call functions respond natural language \"\": Model must always call function (never responds natural language) \"NONE\": Model call functions (temporarily disable without removing tool definitions) \"VALIDATED\" (Preview): Model can call functions respond naturally, schema validation Limiting function selection (optional): mode \"\" \"VALIDATED\", can specify functions allowed: list(mode = \"\", allowed_function_names = c(\"func1\", \"func2\")) Examples: tool_choice = \"AUTO\" - Simple mode specification (character) tool_choice = list(mode = \"\") - Force function call available function tool_choice = list(mode = \"\", allowed_function_names = c(\"get_weather\")) - Force specific function output_schema List. JSON schema structured output (optional) thinking_budget Integer. Thinking budget tokens: 0 (disabled), -1 (dynamic), 0-24575/32768 (fixed budget). Default: 0. include_thoughts Logical. Whether include thought parts response (default: FALSE). TRUE thinking_budget 0, warning issued include_thoughts set FALSE.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"returns-18","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for Google's Gemini API — Google","text":"Character. Text response model.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Client for Google's Gemini API — Google","text":"objects class cloneable method.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"usage-20","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for Google's Gemini API — Google","text":"","code":"Google$clone(deep = FALSE)"},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"arguments-19","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for Google's Gemini API — Google","text":"deep Whether make deep clone.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Google.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for Google's Gemini API — Google","text":"","code":"if (FALSE) { # \\dontrun{ # Initialize with API key from environment google <- Google$new()  # Or provide API key explicitly google <- Google$new(api_key = \"your-api-key\")  # Simple chat completion response <- google$chat(   \"What is R programming?\",   model = \"gemini-2.5-pro\" )  # With system instructions response <- google$chat(   \"Explain quantum computing\",   model = \"gemini-2.5-pro\",   system = \"You are a physics professor\" )  # With R objects (names captured automatically) my_data <- mtcars response <- google$chat(   my_data, \"Analyze this dataset\",   model = \"gemini-2.5-pro\" )  # With context caching for repeated queries cache_name <- google$create_cache(   model = \"gemini-2.5-flash\",   system = \"You are an expert data analyst\",   ttl = \"3600s\" ) } # }  ## ------------------------------------------------ ## Method `Google$embeddings` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ google <- Google$new()  # Generate embeddings embeddings <- google$embeddings(   input = c(\"Hello world\", \"How are you?\"),   model = \"text-embedding-004\" )  # With task type for optimization embeddings <- google$embeddings(   input = \"Sample query\",   model = \"text-embedding-004\",   task_type = \"RETRIEVAL_QUERY\" )  # With dimension reduction embeddings <- google$embeddings(   input = \"Sample text\",   model = \"text-embedding-004\",   output_dimensionality = 256 ) } # }"},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":null,"dir":"Reference","previous_headings":"","what":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"R6 class interacting local LLM servers (e.g., llama.cpp, Ollama) implement OpenAI-compatible APIs.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"features","dir":"Reference","previous_headings":"","what":"Features","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"Client-side conversation state management Client-side tools Multimodal inputs (images, PDFs) Extended thinking support (server-configured) Structured outputs","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"useful-links","dir":"Reference","previous_headings":"","what":"Useful links","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"llama-server (llama.cpp) reference: https://github.com/ggml-org/llama.cpp/tree/master/tools/server","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"main-entrypoints","dir":"Reference","previous_headings":"","what":"Main entrypoints","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"chat(): Multi-turn multimodal conversations tool use structured outputs. embeddings(): Vector embeddings text inputs.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"structured-outputs","dir":"Reference","previous_headings":"","what":"Structured Outputs","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"Function-call trick models: uses tool calling simulate structured outputs, always requiring additional API query full chat history (incurs extra cost).","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"argent::Provider -> LocalLLM","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"argent::Provider$download_generated_files() argent::Provider$dump_history() argent::Provider$get_auto_save_history() argent::Provider$get_chat_history() argent::Provider$get_content_text() argent::Provider$get_generated_code() argent::Provider$get_generated_files() argent::Provider$get_history() argent::Provider$get_history_file_path() argent::Provider$get_last_response() argent::Provider$get_rate_limit() argent::Provider$get_reasoning_text() argent::Provider$get_session_cumulative_token_count() argent::Provider$get_session_history() argent::Provider$get_session_last_token_count() argent::Provider$get_supplementary() argent::Provider$load_history() argent::Provider$print() argent::Provider$reset_history() argent::Provider$set_auto_save_history() argent::Provider$set_history() argent::Provider$set_rate_limit()","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"LocalLLM$new() LocalLLM$get_default_model_id() LocalLLM$set_default_model_id() LocalLLM$get_model_name() LocalLLM$list_models() LocalLLM$chat() LocalLLM$embeddings() LocalLLM$clone()","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"Initialize new Local LLM client","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"","code":"LocalLLM$new(   base_url = \"http://localhost:5000\",   api_key = \"not-needed\",   provider_name = \"LocalLLM\",   rate_limit = 999999,   server_tools = character(0),   default_model = NULL,   auto_save_history = TRUE )"},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"base_url Character. Base URL local server (default: \"http://localhost:5000\") api_key Character. API key (default: \"-needed\") provider_name Character. Provider name (default: \"LocalLLM\") rate_limit Numeric. Rate limit requests per second (default: 999999) server_tools Character vector. Server-side tools available (default: character(0)) default_model Character. Default model name (auto-detected NULL) auto_save_history Logical. Enable/disable automatic history sync (default: TRUE)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"method-get-default-model-id-","dir":"Reference","previous_headings":"","what":"Method get_default_model_id()","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"Get current model","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"","code":"LocalLLM$get_default_model_id()"},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"Character. Model name","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"method-set-default-model-id-","dir":"Reference","previous_headings":"","what":"Method set_default_model_id()","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"Set model use","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"","code":"LocalLLM$set_default_model_id(model)"},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"model Character. Model name","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"method-get-model-name-","dir":"Reference","previous_headings":"","what":"Method get_model_name()","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"Get model name (basename)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"","code":"LocalLLM$get_model_name()"},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"Character. Model basename","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"method-list-models-","dir":"Reference","previous_headings":"","what":"Method list_models()","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"List available models local server","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"","code":"LocalLLM$list_models()"},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"Data frame. Available models","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"method-chat-","dir":"Reference","previous_headings":"","what":"Method chat()","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"Send chat completion request local LLM","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"","code":"LocalLLM$chat(   ...,   system = .default_system_prompt,   model = self$default_model,   temperature = 1,   max_tokens = 4096,   top_p = NULL,   top_k = NULL,   min_p = NULL,   repeat_penalty = NULL,   presence_penalty = NULL,   frequency_penalty = NULL,   mirostat = NULL,   mirostat_tau = NULL,   mirostat_eta = NULL,   seed = NULL,   tools = NULL,   tool_choice = \"auto\",   output_schema = NULL )"},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"... One inputs prompt. Can text strings, file paths, URLs, R objects, content wrapped as_*_content() functions. R objects (plain strings) include names structure context sent model. system Character. System instructions (optional) model Character. Model use (default: current model) temperature Numeric. Sampling temperature (default: 1) max_tokens Integer. Maximum tokens generate (default: 4096) top_p Numeric. Top-p (nucleus) sampling (default: 0.9, 1.0 = disabled) top_k Integer. Top-k sampling (default: 40, 0 = disabled) min_p Numeric. Min-p sampling (default: 0.1, 0.0 = disabled) repeat_penalty Numeric. Penalize repeat sequence tokens (default: 1.0, 1.0 = disabled) presence_penalty Numeric. Repeat alpha presence penalty (default: 0.0, 0.0 = disabled) frequency_penalty Numeric. Repeat alpha frequency penalty (default: 0.0, 0.0 = disabled) mirostat Integer. Use Mirostat sampling (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0) mirostat_tau Numeric. Mirostat target entropy, parameter tau (default: 5.0) mirostat_eta Numeric. Mirostat learning rate, parameter eta (default: 0.1) seed Integer. RNG seed (default: -1, use random seed -1) tools List. Function definitions tool calling (optional) tool_choice Character List. Tool choice mode (default: \"auto\") output_schema List. JSON schema structured output (optional)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"Character. Text response model.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"method-embeddings-","dir":"Reference","previous_headings":"","what":"Method embeddings()","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"Generate embeddings text input","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"","code":"LocalLLM$embeddings(input, model = NULL)"},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"input Character vector. Text(s) embed model Character. Model use (default: current model)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"Numeric matrix. Embeddings one row per input text","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"objects class cloneable method.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"","code":"LocalLLM$clone(deep = FALSE)"},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"deep Whether make deep clone.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/LocalLLM.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for local LLM servers (e.g., llama.cpp, Ollama) — LocalLLM","text":"","code":"if (FALSE) { # \\dontrun{ # Connect to local llama.cpp server llm <- LocalLLM$new(base_url = \"http://localhost:8080\")  # With specific model llm <- LocalLLM$new(   base_url = \"http://localhost:8080\",   model = \"llama-3-8b\" )  # Simple chat completion response <- llm$chat(   prompt = \"What is R programming?\",   temperature = 0.7 )  # With additional sampling parameters response <- llm$chat(   prompt = \"Explain quantum computing\",   temperature = 0.8,   top_p = 0.9,   top_k = 40,   min_p = 0.05,   repeat_penalty = 1.1 ) } # }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":null,"dir":"Reference","previous_headings":"","what":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Parent class OpenAI API clients (Responses, Chat Completions, Assistants). Provides shared infrastructure file management, vector stores, API requests. class inherited OpenAI_Chat, OpenAI_Responses, OpenAI_Assistant provide consistent interfaces across different OpenAI API clients.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"argent::Provider -> OpenAI","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"argent::Provider$download_generated_files() argent::Provider$dump_history() argent::Provider$get_auto_save_history() argent::Provider$get_chat_history() argent::Provider$get_content_text() argent::Provider$get_generated_code() argent::Provider$get_generated_files() argent::Provider$get_history() argent::Provider$get_history_file_path() argent::Provider$get_last_response() argent::Provider$get_rate_limit() argent::Provider$get_reasoning_text() argent::Provider$get_session_cumulative_token_count() argent::Provider$get_session_history() argent::Provider$get_session_last_token_count() argent::Provider$get_supplementary() argent::Provider$load_history() argent::Provider$print() argent::Provider$reset_history() argent::Provider$set_auto_save_history() argent::Provider$set_history() argent::Provider$set_rate_limit()","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"OpenAI$new() OpenAI$list_models() OpenAI$find_models() OpenAI$get_model_info() OpenAI$embeddings() OpenAI$list_assistants() OpenAI$find_assistants() OpenAI$upload_file() OpenAI$upload_file_from_df() OpenAI$list_files() OpenAI$find_file() OpenAI$get_file() OpenAI$get_file_content() OpenAI$download_file() OpenAI$delete_file() OpenAI$delete_files() OpenAI$delete_all_files() OpenAI$create_store() OpenAI$read_store() OpenAI$list_stores() OpenAI$find_store() OpenAI$update_store() OpenAI$delete_store() OpenAI$delete_stores() OpenAI$delete_all_stores() OpenAI$delete_store_and_files() OpenAI$add_file_to_store() OpenAI$read_file_from_store() OpenAI$list_files_in_store() OpenAI$find_file_in_store() OpenAI$delete_file_from_store() OpenAI$delete_files_from_store() OpenAI$delete_all_files_from_store() OpenAI$clone()","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Initialize new OpenAI base client","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$new(   base_url = \"https://api.openai.com\",   api_key = Sys.getenv(\"OPENAI_API_KEY\"),   provider_name = \"OpenAI\",   rate_limit = 60/60,   server_tools = character(0),   default_model = \"gpt-5-mini\",   org = Sys.getenv(\"OPENAI_ORG\"),   auto_save_history = TRUE )"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"base_url Character. Base URL API (default: \"https://api.openai.com\") api_key Character. API key (default: OPENAI_API_KEY env var) provider_name Character. Provider name (default: \"OpenAI\") rate_limit Numeric. Rate limit requests per second (default: 60/60) server_tools Character vector. Server-side tools available (default: character(0)) default_model Character. Default model use chat requests (default: \"gpt-5-mini\") org Character. Organization ID (default: OPENAI_ORG env var) auto_save_history Logical. Enable/disable automatic history sync (default: TRUE)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-list-models-","dir":"Reference","previous_headings":"","what":"Method list_models()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"List available models","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$list_models()"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Data frame. Available models","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-find-models-","dir":"Reference","previous_headings":"","what":"Method find_models()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Find models matching criteria","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$find_models(...)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"... Named arguments filtering","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Data frame. Filtered models","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-get-model-info-","dir":"Reference","previous_headings":"","what":"Method get_model_info()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Get information specific model","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$get_model_info(model_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"model_id Character. ID model get information .","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"list containing information model.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-embeddings-","dir":"Reference","previous_headings":"","what":"Method embeddings()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Generate embeddings text input","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$embeddings(   input,   model,   encoding_format = \"float\",   dimensions = NULL,   return_full_response = FALSE )"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"input Character vector. Text(s) embed model Character. Model use (e.g., \"text-embedding-3-small\", \"text-embedding-3-large\") encoding_format Character. Format embeddings: \"float\" \"base64\" (default: \"float\") dimensions Integer. Number dimensions output (embedding-3 models) return_full_response Logical. Return full API response (default: FALSE)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Numeric matrix (List return_full_response = TRUE). Embeddings one row per input text","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"\\dontrun{ openai <- OpenAI_Chat$new()  # Generate embeddings embeddings <- openai$embeddings(   input = c(\"Hello world\", \"How are you?\"),   model = \"text-embedding-3-small\" )  # With dimension reduction embeddings <- openai$embeddings(   input = \"Sample text\",   model = \"text-embedding-3-large\",   dimensions = 256 ) }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-list-assistants-","dir":"Reference","previous_headings":"","what":"Method list_assistants()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"List assistants.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$list_assistants()"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"data frame assistants.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-find-assistants-","dir":"Reference","previous_headings":"","what":"Method find_assistants()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Find assistants matching criteria.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$find_assistants(..., as_df = TRUE)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"... Named arguments filtering. as_df Logical. Whether return data frame.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"data frame matching assistants.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-upload-file-","dir":"Reference","previous_headings":"","what":"Method upload_file()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Upload file OpenAI use across various endpoints/features. Files uploaded OpenAI can used features like Assistants, fine-tuning, Batch API, . Maximum file size 512 MB per file. Organizations 100 GB total storage limit across files.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$upload_file(file_path, file_name = NULL, purpose = \"user_data\")"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"file_path Character. Path file upload, URL remote file. file_name Character. Optional custom name uploaded file. NULL (default), uses basename file_path. purpose Character. intended purpose uploaded file. Must one : \"assistants\": use Assistants API server tools (file_search, code_interpreter) \"user_data\": user data storage retrieval (default) \"fine-tune\": fine-tuning custom models \"batch\": Batch API operations \"vision\": vision-related tasks","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"List. File object containing: id: Unique file identifier object: Object type (always \"file\") bytes: File size bytes created_at: Unix timestamp creation filename: Name file purpose: purpose specified upload status: Processing status (\"uploaded\", \"processed\", \"error\") status_details: Additional details status \"error\" (optional)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"examples-1","dir":"Reference","previous_headings":"","what":"Examples","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"\\dontrun{ openai <- OpenAI_Chat$new()  # Upload a document for assistants file <- openai$upload_file(\"document.pdf\", purpose = \"assistants\") cat(\"File ID:\", file$id, \"\\n\")  # Upload data with custom name file <- openai$upload_file(\"data.csv\", file_name = \"my_data.csv\", purpose = \"user_data\")  # Upload for fine-tuning file <- openai$upload_file(\"training_data.jsonl\", purpose = \"fine-tune\") }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-upload-file-from-df-","dir":"Reference","previous_headings":"","what":"Method upload_file_from_df()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Upload file data frame. Convenience method upload data frame tab-separated text file OpenAI. Creates temporary file, uploads , cleans .","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$upload_file_from_df(df, file_name = \"data.txt\", purpose = \"user_data\")"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"df Data frame. data upload. file_name Character. Name uploaded file (default: \"data.txt\"). purpose Character. purpose file (default: \"user_data\"). See upload_file() valid purposes.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"List. File object (see upload_file() structure).","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"examples-2","dir":"Reference","previous_headings":"","what":"Examples","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"\\dontrun{ openai <- OpenAI_Chat$new()  # Upload a data frame df <- data.frame(x = 1:10, y = letters[1:10]) file <- openai$upload_file_from_df(df, file_name = \"my_data.txt\") }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-list-files-","dir":"Reference","previous_headings":"","what":"Method list_files()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"List files belonging organization. Returns list files uploaded OpenAI, ordered creation date (recent first).","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$list_files(purpose = NULL)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"purpose Character. Optional filter purpose. provided, returns files specified purpose. Valid values: \"assistants\", \"user_data\", \"fine-tune\", \"batch\", \"vision\".","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Data frame columns: id: File identifier object: Object type (\"file\") bytes: File size created_at: Creation timestamp (POSIXct) filename: File name purpose: File purpose status: Processing status status_details: Error details applicable","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"examples-3","dir":"Reference","previous_headings":"","what":"Examples","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"\\dontrun{ openai <- OpenAI_Chat$new()  # List all files all_files <- openai$list_files()  # List only assistant files assistant_files <- openai$list_files(purpose = \"assistants\") }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-find-file-","dir":"Reference","previous_headings":"","what":"Method find_file()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Find files matching specific criteria. Filters list files based provided criteria (e.g., filename, purpose, status).","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$find_file(..., as_df = TRUE)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"... Named arguments filtering (e.g., filename = \"data.csv\", purpose = \"assistants\"). as_df Logical. Whether return results data frame (default: TRUE). FALSE, returns list.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Data frame (list) matching files.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"examples-4","dir":"Reference","previous_headings":"","what":"Examples","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"\\dontrun{ openai <- OpenAI_Chat$new()  # Find files by name files <- openai$find_file(filename = \"data.csv\")  # Find assistant files files <- openai$find_file(purpose = \"assistants\") }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-get-file-","dir":"Reference","previous_headings":"","what":"Method get_file()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Retrieve information specific file. Returns metadata file, including status, size, purpose. return file contents (use download_file() get_file_content() ).","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$get_file(file_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"file_id Character. ID file retrieve.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-10","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"List. File object containing: id: File identifier object: Object type (\"file\") bytes: File size bytes created_at: Unix timestamp creation filename: Name file purpose: File purpose status: Processing status status_details: Error details status \"error\"","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"examples-5","dir":"Reference","previous_headings":"","what":"Examples","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"\\dontrun{ openai <- OpenAI_Chat$new()  # Get file info file_info <- openai$get_file(file_id = \"file-abc123\") cat(\"File:\", file_info$filename, \"Status:\", file_info$status, \"\\n\") }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-get-file-content-","dir":"Reference","previous_headings":"","what":"Method get_file_content()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Retrieve contents file. Returns raw content specified file character string. Note files purpose \"assistants\" retrieved.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$get_file_content(file_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"file_id Character. ID file retrieve.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-11","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Character. file contents string.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"examples-6","dir":"Reference","previous_headings":"","what":"Examples","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"\\dontrun{ openai <- OpenAI_Chat$new()  # Get file content content <- openai$get_file_content(file_id = \"file-abc123\") cat(content) }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-download-file-","dir":"Reference","previous_headings":"","what":"Method download_file()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Download file disk. Downloads file content saves specified path. Files purpose \"assistants\" downloaded. dest_path directory, file saved original filename. dest_path file path, used complete destination path.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$download_file(file_id, dest_path = \"data\", overwrite = TRUE)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-11","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"file_id Character. ID file download. dest_path Character. Destination path (default: \"data\"). Can either directory path complete file path. Created exist. overwrite Logical. Whether overwrite existing files (default: TRUE).","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-12","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Character. Path downloaded file (invisibly).","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"examples-7","dir":"Reference","previous_headings":"","what":"Examples","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"\\dontrun{ openai <- OpenAI_Chat$new()  # Download to a directory path <- openai$download_file(file_id = \"file-abc123\", dest_path = \"downloads\")  # Download with specific filename path <- openai$download_file(file_id = \"file-abc123\", dest_path = \"downloads/myfile.txt\") }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-delete-file-","dir":"Reference","previous_headings":"","what":"Method delete_file()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Delete file OpenAI. Permanently deletes specified file. action undone.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$delete_file(file_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-12","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"file_id Character. ID file delete.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-13","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"List. Deletion confirmation containing: id: deleted file ID object: Object type (\"file\") deleted: Boolean indicating successful deletion","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"examples-8","dir":"Reference","previous_headings":"","what":"Examples","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"\\dontrun{ openai <- OpenAI_Chat$new()  # Delete a file result <- openai$delete_file(file_id = \"file-abc123\") if (result$deleted) cat(\"File deleted successfully\\n\") }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-delete-files-","dir":"Reference","previous_headings":"","what":"Method delete_files()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Delete multiple files OpenAI. Convenience method delete multiple files . deletion performed sequentially.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$delete_files(file_ids)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-13","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"file_ids Character vector. File IDs delete.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-14","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Data frame. Deletion results file.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"examples-9","dir":"Reference","previous_headings":"","what":"Examples","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"\\dontrun{ openai <- OpenAI_Chat$new()  # Delete multiple files results <- openai$delete_files(c(\"file-abc123\", \"file-def456\")) }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-delete-all-files-","dir":"Reference","previous_headings":"","what":"Method delete_all_files()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Delete files OpenAI. WARNING: permanently deletes files organization. Use extreme caution.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-16","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$delete_all_files()"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-15","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Data frame. Deletion results files.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"examples-10","dir":"Reference","previous_headings":"","what":"Examples","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"\\dontrun{ openai <- OpenAI_Chat$new()  # Delete all files (use with caution!) results <- openai$delete_all_files() }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-create-store-","dir":"Reference","previous_headings":"","what":"Method create_store()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Create vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-17","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$create_store(name, file_ids = NULL)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-14","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"name Character. name vector store. file_ids character vector file IDs.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-16","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"list containing information vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-read-store-","dir":"Reference","previous_headings":"","what":"Method read_store()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Read vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-18","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$read_store(store_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-15","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"store_id Character. ID vector store read.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-17","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"list containing information vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-list-stores-","dir":"Reference","previous_headings":"","what":"Method list_stores()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"List vector stores.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-19","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$list_stores()"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-18","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"data frame vector stores.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-find-store-","dir":"Reference","previous_headings":"","what":"Method find_store()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Find vector store matching criteria.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-20","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$find_store(..., as_df = TRUE)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-16","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"... Named arguments filtering. as_df Logical. Whether return data frame.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-19","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"data frame matching vector stores.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-update-store-","dir":"Reference","previous_headings":"","what":"Method update_store()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Update vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-21","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$update_store(store_id, new_name)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-17","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"store_id Character. ID vector store update. new_name Character. new name vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-20","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"list containing information updated vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-delete-store-","dir":"Reference","previous_headings":"","what":"Method delete_store()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Delete vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-22","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$delete_store(store_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-18","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"store_id Character. ID vector store delete.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-21","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"list containing information deleted vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-delete-stores-","dir":"Reference","previous_headings":"","what":"Method delete_stores()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Delete multiple vector stores.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-23","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$delete_stores(store_ids)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-19","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"store_ids character vector vector store IDs.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-22","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"data frame deleted vector stores.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-delete-all-stores-","dir":"Reference","previous_headings":"","what":"Method delete_all_stores()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Delete vector stores.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-24","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$delete_all_stores()"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-23","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"data frame deleted vector stores.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-delete-store-and-files-","dir":"Reference","previous_headings":"","what":"Method delete_store_and_files()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Delete vector store files.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-25","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$delete_store_and_files(store_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-20","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"store_id Character. ID vector store delete.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-24","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"list containing information deleted vector store files.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-add-file-to-store-","dir":"Reference","previous_headings":"","what":"Method add_file_to_store()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Add file vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-26","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$add_file_to_store(store_id, file_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-21","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"store_id Character. ID vector store. file_id Character. ID file.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-25","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"list containing information file vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-read-file-from-store-","dir":"Reference","previous_headings":"","what":"Method read_file_from_store()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Read file vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-27","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$read_file_from_store(store_id, file_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-22","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"store_id Character. ID vector store. file_id Character. ID file.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-26","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"list containing information file vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-list-files-in-store-","dir":"Reference","previous_headings":"","what":"Method list_files_in_store()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"List files vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-28","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$list_files_in_store(store_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-23","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"store_id Character. ID vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-27","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"data frame files vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-find-file-in-store-","dir":"Reference","previous_headings":"","what":"Method find_file_in_store()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Find file vector store matching criteria.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-29","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$find_file_in_store(store_id, ..., as_df = TRUE)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-24","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"store_id Character. ID vector store. ... Named arguments filtering. as_df Logical. Whether return data frame.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-28","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"data frame matching files.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-delete-file-from-store-","dir":"Reference","previous_headings":"","what":"Method delete_file_from_store()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Delete file vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-30","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$delete_file_from_store(store_id, file_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-25","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"store_id Character. ID vector store. file_id Character. ID file.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-29","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"list containing information deleted file.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-delete-files-from-store-","dir":"Reference","previous_headings":"","what":"Method delete_files_from_store()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Delete multiple files vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-31","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$delete_files_from_store(store_id, file_ids)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-26","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"store_id Character. ID vector store. file_ids character vector file IDs.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-30","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"data frame deleted files.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-delete-all-files-from-store-","dir":"Reference","previous_headings":"","what":"Method delete_all_files_from_store()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"Delete files vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-32","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$delete_all_files_from_store(store_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-27","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"store_id Character. ID vector store.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"returns-31","dir":"Reference","previous_headings":"","what":"Returns","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"data frame deleted files.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"objects class cloneable method.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"usage-33","dir":"Reference","previous_headings":"","what":"Usage","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"OpenAI$clone(deep = FALSE)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"arguments-28","dir":"Reference","previous_headings":"","what":"Arguments","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"deep Whether make deep clone.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Parent class for OpenAI API clients (Responses, Chat Completions, Assistants) — OpenAI","text":"","code":"## ------------------------------------------------ ## Method `OpenAI$embeddings` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ openai <- OpenAI_Chat$new()  # Generate embeddings embeddings <- openai$embeddings(   input = c(\"Hello world\", \"How are you?\"),   model = \"text-embedding-3-small\" )  # With dimension reduction embeddings <- openai$embeddings(   input = \"Sample text\",   model = \"text-embedding-3-large\",   dimensions = 256 ) } # }  ## ------------------------------------------------ ## Method `OpenAI$upload_file` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ openai <- OpenAI_Chat$new()  # Upload a document for assistants file <- openai$upload_file(\"document.pdf\", purpose = \"assistants\") cat(\"File ID:\", file$id, \"\\n\")  # Upload data with custom name file <- openai$upload_file(\"data.csv\", file_name = \"my_data.csv\", purpose = \"user_data\")  # Upload for fine-tuning file <- openai$upload_file(\"training_data.jsonl\", purpose = \"fine-tune\") } # }  ## ------------------------------------------------ ## Method `OpenAI$upload_file_from_df` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ openai <- OpenAI_Chat$new()  # Upload a data frame df <- data.frame(x = 1:10, y = letters[1:10]) file <- openai$upload_file_from_df(df, file_name = \"my_data.txt\") } # }  ## ------------------------------------------------ ## Method `OpenAI$list_files` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ openai <- OpenAI_Chat$new()  # List all files all_files <- openai$list_files()  # List only assistant files assistant_files <- openai$list_files(purpose = \"assistants\") } # }  ## ------------------------------------------------ ## Method `OpenAI$find_file` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ openai <- OpenAI_Chat$new()  # Find files by name files <- openai$find_file(filename = \"data.csv\")  # Find assistant files files <- openai$find_file(purpose = \"assistants\") } # }  ## ------------------------------------------------ ## Method `OpenAI$get_file` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ openai <- OpenAI_Chat$new()  # Get file info file_info <- openai$get_file(file_id = \"file-abc123\") cat(\"File:\", file_info$filename, \"Status:\", file_info$status, \"\\n\") } # }  ## ------------------------------------------------ ## Method `OpenAI$get_file_content` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ openai <- OpenAI_Chat$new()  # Get file content content <- openai$get_file_content(file_id = \"file-abc123\") cat(content) } # }  ## ------------------------------------------------ ## Method `OpenAI$download_file` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ openai <- OpenAI_Chat$new()  # Download to a directory path <- openai$download_file(file_id = \"file-abc123\", dest_path = \"downloads\")  # Download with specific filename path <- openai$download_file(file_id = \"file-abc123\", dest_path = \"downloads/myfile.txt\") } # }  ## ------------------------------------------------ ## Method `OpenAI$delete_file` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ openai <- OpenAI_Chat$new()  # Delete a file result <- openai$delete_file(file_id = \"file-abc123\") if (result$deleted) cat(\"File deleted successfully\\n\") } # }  ## ------------------------------------------------ ## Method `OpenAI$delete_files` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ openai <- OpenAI_Chat$new()  # Delete multiple files results <- openai$delete_files(c(\"file-abc123\", \"file-def456\")) } # }  ## ------------------------------------------------ ## Method `OpenAI$delete_all_files` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ openai <- OpenAI_Chat$new()  # Delete all files (use with caution!) results <- openai$delete_all_files() } # }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":null,"dir":"Reference","previous_headings":"","what":"Client for OpenAI's Assistants API — OpenAI_Assistant","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"R6 class interacting OpenAI's Assistants API. Inherits file management vector store methods OpenAI_Base.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"features","dir":"Reference","previous_headings":"","what":"Features","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Server-side conversation state management via threads Client-side tools Server-side tools Multimodal inputs (files, images, PDFs, R objects) File uploads management Server-side RAG stores & file_search server tool Structured outputs","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"useful-links","dir":"Reference","previous_headings":"","what":"Useful links","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"API reference: https://platform.openai.com/docs/api-reference/assistants API docs: https://platform.openai.com/docs/assistants/deep-dive","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"deprecation-notice","dir":"Reference","previous_headings":"","what":"Deprecation notice","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"DEPRECATED: OpenAI deprecated Assistants API favor Responses API. shut August 26, 2026. Users migrate Responses API instead. information, see: https://platform.openai.com/docs/assistants/migration","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"history-management","dir":"Reference","previous_headings":"","what":"History management","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Assistants API client-side history management. uses server-side thread state conversation history. However,also maintains client-side session_history tracking conversations token counts (unreliable).","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"main-entrypoints","dir":"Reference","previous_headings":"","what":"Main entrypoints","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"chat(): Multi-turn multimodal conversations tool use structured outputs. embeddings(): Vector embeddings text inputs.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"argent::Provider -> argent::OpenAI -> OpenAIAssistant","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"assistant List. Current assistant object thread List. Current thread object","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"argent::Provider$download_generated_files() argent::Provider$dump_history() argent::Provider$get_auto_save_history() argent::Provider$get_content_text() argent::Provider$get_generated_code() argent::Provider$get_generated_files() argent::Provider$get_history() argent::Provider$get_history_file_path() argent::Provider$get_last_response() argent::Provider$get_rate_limit() argent::Provider$get_reasoning_text() argent::Provider$get_session_cumulative_token_count() argent::Provider$get_session_history() argent::Provider$get_session_last_token_count() argent::Provider$get_supplementary() argent::Provider$load_history() argent::Provider$print() argent::Provider$reset_history() argent::Provider$set_auto_save_history() argent::Provider$set_history() argent::Provider$set_rate_limit() argent::OpenAI$add_file_to_store() argent::OpenAI$create_store() argent::OpenAI$delete_all_files() argent::OpenAI$delete_all_files_from_store() argent::OpenAI$delete_all_stores() argent::OpenAI$delete_file() argent::OpenAI$delete_file_from_store() argent::OpenAI$delete_files() argent::OpenAI$delete_files_from_store() argent::OpenAI$delete_store() argent::OpenAI$delete_store_and_files() argent::OpenAI$delete_stores() argent::OpenAI$download_file() argent::OpenAI$embeddings() argent::OpenAI$find_assistants() argent::OpenAI$find_file() argent::OpenAI$find_file_in_store() argent::OpenAI$find_models() argent::OpenAI$find_store() argent::OpenAI$get_file() argent::OpenAI$get_file_content() argent::OpenAI$get_model_info() argent::OpenAI$list_assistants() argent::OpenAI$list_files() argent::OpenAI$list_files_in_store() argent::OpenAI$list_stores() argent::OpenAI$read_file_from_store() argent::OpenAI$read_store() argent::OpenAI$update_store() argent::OpenAI$upload_file() argent::OpenAI$upload_file_from_df()","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"OpenAI_Assistant$new() OpenAI_Assistant$list_models() OpenAI_Assistant$get_chat_history() OpenAI_Assistant$dump_chat_history() OpenAI_Assistant$load_chat_history() OpenAI_Assistant$get_total_tokens() OpenAI_Assistant$get_assistant() OpenAI_Assistant$get_assistant_id() OpenAI_Assistant$set_assistant_id() OpenAI_Assistant$create_assistant() OpenAI_Assistant$load_assistant() OpenAI_Assistant$register_tools() OpenAI_Assistant$read_assistant() OpenAI_Assistant$delete_assistant() OpenAI_Assistant$delete_assistant_and_contents() OpenAI_Assistant$chat() OpenAI_Assistant$clone()","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Initialize new OpenAI Assistant client","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"OpenAI_Assistant$new(   base_url = \"https://api.openai.com\",   api_key = Sys.getenv(\"OPENAI_API_KEY\"),   provider_name = \"OpenAI Assistant\",   rate_limit = 60/60,   server_tools = c(\"file_search\", \"code_interpreter\"),   default_model = \"gpt-4o\",   org = Sys.getenv(\"OPENAI_ORG\"),   auto_save_history = TRUE )"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"base_url Character. Base URL API (default: \"https://api.openai.com\") api_key Character. API key (default: OPENAI_API_KEY env var) provider_name Character. Provider name (default: \"OpenAI Assistant\") rate_limit Numeric. Rate limit requests per second (default: 60/60) server_tools Character vector. Server-side tools available (default: c(\"file_search\", \"code_interpreter\")) default_model Character. Default model use chat requests (default: \"gpt-4o\") org Character. Organization ID (default: OPENAI_ORG env var) auto_save_history Logical. Enable/disable automatic history sync (default: TRUE)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"method-list-models-","dir":"Reference","previous_headings":"","what":"Method list_models()","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Override list_models warn gpt-5 incompatibility","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"OpenAI_Assistant$list_models()"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Data frame. Available models","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"method-get-chat-history-","dir":"Reference","previous_headings":"","what":"Method get_chat_history()","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Get chat history current thread (server-side state). Returns content array thread messages.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"OpenAI_Assistant$get_chat_history()"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"List. content array thread messages, NULL thread exists.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"method-dump-chat-history-","dir":"Reference","previous_headings":"","what":"Method dump_chat_history()","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Dump chat history (applicable - server-side state)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"OpenAI_Assistant$dump_chat_history(file_path = NULL)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"file_path Character. File path","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"method-load-chat-history-","dir":"Reference","previous_headings":"","what":"Method load_chat_history()","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Load chat history (applicable - server-side state)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"OpenAI_Assistant$load_chat_history(file_path)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"file_path Character. File path","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"method-get-total-tokens-","dir":"Reference","previous_headings":"","what":"Method get_total_tokens()","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Get total tokens (applicable - per-run tracking)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"OpenAI_Assistant$get_total_tokens()"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"method-get-assistant-","dir":"Reference","previous_headings":"","what":"Method get_assistant()","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Get current assistant.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"OpenAI_Assistant$get_assistant()"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"current assistant.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"method-get-assistant-id-","dir":"Reference","previous_headings":"","what":"Method get_assistant_id()","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Get ID current assistant.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"OpenAI_Assistant$get_assistant_id()"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"ID current assistant.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"method-set-assistant-id-","dir":"Reference","previous_headings":"","what":"Method set_assistant_id()","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Set current assistant ID.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"OpenAI_Assistant$set_assistant_id(assistant_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"assistant_id ID assistant set.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"method-create-assistant-","dir":"Reference","previous_headings":"","what":"Method create_assistant()","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Create new assistant","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"OpenAI_Assistant$create_assistant(   name,   model = self$default_model,   system = .default_system_prompt,   temperature = 1,   top_p = 1,   tools = NULL,   response_format = \"auto\" )"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"name Character. Name assistant model Character. Model use (default: \"gpt-4o\") system Character. System instructions (default: .default_system_prompt) temperature Numeric. Sampling temperature 0-2 (default: 1) top_p Numeric. Nucleus sampling parameter 0-1 (default: 1). Alternative temperature. recommend altering temperature . tools List. Tool definitions (optional). Supports three formats: String form: \"file_search\", \"code_interpreter\" List form resources: list(type = \"file_search\", store_ids = list(\"vs_123\")) list(type = \"code_interpreter\", file_ids = list(\"file-123\")) Client-side functions: created as_tool(fn) tool() helpers response_format Character. Response format: \"auto\" (default), list JSON schema","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"assistant object (invisibly)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"\\dontrun{ assistant <- OpenAI_Assistant$new() assistant$create_assistant(   name = \"My Assistant\",   model = \"gpt-4o\",   system = \"You are a helpful assistant\" ) }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"method-load-assistant-","dir":"Reference","previous_headings":"","what":"Method load_assistant()","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Load existing assistant ID","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"OpenAI_Assistant$load_assistant(id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"id Character. Assistant ID load","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"assistant object (invisibly)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"examples-1","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"\\dontrun{ assistant <- OpenAI_Assistant$new() assistant$load_assistant(id = \"asst_...\") }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"method-register-tools-","dir":"Reference","previous_headings":"","what":"Method register_tools()","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Register client/MCP tools use current assistant","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"OpenAI_Assistant$register_tools(tools)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"tools List. Tool definitions (client functions MCP tools)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"method used register tools loading existing assistant. load assistant created client/MCP tools, must call method re-register tools execution.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Self (invisibly) method chaining","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"examples-2","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"\\dontrun{ assistant <- OpenAI_Assistant$new() assistant$load_assistant(id = \"asst_...\") assistant$register_tools(list(my_tool_function)) }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"method-read-assistant-","dir":"Reference","previous_headings":"","what":"Method read_assistant()","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Read assistant.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"OpenAI_Assistant$read_assistant(assistant_id = self$assistant$id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"assistant_id ID assistant read.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"assistant object.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"method-delete-assistant-","dir":"Reference","previous_headings":"","what":"Method delete_assistant()","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Delete assistant.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"OpenAI_Assistant$delete_assistant(assistant_id = self$assistant$id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"assistant_id ID assistant delete.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"deletion status.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"method-delete-assistant-and-contents-","dir":"Reference","previous_headings":"","what":"Method delete_assistant_and_contents()","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Delete assistant contents.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"OpenAI_Assistant$delete_assistant_and_contents(   assistant_id = self$assistant$id )"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"assistant_id ID assistant delete.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"list deletion status assistant contents.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"method-chat-","dir":"Reference","previous_headings":"","what":"Method chat()","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Send chat message assistant. Note: Unlike OpenAI$chat(), assistant configuration (model, temperature, tools, system) set initialization changed per-chat. sending message, can use base class methods like get_content_text(), get_supplementary(), download_generated_files() extract information responses.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"OpenAI_Assistant$chat(   ...,   in_new_thread = FALSE,   output_schema = NULL,   remove_citations = TRUE )"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"... One inputs prompt. Can text strings, file paths, URLs, R objects, content wrapped as_*_content() functions. R objects (plain strings) include names structure context sent model. in_new_thread Logical. Start new thread (default: FALSE). Assistant-specific: Controls thread management. Set TRUE start fresh conversation. output_schema List. JSON schema structured output (optional). assistant uses server tools, forces second call schema ensure structured output. remove_citations Logical. Remove file_search citation markers (default: TRUE). TRUE, removes citation markers like 【35†source】 [3:0†source] responses.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"returns-10","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"Character. OpenAI Assistant API's response object.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"objects class cloneable method.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"usage-16","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"OpenAI_Assistant$clone(deep = FALSE)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"arguments-11","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"deep Whether make deep clone.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Assistant.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for OpenAI's Assistants API — OpenAI_Assistant","text":"","code":"if (FALSE) { # \\dontrun{ # Initialize client assistant <- OpenAI_Assistant$new()  # Create a new assistant assistant$create_assistant(   name = \"My Assistant\",   model = \"gpt-4o\",   instructions = \"You are a helpful assistant\" )  # Or load an existing assistant assistant$load_assistant(id = \"asst_...\")  # Send a message response <- assistant$chat(\"Hello!\")  # Create assistant with tools assistant <- OpenAI_Assistant$new() assistant$create_assistant(   name = \"Research Assistant\",   model = \"gpt-4o\",   instructions = \"Research assistant with web access\",   tools = list(     list(type = \"file_search\", store_ids = list(store_id))   ) )  # Code execution with embedded file resources assistant <- OpenAI_Assistant$new() assistant$create_assistant(   name = \"Data Analyst\",   model = \"gpt-4o\",   tools = list(list(type = \"code_interpreter\", file_ids = list(file_id))) )  # Using PDFs and files in messages # PDFs are automatically uploaded and attached to messages assistant <- OpenAI_Assistant$new() assistant$create_assistant(   name = \"Document Analyst\",   model = \"gpt-4o\",   tools = list(list(type = \"file_search\")) ) response <- assistant$chat(\"Summarize this document\", \"path/to/document.pdf\") } # }  ## ------------------------------------------------ ## Method `OpenAI_Assistant$create_assistant` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ assistant <- OpenAI_Assistant$new() assistant$create_assistant(   name = \"My Assistant\",   model = \"gpt-4o\",   system = \"You are a helpful assistant\" ) } # }  ## ------------------------------------------------ ## Method `OpenAI_Assistant$load_assistant` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ assistant <- OpenAI_Assistant$new() assistant$load_assistant(id = \"asst_...\") } # }  ## ------------------------------------------------ ## Method `OpenAI_Assistant$register_tools` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ assistant <- OpenAI_Assistant$new() assistant$load_assistant(id = \"asst_...\") assistant$register_tools(list(my_tool_function)) } # }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":null,"dir":"Reference","previous_headings":"","what":"Client for OpenAI's Chat Completions API — OpenAI_Chat","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"R6 class interacting OpenAI's Chat Completions API (v1/chat/completions). Inherits file management vector store methods OpenAI_Base.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"features","dir":"Reference","previous_headings":"","what":"Features","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"Client-side conversation state management Client-side tools Server-side tools Multimodal inputs (files, images, PDFs, R objects) File uploads management Reasoning Structured outputs","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"useful-links","dir":"Reference","previous_headings":"","what":"Useful links","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"API reference: https://platform.openai.com/docs/api-reference/chat API docs: https://platform.openai.com/docs/guides/completions/introduction","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"main-entrypoints","dir":"Reference","previous_headings":"","what":"Main entrypoints","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"chat(): Multi-turn multimodal conversations tool use structured outputs. embeddings(): Vector embeddings text inputs.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"server-side-tools","dir":"Reference","previous_headings":"","what":"Server-side tools","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"\"web_search\" web search grounding via OpenAI's web plugin","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"structured-outputs","dir":"Reference","previous_headings":"","what":"Structured outputs","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"Fully native structured outputs via JSON schema. additional API calls required.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"argent::Provider -> argent::OpenAI -> OpenAI_Chat","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"argent::Provider$download_generated_files() argent::Provider$dump_history() argent::Provider$get_auto_save_history() argent::Provider$get_chat_history() argent::Provider$get_content_text() argent::Provider$get_generated_code() argent::Provider$get_generated_files() argent::Provider$get_history() argent::Provider$get_history_file_path() argent::Provider$get_last_response() argent::Provider$get_rate_limit() argent::Provider$get_reasoning_text() argent::Provider$get_session_cumulative_token_count() argent::Provider$get_session_history() argent::Provider$get_session_last_token_count() argent::Provider$get_supplementary() argent::Provider$load_history() argent::Provider$print() argent::Provider$reset_history() argent::Provider$set_auto_save_history() argent::Provider$set_history() argent::Provider$set_rate_limit() argent::OpenAI$add_file_to_store() argent::OpenAI$create_store() argent::OpenAI$delete_all_files() argent::OpenAI$delete_all_files_from_store() argent::OpenAI$delete_all_stores() argent::OpenAI$delete_file() argent::OpenAI$delete_file_from_store() argent::OpenAI$delete_files() argent::OpenAI$delete_files_from_store() argent::OpenAI$delete_store() argent::OpenAI$delete_store_and_files() argent::OpenAI$delete_stores() argent::OpenAI$download_file() argent::OpenAI$embeddings() argent::OpenAI$find_assistants() argent::OpenAI$find_file() argent::OpenAI$find_file_in_store() argent::OpenAI$find_models() argent::OpenAI$find_store() argent::OpenAI$get_file() argent::OpenAI$get_file_content() argent::OpenAI$get_model_info() argent::OpenAI$list_assistants() argent::OpenAI$list_files() argent::OpenAI$list_files_in_store() argent::OpenAI$list_models() argent::OpenAI$list_stores() argent::OpenAI$read_file_from_store() argent::OpenAI$read_store() argent::OpenAI$update_store() argent::OpenAI$upload_file() argent::OpenAI$upload_file_from_df()","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"OpenAI_Chat$new() OpenAI_Chat$chat() OpenAI_Chat$clone()","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"Initialize new OpenAI Chat client","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"","code":"OpenAI_Chat$new(   base_url = \"https://api.openai.com\",   api_key = Sys.getenv(\"OPENAI_API_KEY\"),   provider_name = \"OpenAI Chat\",   rate_limit = 60/60,   server_tools = c(\"web_search\"),   default_model = \"gpt-5-mini\",   org = Sys.getenv(\"OPENAI_ORG\"),   auto_save_history = TRUE )"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"base_url Character. Base URL API (default: \"https://api.openai.com\") api_key Character. API key (default: OPENAI_API_KEY env var) provider_name Character. Provider name (default: \"OpenAI Chat\") rate_limit Numeric. Rate limit requests per second (default: 60/60) server_tools Character vector. Server-side tools available (default: c(\"web_search\")) default_model Character. Default model use chat requests (default: \"gpt-5-mini\") org Character. Organization ID (default: OPENAI_ORG env var) auto_save_history Logical. Enable/disable automatic history sync (default: TRUE)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"method-chat-","dir":"Reference","previous_headings":"","what":"Method chat()","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"Send chat completion request OpenAI","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"","code":"OpenAI_Chat$chat(   ...,   system = .default_system_prompt,   model = self$default_model,   temperature = 1,   max_completion_tokens = 4096,   top_p = 1,   frequency_penalty = 0,   presence_penalty = 0,   logprobs = FALSE,   top_logprobs = NULL,   n = 1,   logit_bias = NULL,   tools = NULL,   tool_choice = \"auto\",   output_schema = NULL,   reasoning_effort = NULL,   verbosity = \"medium\",   store = FALSE )"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"... One inputs prompt. Can text strings, file paths, URLs, R objects, content wrapped as_*_content() functions. R objects (plain strings) include names structure context sent model. system Character. System instructions (optional) model Character. Model use (default: \"gpt-5-mini\") temperature Numeric. Sampling temperature (default: 1) max_completion_tokens Integer. Maximum tokens generate (default: 4096) top_p Numeric. Nucleus sampling parameter 0-1 (default: 1). Alternative temperature. recommend altering temperature . frequency_penalty Numeric. Penalty token frequency -2.0 2.0 (default: 0). Positive values decrease likelihood repeating line verbatim. presence_penalty Numeric. Penalty token presence -2.0 2.0 (default: 0). Positive values increase likelihood talking new topics. logprobs Logical. Whether return log probabilities (default: FALSE) top_logprobs Integer. Number likely tokens (0-20) return position (default: NULL). Requires logprobs = TRUE. n Integer. Number chat completion choices generate (default: 1) logit_bias Named list. Modify likelihood specified tokens token ID (default: NULL). Values -100 100. tools List. Client-side function definitions tool calling (optional). web search search-enabled models (gpt-4o-mini-search-preview, gpt-4o-search-preview, gpt-5-search-api), pass : list(\"web_search\") options: list(list(type = \"web_search\", user_location = list(...), search_context_size = \"medium\")). Supported web_search options: user_location: list type = \"approximate\" approximate containing country (ISO 3166-1), city, region, /timezone (IANA) search_context_size: \"low\", \"medium\" (default), \"high\" Note: Search models support standard sampling parameters (temperature, top_p, frequency_penalty, presence_penalty, n). automatically omitted using search models. tool_choice Character List. Tool choice mode (default: \"auto\") output_schema List. JSON schema structured output (optional) reasoning_effort Character. Reasoning effort level reasoning models: \"low\", \"medium\", \"high\" (optional). applicable reasoning models (o1, o3, o4, gpt-5). Default: NULL (uses model default) verbosity Character. Verbosity level output: \"low\", \"medium\", \"high\" (default: \"medium\") store Logical. Whether store output chat completion request OpenAI servers (default: FALSE)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"Character. OpenAI Chat API's response object.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"objects class cloneable method.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"","code":"OpenAI_Chat$clone(deep = FALSE)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"deep Whether make deep clone.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Chat.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for OpenAI's Chat Completions API — OpenAI_Chat","text":"","code":"if (FALSE) { # \\dontrun{ # Initialize with API key from environment openai <- OpenAI_Chat$new()  # Or provide API key explicitly openai <- OpenAI_Chat$new(api_key = \"your-api-key\")  # Simple chat completion response <- openai$chat(   prompt = \"What is R programming?\",   model = \"gpt-5-chat-latest\" )  # With tools/function calling response <- openai$chat(   prompt = \"What's the weather in Paris?\",   tools = list(get_weather_tool) )  # Upload file and use in chat file <- openai$upload_file(\"document.pdf\", purpose = \"user_data\") response <- openai$chat(   prompt = openai$multimodal_input(     \"Summarize this document:\",     as_file_ref(file$id)   ) ) } # }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":null,"dir":"Reference","previous_headings":"","what":"Client for OpenAI's Responses API — OpenAI_Responses","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"R6 class interacting OpenAI's Responses API (v1/responses). Inherits file management vector store methods OpenAI_Base.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"features","dir":"Reference","previous_headings":"","what":"Features","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"Client-side conversation state management Server-side conversation state management via previous_response_id & response forking Client-side tools Server-side tools Multimodal inputs (files, images, PDFs, R objects) File uploads management Server-side RAG stores & file_search server tool Reasoning Structured outputs","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"useful-links","dir":"Reference","previous_headings":"","what":"Useful links","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"API reference: https://platform.openai.com/docs/api-reference/responses/create API docs: https://platform.openai.com/docs/quickstart","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"main-entrypoints","dir":"Reference","previous_headings":"","what":"Main entrypoints","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"chat(): Multi-turn multimodal conversations tool use structured outputs. embeddings(): Vector embeddings text inputs.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"server-side-tools","dir":"Reference","previous_headings":"","what":"Server-side tools","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"\"web_search\" web search grounding via OpenAI's web plugin \"file_search\" file search vector stores \"code_interpreter\" Python code execution sandboxed containers","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"structured-outputs","dir":"Reference","previous_headings":"","what":"Structured outputs","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"Fully native structured outputs via JSON schema. additional API calls required.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"super-classes","dir":"Reference","previous_headings":"","what":"Super classes","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"argent::Provider -> argent::OpenAI -> OpenAI_Responses","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"argent::Provider$download_generated_files() argent::Provider$dump_history() argent::Provider$get_auto_save_history() argent::Provider$get_chat_history() argent::Provider$get_content_text() argent::Provider$get_generated_code() argent::Provider$get_generated_files() argent::Provider$get_history() argent::Provider$get_history_file_path() argent::Provider$get_last_response() argent::Provider$get_rate_limit() argent::Provider$get_reasoning_text() argent::Provider$get_session_cumulative_token_count() argent::Provider$get_session_history() argent::Provider$get_session_last_token_count() argent::Provider$get_supplementary() argent::Provider$load_history() argent::Provider$print() argent::Provider$reset_history() argent::Provider$set_auto_save_history() argent::Provider$set_history() argent::Provider$set_rate_limit() argent::OpenAI$add_file_to_store() argent::OpenAI$create_store() argent::OpenAI$delete_all_files() argent::OpenAI$delete_all_files_from_store() argent::OpenAI$delete_all_stores() argent::OpenAI$delete_file() argent::OpenAI$delete_file_from_store() argent::OpenAI$delete_files() argent::OpenAI$delete_files_from_store() argent::OpenAI$delete_store() argent::OpenAI$delete_store_and_files() argent::OpenAI$delete_stores() argent::OpenAI$download_file() argent::OpenAI$embeddings() argent::OpenAI$find_assistants() argent::OpenAI$find_file() argent::OpenAI$find_file_in_store() argent::OpenAI$find_models() argent::OpenAI$find_store() argent::OpenAI$get_file() argent::OpenAI$get_file_content() argent::OpenAI$get_model_info() argent::OpenAI$list_assistants() argent::OpenAI$list_files() argent::OpenAI$list_files_in_store() argent::OpenAI$list_models() argent::OpenAI$list_stores() argent::OpenAI$read_file_from_store() argent::OpenAI$read_store() argent::OpenAI$update_store() argent::OpenAI$upload_file() argent::OpenAI$upload_file_from_df()","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"OpenAI_Responses$new() OpenAI_Responses$get_last_response_id() OpenAI_Responses$create_container() OpenAI_Responses$list_containers() OpenAI_Responses$get_container() OpenAI_Responses$delete_container() OpenAI_Responses$list_container_files() OpenAI_Responses$get_container_file_metadata() OpenAI_Responses$get_container_file_content() OpenAI_Responses$download_container_file() OpenAI_Responses$chat() OpenAI_Responses$clone()","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"Initialize new OpenAI Responses client","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"OpenAI_Responses$new(   base_url = \"https://api.openai.com\",   api_key = Sys.getenv(\"OPENAI_API_KEY\"),   provider_name = \"OpenAI Responses\",   rate_limit = 60/60,   server_tools = c(\"web_search\", \"file_search\", \"code_interpreter\"),   default_model = \"gpt-5-mini\",   org = Sys.getenv(\"OPENAI_ORG\"),   auto_save_history = TRUE )"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"base_url Character. Base URL API (default: \"https://api.openai.com\") api_key Character. API key (default: OPENAI_API_KEY env var) provider_name Character. Provider name (default: \"OpenAI Responses\") rate_limit Numeric. Rate limit requests per second (default: 60/60) server_tools Character vector. Server-side tools available (default: c(\"web_search\", \"file_search\", \"code_interpreter\")) default_model Character. Default model use chat requests (default: \"gpt-5-mini\") org Character. Organization ID (default: OPENAI_ORG env var) auto_save_history Logical. Enable/disable automatic history sync (default: TRUE)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"method-get-last-response-id-","dir":"Reference","previous_headings":"","what":"Method get_last_response_id()","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"Get ID last response conversation chaining convenience wrapper around get_last_response()$id, useful chaining responses via previous_response_id parameter.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"OpenAI_Responses$get_last_response_id()"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"Character. ID last response, NULL previous response exists","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"\\dontrun{ responses <- OpenAI_Responses$new() res1 <- responses$chat(prompt = \"Tell me a joke\", model = \"gpt-5-mini\") id <- responses$get_last_response_id() res2 <- responses$chat(   prompt = \"Explain why it's funny\",   previous_response_id = id ) }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"method-create-container-","dir":"Reference","previous_headings":"","what":"Method create_container()","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"Create new container code execution Containers sandboxed virtual machines code_interpreter can execute Python code. container costs $0.03 active 1 hour 20 minute idle timeout.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"OpenAI_Responses$create_container(file_ids = NULL)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"file_ids Character vector. Optional file IDs initialize container .","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"List. Container object id, created_at, status","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"examples-1","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"\\dontrun{ responses <- OpenAI_Responses$new() container <- responses$create_container() container <- responses$create_container(file_ids = c(\"file-123\", \"file-456\")) }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"method-list-containers-","dir":"Reference","previous_headings":"","what":"Method list_containers()","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"List containers","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"OpenAI_Responses$list_containers()"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"Data frame. Available containers id, created_at, status","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"examples-2","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"\\dontrun{ responses <- OpenAI_Responses$new() containers <- responses$list_containers() }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"method-get-container-","dir":"Reference","previous_headings":"","what":"Method get_container()","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"Get information specific container","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"OpenAI_Responses$get_container(container_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"container_id Character. Container ID retrieve.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"List. Container metadata","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"examples-3","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"\\dontrun{ responses <- OpenAI_Responses$new() container <- responses$get_container(\"container-123\") }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"method-delete-container-","dir":"Reference","previous_headings":"","what":"Method delete_container()","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"Delete container","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"OpenAI_Responses$delete_container(container_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"container_id Character. Container ID delete.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"List. Deletion confirmation","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"examples-4","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"\\dontrun{ responses <- OpenAI_Responses$new() responses$delete_container(\"container-123\") }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"method-list-container-files-","dir":"Reference","previous_headings":"","what":"Method list_container_files()","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"List files container","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"OpenAI_Responses$list_container_files(container_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"container_id Character. Container ID list files .","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"Data frame. Files container paths","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"examples-5","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"\\dontrun{ responses <- OpenAI_Responses$new() files <- responses$list_container_files(\"container-123\") }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"method-get-container-file-metadata-","dir":"Reference","previous_headings":"","what":"Method get_container_file_metadata()","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"Get metadata specific file container","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"OpenAI_Responses$get_container_file_metadata(container_id, file_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"container_id Character. Container ID. file_id Character. Container file ID (e.g., \"cfile_abc123xyz\").","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"List. File metadata.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"method-get-container-file-content-","dir":"Reference","previous_headings":"","what":"Method get_container_file_content()","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"Get file content container","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"OpenAI_Responses$get_container_file_content(container_id, file_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"container_id Character. Container ID. file_id Character. Container file ID (e.g., \"cfile_abc123xyz\").","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"Raw. File content raw bytes","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"examples-6","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"\\dontrun{ responses <- OpenAI_Responses$new() annotations <- responses$get_last_annotations() file_id <- annotations[[1]]$file_id content <- responses$get_container_file_content(\"container-123\", file_id) }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"method-download-container-file-","dir":"Reference","previous_headings":"","what":"Method download_container_file()","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"Download file container local filesystem Downloads file content saves specified path. dest_path directory, file saved original filename. dest_path file path, used complete destination path.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"OpenAI_Responses$download_container_file(   container_id,   file_id,   dest_path = \"data\",   overwrite = TRUE )"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"container_id Character. Container ID. file_id Character. Container file ID (e.g., \"cfile_abc123xyz\"). dest_path Character. Destination path (default: \"data\"). Can either directory path complete file path. Created exist. overwrite Logical. Whether overwrite existing files (default: TRUE). filename Character. Optional filename use dest_path directory. NULL, fetches filename container file list.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"Character. Path downloaded file (invisibly)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"examples-7","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"\\dontrun{ responses <- OpenAI_Responses$new() annotations <- responses$get_last_annotations() file_id <- annotations[[1]]$file_id  # Download to a directory path <- responses$download_container_file(\"container-123\", file_id, \"downloads\")  # Download with specific filename path <- responses$download_container_file(\"container-123\", file_id, \"downloads/output.png\")  # Pass filename explicitly (from annotations) path <- responses$download_container_file(   \"container-123\",   file_id,   \"downloads\",   filename = annotations[[1]]$filename ) }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"method-chat-","dir":"Reference","previous_headings":"","what":"Method chat()","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"Create response Responses API See: https://platform.openai.com/docs/api-reference/responses/create","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"OpenAI_Responses$chat(   ...,   model = self$default_model,   system = .default_system_prompt,   temperature = 1,   max_tokens = 4096,   top_p = 1,   top_logprobs = NULL,   input_truncation = \"disabled\",   previous_response_id = NULL,   store = TRUE,   include = NULL,   tools = NULL,   tool_choice = \"auto\",   max_tool_calls = NULL,   parallel_tool_calls = TRUE,   output_schema = NULL,   output_verbosity = \"medium\",   reasoning_effort = NULL,   reasoning_summary = NULL )"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"... One inputs prompt. Can text strings, file paths, URLs, R objects, content wrapped as_*_content() functions. R objects (plain strings) include names structure context sent model. model Character. Model use (default: \"gpt-5-mini\") system Character. System prompt/instructions (default: .default_system_prompt) temperature Numeric. Sampling temperature 0-2 (default: 1) max_tokens Integer. Maximum output tokens generate (default: 4096) top_p Numeric. Nucleus sampling parameter 0-1 (default: 1). Alternative temperature. recommend altering temperature . top_logprobs Integer. Number likely tokens (0-20) return position associated log probabilities (default: NULL) input_truncation Character. Truncation strategy: \"auto\" \"disabled\" (default: \"disabled\") previous_response_id Character. ID previous response chain server-side state management. provided, new prompt sent (full chat history). used conversation parameter. (default: NULL) store Logical. Whether store response server-side later retrieval (default: TRUE) include Character vector. Additional output data include model response. Supported values: \"web_search_call.action.sources\" - Include sources web search tool calls \"code_interpreter_call.outputs\" - Include Python code execution outputs \"computer_call_output.output.image_url\" - Include image URLs computer call output \"file_search_call.results\" - Include file search tool call results \"message.input_image.image_url\" - Include input message image URLs \"message.output_text.logprobs\" - Include logprobs assistant messages \"reasoning.encrypted_content\" - Include encrypted reasoning tokens multi-turn conversations tools List. Tool definitions (server-side client-side functions). Server-side tools: list(type = \"web_search\") web search list(type = \"file_search\", store_ids = list(\"vs_123\")) file search vector stores Client-side functions: use created as_tool(fn) tool() helpers. tool_choice Character List. Tool choice mode (default: \"auto\") max_tool_calls Integer. Maximum number tool calls (default: NULL) parallel_tool_calls Logical. Allow parallel tool calls (default: TRUE) output_schema List. JSON schema structured output via build_output_schema_openai() (optional) output_verbosity Character. Output verbosity: \"low\", \"medium\", \"high\" (default: \"medium\") reasoning_effort Character. Reasoning effort reasoning models: \"minimal\", \"low\", \"medium\", \"high\" (optional, o1/o3/gpt-5 models) reasoning_summary Character. Reasoning summary mode: \"auto\", \"concise\", \"detailed\" (optional, requires reasoning_effort set)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"Character. OpenAI Responses API's response object.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"objects class cloneable method.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"OpenAI_Responses$clone(deep = FALSE)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"deep Whether make deep clone.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenAI_Responses.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for OpenAI's Responses API — OpenAI_Responses","text":"","code":"if (FALSE) { # \\dontrun{ # Initialize responses <- OpenAI_Responses$new()  # Simple response res <- responses$chat(   prompt = \"What is R programming?\",   model = \"gpt-5-mini\" )  # Continue conversation res2 <- responses$chat(   prompt = \"Tell me more\",   previous_response_id = res$id )  # With web search res <- responses$chat(   prompt = \"What are the latest AI developments?\",   tools = list(list(type = \"web_search\")) )  # With file search and vector stores file_id <- responses$upload_file(\"document.pdf\", purpose = \"assistants\") store <- responses$create_store(\"docs\", file_ids = list(file_id)) res <- responses$chat(   prompt = \"Summarize the document\",   tools = list(list(type = \"file_search\", store_ids = list(store$id))) ) } # }  ## ------------------------------------------------ ## Method `OpenAI_Responses$get_last_response_id` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ responses <- OpenAI_Responses$new() res1 <- responses$chat(prompt = \"Tell me a joke\", model = \"gpt-5-mini\") id <- responses$get_last_response_id() res2 <- responses$chat(   prompt = \"Explain why it's funny\",   previous_response_id = id ) } # }  ## ------------------------------------------------ ## Method `OpenAI_Responses$create_container` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ responses <- OpenAI_Responses$new() container <- responses$create_container() container <- responses$create_container(file_ids = c(\"file-123\", \"file-456\")) } # }  ## ------------------------------------------------ ## Method `OpenAI_Responses$list_containers` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ responses <- OpenAI_Responses$new() containers <- responses$list_containers() } # }  ## ------------------------------------------------ ## Method `OpenAI_Responses$get_container` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ responses <- OpenAI_Responses$new() container <- responses$get_container(\"container-123\") } # }  ## ------------------------------------------------ ## Method `OpenAI_Responses$delete_container` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ responses <- OpenAI_Responses$new() responses$delete_container(\"container-123\") } # }  ## ------------------------------------------------ ## Method `OpenAI_Responses$list_container_files` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ responses <- OpenAI_Responses$new() files <- responses$list_container_files(\"container-123\") } # }  ## ------------------------------------------------ ## Method `OpenAI_Responses$get_container_file_content` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ responses <- OpenAI_Responses$new() annotations <- responses$get_last_annotations() file_id <- annotations[[1]]$file_id content <- responses$get_container_file_content(\"container-123\", file_id) } # }  ## ------------------------------------------------ ## Method `OpenAI_Responses$download_container_file` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ responses <- OpenAI_Responses$new() annotations <- responses$get_last_annotations() file_id <- annotations[[1]]$file_id  # Download to a directory path <- responses$download_container_file(\"container-123\", file_id, \"downloads\")  # Download with specific filename path <- responses$download_container_file(\"container-123\", file_id, \"downloads/output.png\")  # Pass filename explicitly (from annotations) path <- responses$download_container_file(   \"container-123\",   file_id,   \"downloads\",   filename = annotations[[1]]$filename ) } # }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":null,"dir":"Reference","previous_headings":"","what":"Client for the OpenRouter API — OpenRouter","title":"Client for the OpenRouter API — OpenRouter","text":"R6 class interacting OpenRouter's API.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"features","dir":"Reference","previous_headings":"","what":"Features","title":"Client for the OpenRouter API — OpenRouter","text":"Client-side conversation state management Client-side tools Server-side tools Multimodal inputs (files, images, PDFs, R objects) Provider routing preferences Reasoning Structured outputs","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"useful-links","dir":"Reference","previous_headings":"","what":"Useful links","title":"Client for the OpenRouter API — OpenRouter","text":"API reference: https://openrouter.ai/docs/api-reference/overview API docs: https://openrouter.ai/docs/quickstart","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"main-entrypoints","dir":"Reference","previous_headings":"","what":"Main entrypoints","title":"Client for the OpenRouter API — OpenRouter","text":"chat(): Multi-turn multimodal conversations tool use structured outputs. embeddings(): Vector embeddings text inputs.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"server-side-tools","dir":"Reference","previous_headings":"","what":"Server-side tools","title":"Client for the OpenRouter API — OpenRouter","text":"\"web_search\" web search grounding via OpenRouter's web plugin","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"structured-outputs","dir":"Reference","previous_headings":"","what":"Structured outputs","title":"Client for the OpenRouter API — OpenRouter","text":"Function-call trick models: uses tool calling simulate structured outputs, always requiring additional API query full chat history (incurs extra cost).","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"super-class","dir":"Reference","previous_headings":"","what":"Super class","title":"Client for the OpenRouter API — OpenRouter","text":"argent::Provider -> OpenRouter","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Client for the OpenRouter API — OpenRouter","text":"allowed_providers Character vector. Allowed provider slugs (default: NULL) blocked_providers Character vector. Blocked provider slugs (default: NULL)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"methods","dir":"Reference","previous_headings":"","what":"Methods","title":"Client for the OpenRouter API — OpenRouter","text":"argent::Provider$download_generated_files() argent::Provider$dump_history() argent::Provider$get_auto_save_history() argent::Provider$get_chat_history() argent::Provider$get_content_text() argent::Provider$get_generated_code() argent::Provider$get_generated_files() argent::Provider$get_history() argent::Provider$get_history_file_path() argent::Provider$get_last_response() argent::Provider$get_rate_limit() argent::Provider$get_reasoning_text() argent::Provider$get_session_cumulative_token_count() argent::Provider$get_session_history() argent::Provider$get_session_last_token_count() argent::Provider$get_supplementary() argent::Provider$load_history() argent::Provider$print() argent::Provider$reset_history() argent::Provider$set_auto_save_history() argent::Provider$set_history() argent::Provider$set_rate_limit()","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Client for the OpenRouter API — OpenRouter","text":"OpenRouter$new() OpenRouter$get_allowed_providers() OpenRouter$set_allowed_providers() OpenRouter$get_blocked_providers() OpenRouter$set_blocked_providers() OpenRouter$list_providers() OpenRouter$list_models() OpenRouter$get_model_info() OpenRouter$embeddings() OpenRouter$chat() OpenRouter$clone()","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Client for the OpenRouter API — OpenRouter","text":"Initialize new OpenRouter client","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for the OpenRouter API — OpenRouter","text":"","code":"OpenRouter$new(   base_url = \"https://openrouter.ai/api\",   api_key = Sys.getenv(\"OPENROUTER_API_KEY\"),   provider_name = \"OpenRouter\",   rate_limit = 20/60,   server_tools = c(\"web_search\"),   default_model = \"openrouter/auto\",   allowed_providers = NULL,   blocked_providers = NULL,   auto_save_history = TRUE )"},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for the OpenRouter API — OpenRouter","text":"base_url Character. Base URL API (default: \"https://openrouter.ai/api\") api_key Character. API key (default: OPENROUTER_API_KEY env var) provider_name Character. Provider name (default: \"OpenRouter\") rate_limit Numeric. Rate limit requests per second (default: 20/60) server_tools Character vector. Server-side tools available (default: c(\"web_search\")) default_model Character. Default model use chat requests (default: \"openrouter/auto\") allowed_providers Character vector. Allowed provider slugs (default: NULL) blocked_providers Character vector. Blocked provider slugs (default: NULL) auto_save_history Logical. Enable/disable automatic history sync (default: TRUE)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"method-get-allowed-providers-","dir":"Reference","previous_headings":"","what":"Method get_allowed_providers()","title":"Client for the OpenRouter API — OpenRouter","text":"Get list allowed providers","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for the OpenRouter API — OpenRouter","text":"","code":"OpenRouter$get_allowed_providers()"},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for the OpenRouter API — OpenRouter","text":"Character vector. Allowed provider slugs, NULL none set","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"method-set-allowed-providers-","dir":"Reference","previous_headings":"","what":"Method set_allowed_providers()","title":"Client for the OpenRouter API — OpenRouter","text":"Set list allowed providers requests","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for the OpenRouter API — OpenRouter","text":"","code":"OpenRouter$set_allowed_providers(providers)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for the OpenRouter API — OpenRouter","text":"providers Character vector. Provider slugs allow (e.g., c(\"anthropic\", \"openai\"))","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"method-get-blocked-providers-","dir":"Reference","previous_headings":"","what":"Method get_blocked_providers()","title":"Client for the OpenRouter API — OpenRouter","text":"Get list blocked providers","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for the OpenRouter API — OpenRouter","text":"","code":"OpenRouter$get_blocked_providers()"},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for the OpenRouter API — OpenRouter","text":"Character vector. Blocked provider slugs, NULL none set","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"method-set-blocked-providers-","dir":"Reference","previous_headings":"","what":"Method set_blocked_providers()","title":"Client for the OpenRouter API — OpenRouter","text":"Set list blocked providers requests","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for the OpenRouter API — OpenRouter","text":"","code":"OpenRouter$set_blocked_providers(providers)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for the OpenRouter API — OpenRouter","text":"providers Character vector. Provider slugs block (e.g., c(\"deepinfra\", \"together\"))","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"method-list-providers-","dir":"Reference","previous_headings":"","what":"Method list_providers()","title":"Client for the OpenRouter API — OpenRouter","text":"List available providers OpenRouter","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for the OpenRouter API — OpenRouter","text":"","code":"OpenRouter$list_providers()"},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for the OpenRouter API — OpenRouter","text":"Data frame. Available providers specifications","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"method-list-models-","dir":"Reference","previous_headings":"","what":"Method list_models()","title":"Client for the OpenRouter API — OpenRouter","text":"List available models OpenRouter","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for the OpenRouter API — OpenRouter","text":"","code":"OpenRouter$list_models(supported_parameters = NULL)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for the OpenRouter API — OpenRouter","text":"supported_parameters Character vector. Supported parameters filter models . Options include: \"tools\", \"temperature\", \"top_p\", \"top_k\", \"min_p\", \"top_a\", \"frequency_penalty\", \"presence_penalty\", \"repetition_penalty\", \"max_tokens\", \"logit_bias\", \"logprobs\", \"top_logprobs\", \"seed\", \"response_format\", \"structured_outputs\", \"stop\", \"parallel_tool_calls\", \"include_reasoning\", \"reasoning\", \"web_search_options\", \"verbosity\". Example: c(\"tools\", \"response_format\")","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for the OpenRouter API — OpenRouter","text":"Data frame. Available models specifications","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"method-get-model-info-","dir":"Reference","previous_headings":"","what":"Method get_model_info()","title":"Client for the OpenRouter API — OpenRouter","text":"Get information specific model","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for the OpenRouter API — OpenRouter","text":"","code":"OpenRouter$get_model_info(model_id)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for the OpenRouter API — OpenRouter","text":"model_id Character. Model ID (e.g., \"anthropic/claude-3.5-sonnet\")","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for the OpenRouter API — OpenRouter","text":"List. Model information","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"method-embeddings-","dir":"Reference","previous_headings":"","what":"Method embeddings()","title":"Client for the OpenRouter API — OpenRouter","text":"Generate embeddings text input","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for the OpenRouter API — OpenRouter","text":"","code":"OpenRouter$embeddings(   input,   model,   encoding_format = \"float\",   dimensions = NULL,   provider = NULL,   return_full_response = FALSE )"},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for the OpenRouter API — OpenRouter","text":"input Character vector. Text(s) embed model Character. Model use (e.g., \"text-embedding-3-small\", \"text-embedding-3-large\") encoding_format Character. Format embeddings: \"float\" \"base64\" (default: \"float\") dimensions Integer. Number dimensions output (embedding-3 models) provider Character. Specific provider use (optional, enables provider routing) return_full_response Logical. Return full API response (default: FALSE)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for the OpenRouter API — OpenRouter","text":"Numeric matrix (List return_full_response = TRUE). Embeddings one row per input text","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for the OpenRouter API — OpenRouter","text":"","code":"\\dontrun{ openrouter <- OpenRouter$new()  # Generate embeddings embeddings <- openrouter$embeddings(   input = c(\"Hello world\", \"How are you?\"),   model = \"openai/text-embedding-3-small\" )  # With dimension reduction embeddings <- openrouter$embeddings(   input = \"Sample text\",   model = \"openai/text-embedding-3-large\",   dimensions = 256 )  # With provider routing embeddings <- openrouter$embeddings(   input = \"Sample text\",   model = \"text-embedding-3-small\",   provider = \"openai\" ) }"},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"method-chat-","dir":"Reference","previous_headings":"","what":"Method chat()","title":"Client for the OpenRouter API — OpenRouter","text":"Send chat completion request OpenRouter","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for the OpenRouter API — OpenRouter","text":"","code":"OpenRouter$chat(   ...,   system = .default_system_prompt,   model = self$default_model,   temperature = 1,   max_tokens = 4096,   top_p = NULL,   top_k = NULL,   frequency_penalty = NULL,   presence_penalty = NULL,   repetition_penalty = NULL,   min_p = NULL,   top_a = NULL,   seed = NULL,   stop_sequences = NULL,   logit_bias = NULL,   logprobs = NULL,   top_logprobs = NULL,   tools = NULL,   tool_choice = \"auto\",   parallel_tool_calls = NULL,   cache_prompt = FALSE,   cache_system = FALSE,   thinking_budget = 0,   verbosity = NULL,   provider_options = NULL,   output_schema = NULL )"},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for the OpenRouter API — OpenRouter","text":"... One inputs prompt. Can text strings, file paths, URLs, R objects, content wrapped as_*_content() functions. R objects (plain strings) include names structure context sent model. system Character. System instructions (optional) model Character. Model use (e.g., \"anthropic/claude-3.5-sonnet\") temperature Numeric. Sampling temperature (default: 1) max_tokens Integer. Maximum tokens generate (default: 4096) top_p Numeric. Nucleus sampling - restricts token selection whose cumulative probability equals top_p. Range: 0.0-1.0 (optional) top_k Integer. Top-K sampling - limits token choices top N selections. Set 1 deterministic output (optional) frequency_penalty Numeric. Reduces reuse tokens appearing frequently input. Range: -2.0 2.0 (optional) presence_penalty Numeric. Penalizes token reuse regardless frequency. Range: -2.0 2.0 (optional) repetition_penalty Numeric. Decreases token repetition input. Range: 0.0-2.0 (optional) min_p Numeric. Minimum relative probability threshold token consideration. Range: 0.0-1.0 (optional) top_a Numeric. Filters tokens based sufficiently high probabilities relative likely token. Range: 0.0-1.0 (optional) seed Integer. Enables deterministic sampling repeated identical parameters (optional) stop_sequences Character vector. Sequences halt generation encountered (optional) logit_bias Named list. Applies bias values (-100 100) token logits sampling (optional) logprobs Logical. Returns log probabilities output tokens enabled (optional) top_logprobs Integer. Number probable tokens log probabilities return. Range: 0-20. Requires logprobs = TRUE (optional) tools List. Function definitions tool calling, server tools (optional). Server-side tools: \"web_search\" web search grounding via OpenRouter's web plugin list(type = \"web_search\", engine = \"exa\", max_results = 3) advanced configuration engine: \"native\" (provider's server-side search), \"exa\" (Exa API), NULL (auto-select) max_results: Maximum results include (default: 5, Exa) search_prompt: Custom prompt results (optional) Pricing: Exa charges $4/1000 results (~$0.02 per request default 5 results). Native search pricing varies provider. tool_choice Character List. Tool choice mode (default: \"auto\"). Note: models support \"auto\". get error \"support auto tool\", either: Use tool_choice = \"none\" disable tools Use tool_choice = list(type = \"function\", function = list(name = \"tool_name\")) force specific tool (model always use ) Choose different model supports \"auto\" parallel_tool_calls Logical. Allow simultaneous tool execution (default: TRUE) (optional) cache_prompt Logical. Cache user prompt (default: FALSE). Anthropic/Google models, adds cache_control breakpoint. Minimum 1024 tokens Google 2.5 Flash, 2048 Google 2.5 Pro, 4096 Anthropic. providers cache automatically. cache_system Logical. Cache system instructions (default: FALSE). Anthropic/Google models . thinking_budget Integer. Thinking budget tokens: 0 (disabled) positive integer (default: 0). models support reasoning tokens (e.g., o1, DeepSeek-R1). verbosity Character. Adjusts response length detail level: \"low\", \"medium\", \"high\" (default: \"medium\") (optional) provider_options List. Provider routing options (optional). See https://openrouter.ai/docs/features/provider-routing details. Possible elements: order: Character vector. List provider slugs try order allow_fallbacks: Logical. Allow backup providers primary unavailable (default: TRUE) require_parameters: Logical. use providers supporting parameters (default: FALSE) data_collection: Character. \"allow\" \"deny\" - control provider data storage zdr: Logical. Restrict routing ZDR (Zero Data Retention) endpoints enforce_distillable_text: Logical. Restrict routing models allowing text distillation : Character vector. List provider slugs allow (overrides class-level allowed_providers) ignore: Character vector. List provider slugs skip (overrides class-level blocked_providers) quantizations: Character vector. List quantization levels filter (e.g., c(\"int4\", \"int8\")) sort: Character. Sort providers \"price\" \"throughput\" max_price: List. Maximum pricing (e.g., list(prompt = 0.001, completion = 0.002)) output_schema List. JSON schema structured output (optional)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Client for the OpenRouter API — OpenRouter","text":"Character. OpenRouter API's response object.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Client for the OpenRouter API — OpenRouter","text":"objects class cloneable method.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Client for the OpenRouter API — OpenRouter","text":"","code":"OpenRouter$clone(deep = FALSE)"},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Client for the OpenRouter API — OpenRouter","text":"deep Whether make deep clone.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/OpenRouter.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Client for the OpenRouter API — OpenRouter","text":"","code":"if (FALSE) { # \\dontrun{ # Initialize with API key from environment openrouter <- OpenRouter$new()  # Or provide API key explicitly openrouter <- OpenRouter$new(api_key = \"your-api-key\")  # Simple chat completion response <- openrouter$chat(   prompt = \"What is R programming?\",   model = \"anthropic/claude-3.5-sonnet\" )  # With web search (simple method) response <- openrouter$chat(   prompt = \"What are the latest developments in quantum computing?\",   model = \"anthropic/claude-3.5-sonnet\",   tools = list(\"web_search\") )  # With web search using specific engine and custom settings response <- openrouter$chat(   prompt = \"Recent news about R programming language\",   model = \"openai/gpt-4o\",   tools = list(     list(type = \"web_search\", engine = \"exa\", max_results = 3)   ) )  # With tools/function calling response <- openrouter$chat(   prompt = \"What's the weather in Paris?\",   model = \"anthropic/claude-3.5-sonnet\",   tools = list(get_weather_tool) ) } # }  ## ------------------------------------------------ ## Method `OpenRouter$embeddings` ## ------------------------------------------------  if (FALSE) { # \\dontrun{ openrouter <- OpenRouter$new()  # Generate embeddings embeddings <- openrouter$embeddings(   input = c(\"Hello world\", \"How are you?\"),   model = \"openai/text-embedding-3-small\" )  # With dimension reduction embeddings <- openrouter$embeddings(   input = \"Sample text\",   model = \"openai/text-embedding-3-large\",   dimensions = 256 )  # With provider routing embeddings <- openrouter$embeddings(   input = \"Sample text\",   model = \"text-embedding-3-small\",   provider = \"openai\" ) } # }"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":null,"dir":"Reference","previous_headings":"","what":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Base R6 class LLM provider clients. Provides shared infrastructure : Chat history management (get, set, reset, append) Automatic history persistence JSON (enabled default, per-instance control) Rate limiting configuration Token usage tracking class inherited provider classes (Google, Anthropic, OpenAI_Base) ensure consistent interfaces eliminate code duplication.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"history-auto-save","dir":"Reference","previous_headings":"","what":"History Auto-Save","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"History auto-save controlled per-instance enabled default. control : initialization: Pass auto_save_history = TRUE/FALSE $new() (default: TRUE) creation: Use $set_auto_save_history(TRUE/FALSE) $get_auto_save_history() Manual save/load available via dump_history() load_history().","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"public-fields","dir":"Reference","previous_headings":"","what":"Public fields","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"base_url Character. Base URL API endpoint provider_name Character. Provider name rate_limit Numeric. Rate limit requests per second history_file_path Character. Full file path persistent history storage chat_history List. Conversation history (provider-specific format) session_history List. Ground truth: alternating query_data API responses server_tools Character vector. Server-side tools use API requests default_model Character. Default model use chat requests","code":""},{"path":[]},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"public-methods","dir":"Reference","previous_headings":"","what":"Public methods","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Provider$new() Provider$get_rate_limit() Provider$set_rate_limit() Provider$get_history() Provider$set_history() Provider$get_chat_history() Provider$get_session_history() Provider$dump_history() Provider$reset_history() Provider$get_history_file_path() Provider$load_history() Provider$get_auto_save_history() Provider$set_auto_save_history() Provider$get_session_last_token_count() Provider$get_session_cumulative_token_count() Provider$get_last_response() Provider$get_content_text() Provider$get_reasoning_text() Provider$get_generated_code() Provider$get_generated_files() Provider$download_generated_files() Provider$get_supplementary() Provider$print() Provider$clone()","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-new-","dir":"Reference","previous_headings":"","what":"Method new()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Initialize new Provider instance","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$new(   base_url = NULL,   api_key = NULL,   provider_name = \"Provider\",   rate_limit = NULL,   server_tools = character(0),   default_model = NULL,   auto_save_history = TRUE )"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"base_url Character. Base URL API endpoint api_key Character. API key provider provider_name Character. Provider name rate_limit Numeric. Rate limit requests per second server_tools Character vector. Server-side tools available default_model Character. Default model use chat requests auto_save_history Logical. Enable/disable automatic history sync (default: TRUE)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-get-rate-limit-","dir":"Reference","previous_headings":"","what":"Method get_rate_limit()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Get rate limit","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-1","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$get_rate_limit()"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Numeric. Rate limit requests per second","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-set-rate-limit-","dir":"Reference","previous_headings":"","what":"Method set_rate_limit()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Set rate limit","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-2","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$set_rate_limit(rate_limit)"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"arguments-1","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"rate_limit Numeric. Rate limit requests per second","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-get-history-","dir":"Reference","previous_headings":"","what":"Method get_history()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Get chat session history","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-3","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$get_history()"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns-1","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"List. History containing chat_history session_history","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-set-history-","dir":"Reference","previous_headings":"","what":"Method set_history()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Set chat session history","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-4","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$set_history(history)"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"arguments-2","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"history List. Chat session history set (list(chat_history = ..., session_history = ...))","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns-2","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Self (invisibly) method chaining","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-get-chat-history-","dir":"Reference","previous_headings":"","what":"Method get_chat_history()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Get chat history. chat history history messages exchanged user model.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-5","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$get_chat_history()"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns-3","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"List. Chat history","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-get-session-history-","dir":"Reference","previous_headings":"","what":"Method get_session_history()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Get session history","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-6","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$get_session_history()"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns-4","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"List. Session history (alternating query_data responses)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-dump-history-","dir":"Reference","previous_headings":"","what":"Method dump_history()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Dump chat session history JSON file","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-7","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$dump_history(dest_path = NULL)"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"arguments-3","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"dest_path Character. Optional custom file path. NULL, uses history_file_path.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns-5","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Character. Path saved file (invisibly)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-reset-history-","dir":"Reference","previous_headings":"","what":"Method reset_history()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Reset chat session history","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-8","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$reset_history()"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Archives current history resetting, generates new history_file_path","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-get-history-file-path-","dir":"Reference","previous_headings":"","what":"Method get_history_file_path()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Get history file path","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-9","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$get_history_file_path()"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns-6","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Character. History file path","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-load-history-","dir":"Reference","previous_headings":"","what":"Method load_history()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Load chat session history JSON file","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-10","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$load_history(file_path)"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"arguments-4","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"file_path Character. Path history file (absolute relative project root) just filename","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns-7","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Self (invisibly) method chaining","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-get-auto-save-history-","dir":"Reference","previous_headings":"","what":"Method get_auto_save_history()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Get auto-save history setting","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-11","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$get_auto_save_history()"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns-8","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Logical. Auto-save history setting","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-set-auto-save-history-","dir":"Reference","previous_headings":"","what":"Method set_auto_save_history()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Set auto-save history setting","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-12","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$set_auto_save_history(enabled)"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"arguments-5","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"enabled Logical. Enable/disable automatic history sync","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-get-session-last-token-count-","dir":"Reference","previous_headings":"","what":"Method get_session_last_token_count()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Get total tokens used session_history","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-13","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$get_session_last_token_count()"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns-9","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Integer. Total tokens used last API call","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-get-session-cumulative-token-count-","dir":"Reference","previous_headings":"","what":"Method get_session_cumulative_token_count()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Get cumulative tokens used session_history","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-14","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$get_session_cumulative_token_count(up_to_index = NULL)"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"arguments-6","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"up_to_index Integer. Index calculate cumulative tokens (default: NULL)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns-10","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Integer. Cumulative tokens used computed session_history, specified index","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-get-last-response-","dir":"Reference","previous_headings":"","what":"Method get_last_response()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Get last API response","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-15","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$get_last_response()"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns-11","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"List. Last API response object, NULL response stored","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-get-content-text-","dir":"Reference","previous_headings":"","what":"Method get_content_text()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Get text content API response","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-16","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$get_content_text(api_res = self$get_last_response())"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"arguments-7","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"api_res List. API response object (defaults last response)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns-12","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Character. Text content response","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-get-reasoning-text-","dir":"Reference","previous_headings":"","what":"Method get_reasoning_text()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Get text content reasoning API response","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-17","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$get_reasoning_text(api_res = self$get_last_response())"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"arguments-8","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"api_res List. API response object (defaults last response)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns-13","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Character List. Text content reasoning response","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-get-generated-code-","dir":"Reference","previous_headings":"","what":"Method get_generated_code()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Get generated code API response (e.g. code execution tools)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-18","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$get_generated_code(   api_res = self$get_last_response(),   langs = NULL,   as_chunks = FALSE )"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"arguments-9","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"api_res List. API response object (defaults last response) langs Character vector. Languages filter code parts (default: NULL) as_chunks Logical. Whether return code list chunks (default: FALSE)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns-14","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Character List. Code content response list chunks single string","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-get-generated-files-","dir":"Reference","previous_headings":"","what":"Method get_generated_files()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Get generated files API response (e.g. code execution tools)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-19","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$get_generated_files(api_res = self$get_last_response())"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"arguments-10","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"api_res List. API response object (defaults last response)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns-15","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"List. Files response (mime_type data), NULL none found","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-download-generated-files-","dir":"Reference","previous_headings":"","what":"Method download_generated_files()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Download files generated code execution API response (e.g. code execution tools)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-20","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$download_generated_files(   api_res = self$get_last_response(),   dest_path = \"data\",   overwrite = TRUE )"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"arguments-11","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"api_res List. API response object (defaults last response) dest_path Character. Destination path downloaded files overwrite Logical. Whether overwrite existing files","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns-16","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Character vector. Paths saved files (invisibly)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-get-supplementary-","dir":"Reference","previous_headings":"","what":"Method get_supplementary()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Get supplementary data API response (annotations, citations, grounding metadata, etc.)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-21","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$get_supplementary(api_res = self$get_last_response())"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"arguments-12","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"api_res List. API response object (defaults last response)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns-17","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"List. Supplementary data response (provider-specific structure)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-print-","dir":"Reference","previous_headings":"","what":"Method print()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Print chat history formatted view. Inspired ellmer","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-22","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$print(   show_system = TRUE,   show_reasoning = TRUE,   show_code = FALSE,   show_tools = FALSE,   show_supplementary = FALSE,   show_output_schema = TRUE,   max_content_length = 999 )"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"arguments-13","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"show_system Logical. Include system messages (default: TRUE) show_reasoning Logical. Include reasoning/thinking blocks (default: TRUE) show_code Logical. Include code blocks (default: FALSE) show_tools Logical. Include tool calls results (default: FALSE) show_supplementary Logical. Include supplementary data like annotations, citations (default: FALSE) show_output_schema Logical. Include output schema query display (default: TRUE) max_content_length Integer. Maximum content length truncation (default: 999)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"returns-18","dir":"Reference","previous_headings":"","what":"Returns","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"Self (invisibly) method chaining","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"method-clone-","dir":"Reference","previous_headings":"","what":"Method clone()","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"objects class cloneable method.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"usage-23","dir":"Reference","previous_headings":"","what":"Usage","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"","code":"Provider$clone(deep = FALSE)"},{"path":"https://ma-riviere.github.io/argent/reference/Provider.html","id":"arguments-14","dir":"Reference","previous_headings":"","what":"Arguments","title":"Provider Base Class: shared interface & common functionalities for all LLM providers — Provider","text":"deep Whether make deep clone.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/argent-package.html","id":null,"dir":"Reference","previous_headings":"","what":"argent: Argent: LLM Agents in R — argent-package","title":"argent: Argent: LLM Agents in R — argent-package","text":"Provides unified interface interacting Large Language Models (LLMs) multiple providers, specialized creating AI agents tool calling, multimodal inputs, structured outputs. argent package provides unified interface creating AI agents can interact multiple Large Language Model (LLM) providers R6 classes. supports Google, Anthropic, OpenAI, OpenRouter, local LLM servers (e.g., llama.cpp, Ollama). argent specialized building AI agents conversation history management, local function MCP tools, server-side tools, multimodal inputs, universal structured outputs.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/argent-package.html","id":"main-classes","dir":"Reference","previous_headings":"","what":"Main Classes","title":"argent: Argent: LLM Agents in R — argent-package","text":"package provides following R6 classes: Google Client Google's API support chat completions, function calling, thinking mode, code execution, web search Anthropic Client Anthropic's API prompt caching, tool calling, extended thinking capabilities OpenAI_Chat Client OpenAI's Chat Completions API OpenAI_Responses Client OpenAI's Responses API comprehensive file management, vector stores, code execution, web search OpenAI_Assistant Client OpenAI's Assistants API (Deprecated) OpenRouter Client OpenRouter API providing access multiple LLM providers unified interface LocalLLM Client local LLM servers implementing OpenAI-compatible APIs","code":""},{"path":"https://ma-riviere.github.io/argent/reference/argent-package.html","id":"features","dir":"Reference","previous_headings":"","what":"Features","title":"argent: Argent: LLM Agents in R — argent-package","text":"Unified interface across multiple LLM providers Function MCP tool calling parallel execution support Universal structured JSON outputs (works model supporting tool calling) Multimodal inputs (text, images, PDFs, data files, URLs, R objects) Server-side tools (code execution, web search, file search, RAG) Conversation history management automatic persistence Prompt caching (Anthropic, OpenAI) File upload vector store management (Google, Anthropic, OpenAI) Reasoning thinking modes (Google, Anthropic, OpenAI)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/argent-package.html","id":"parallel-tool-calls","dir":"Reference","previous_headings":"","what":"Parallel Tool Calls","title":"argent: Argent: LLM Agents in R — argent-package","text":"Tool calls can executed parallel using mirai::daemons(). can significantly speed responses multiple tools called simultaneously. enable parallel execution: Set mirai daemons making requests: mirai::daemons(6) argent automatically parallelize multiple tool calls response Without daemons, tool calls execute sequentially (default fallback behavior) ensure parallel execution always used, add mirai::require_daemons() making requests. disable parallel processing, call mirai::daemons(0). Performance considerations: Parallelization overhead can outweigh benefits fast tool calls beneficial tools take 100+ microseconds per call rule thumb, use one fewer daemon available CPU cores","code":""},{"path":"https://ma-riviere.github.io/argent/reference/argent-package.html","id":"getting-started","dir":"Reference","previous_headings":"","what":"Getting Started","title":"argent: Argent: LLM Agents in R — argent-package","text":"Set API keys environment variables: GEMINI_API_KEY Google ANTHROPIC_API_KEY Anthropic OPENAI_API_KEY OpenAI OPENAI_ORG OpenAI organization (optional) OPENROUTER_API_KEY OpenRouter","code":""},{"path":[]},{"path":"https://ma-riviere.github.io/argent/reference/argent-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"argent: Argent: LLM Agents in R — argent-package","text":"Maintainer: Marc-Aurèle Rivière marc.aurele.riviere@gmail.com (ORCID)","code":""},{"path":"https://ma-riviere.github.io/argent/reference/argent-package.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"argent: Argent: LLM Agents in R — argent-package","text":"","code":"if (FALSE) { # \\dontrun{ # Google google <- Google$new() response <- google$chat(\"What is R programming?\")  # Anthropic anthropic <- Anthropic$new() response <- anthropic$chat(   prompt = \"Explain quantum computing\",   model = \"claude-sonnet-4-5-20250929\" )  # OpenAI Responses API openai <- OpenAI_Responses$new() response <- openai$chat(   prompt = \"Write a haiku about R\",   model = \"gpt-5-chat-latest\" )  # OpenRouter openrouter <- OpenRouter$new() response <- openrouter$chat(   prompt = \"Explain machine learning\",   model = \"anthropic/claude-sonnet-4\" )  # Local LLM llm <- LocalLLM$new(base_url = \"http://localhost:5000\") response <- llm$chat(prompt = \"Hello!\")  # Parallel tool calling library(mirai) daemons(4)  # Set up 4 parallel workers  get_weather <- function(location) {   #' @description Get weather information for a location   #' @param location:string* The location to get weather for   paste(\"Weather in\", location, \"is sunny\") }  google$chat(   \"What's the weather in Paris, London, and Tokyo?\",   tools = list(as_tool(get_weather)),   model = \"gemini-2.5-flash\" ) # Tool calls will execute in parallel across the 4 workers  daemons(0)  # Clean up when done } # }"},{"path":"https://ma-riviere.github.io/argent/reference/content_converters.html","id":null,"dir":"Reference","previous_headings":"","what":"Convert file paths, URLs or R objects to the specified content format before passing it to the LLM API — content_converters","title":"Convert file paths, URLs or R objects to the specified content format before passing it to the LLM API — content_converters","text":"functions convert various input types formats suitable LLM API requests. function handles specific content types automatic detection conversion applicable.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/content_converters.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Convert file paths, URLs or R objects to the specified content format before passing it to the LLM API — content_converters","text":"","code":"as_text_content(   ...,   .vec_len = 999,   .nchar_max = 999,   .converter = default_text_converter,   .provider_options = list() )  as_image_content(   ...,   .resize = \"none\",   .converter = default_image_converter,   .provider_options = list() )  as_file_content(..., .provider_options = list())  as_pdf_content(   ...,   .converter = default_pdf_converter,   .provider_options = list() )  as_json_content(   ...,   .max_rows = 10,   .converter = default_json_converter,   .provider_options = list() )"},{"path":"https://ma-riviere.github.io/argent/reference/content_converters.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Convert file paths, URLs or R objects to the specified content format before passing it to the LLM API — content_converters","text":"... One inputs convert. Can file paths, URLs, R objects. .vec_len Integer. as_text_content() . Maximum vector length display converting R objects. Default 999. .nchar_max Integer. as_text_content() . Maximum characters per element converting R objects. Minimum value 17. Default 999. .converter Function. Custom converter function. Signature varies function: as_text_content(): function(input, vec_len, nchar_max) as_image_content(): function(input) as_pdf_content(): function(input) as_json_content(): function(input, max_rows) Default converters provided function. Can overridden passing custom function. .provider_options List. Provider-specific options attach attributes. Default empty list. OpenAI providers images, use list(detail = \"low\") control image processing detail level. Options: \"low\" (85 tokens, 512px, faster), \"high\" (better understanding), \"auto\" (model decides). OpenAI Assistants PDFs, can include 'tools' specify tools use attachment (default: file_search). .resize Character. as_image_content() . Image resizing strategy: \"none\": resizing (default) \"low\": Resize fit within 512x512 (faster, cheaper processing) \"high\": Resize fit within 2000x768 768x2000 based orientation Custom geometry string (e.g., \"800x600\", \"50%\", \"200x200>\", \"300x200>!\") following magick::image_resize() syntax. Append > resize larger, ! ignore aspect ratio. .max_rows Integer. as_json_content() . Maximum number rows include converting data frames. Default 10.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/content_converters.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Convert file paths, URLs or R objects to the specified content format before passing it to the LLM API — content_converters","text":"Character vector processed content. attribute argent_input = TRUE element argent_input_type set appropriate type (\"text\", \"image\", \"file_ref\", \"pdf\").","code":""},{"path":[]},{"path":"https://ma-riviere.github.io/argent/reference/content_converters.html","id":"supported-file-formats","dir":"Reference","previous_headings":"","what":"Supported File Formats","title":"Convert file paths, URLs or R objects to the specified content format before passing it to the LLM API — content_converters","text":"as_text_content() supports: JSON, YAML, CSV, TSV, RDS, XML, HTML, plain text files PDF files (requires pdftools package) Image files (converted base64 data URIs) as_image_content() supports: Common image formats (JPEG, PNG, GIF, etc.) PDF files (converted images via magick) as_pdf_content() supports: PDF file paths URLs Handling varies provider (e.g., OpenAI Assistants API, Google Gemini File API) as_json_content() supports: R objects (converted via jsonlite::toJSON) JSON file paths JSON URLs","code":""},{"path":"https://ma-riviere.github.io/argent/reference/content_converters.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Convert file paths, URLs or R objects to the specified content format before passing it to the LLM API — content_converters","text":"","code":"if (FALSE) { # \\dontrun{ # Text content as_text_content(\"Hello, world!\") as_text_content(mtcars, .vec_len = 5) as_text_content(\"path/to/file.txt\")  # Image content as_image_content(\"image.jpg\") as_image_content(\"large_image.jpg\", .resize = \"low\") as_image_content(\"image.jpg\", .provider_options = list(detail = \"high\"))  # File references as_file_content(\"file-abc123\")  # PDF content as_pdf_content(\"document.pdf\")  # JSON content as_json_content(mtcars, .max_rows = 5) as_json_content(list(a = 1, b = 2)) } # }"},{"path":"https://ma-riviere.github.io/argent/reference/execute_mcp_tool.html","id":null,"dir":"Reference","previous_headings":"","what":"Execute an MCP tool — execute_mcp_tool","title":"Execute an MCP tool — execute_mcp_tool","text":"Execute MCP tool","code":""},{"path":"https://ma-riviere.github.io/argent/reference/execute_mcp_tool.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Execute an MCP tool — execute_mcp_tool","text":"","code":"execute_mcp_tool(tool_def, arguments)"},{"path":"https://ma-riviere.github.io/argent/reference/execute_mcp_tool.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Execute an MCP tool — execute_mcp_tool","text":"tool_def List. tool definition .mcp metadata arguments List. arguments pass tool","code":""},{"path":"https://ma-riviere.github.io/argent/reference/execute_mcp_tool.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Execute an MCP tool — execute_mcp_tool","text":"result tool execution","code":""},{"path":"https://ma-riviere.github.io/argent/reference/execute_mcp_tool.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Execute an MCP tool — execute_mcp_tool","text":"","code":"if (FALSE) { # \\dontrun{ # Assuming you have a tool_def from mcp_tools() execute_mcp_tool(   tool_def = github_tools[[1]],   arguments = list(owner = \"ma-riviere\", repo = \"argent\", path = \"R/aaa-utils.R\", ref = \"main\") ) } # }"},{"path":"https://ma-riviere.github.io/argent/reference/flat_list.html","id":null,"dir":"Reference","previous_headings":"","what":"Flatten a list of elements to a single-depth list — flat_list","title":"Flatten a list of elements to a single-depth list — flat_list","text":"Flatten list elements single-depth list","code":""},{"path":"https://ma-riviere.github.io/argent/reference/flat_list.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Flatten a list of elements to a single-depth list — flat_list","text":"","code":"flat_list(...)"},{"path":"https://ma-riviere.github.io/argent/reference/flat_list.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Flatten a list of elements to a single-depth list — flat_list","text":"... One elements lists flatten","code":""},{"path":"https://ma-riviere.github.io/argent/reference/flat_list.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Flatten a list of elements to a single-depth list — flat_list","text":"List elements depth 1","code":""},{"path":"https://ma-riviere.github.io/argent/reference/mcp_integration.html","id":null,"dir":"Reference","previous_headings":"","what":"MCP (Model Context Protocol) Integration — mcp_integration","title":"MCP (Model Context Protocol) Integration — mcp_integration","text":"Functions integrating MCP servers argent. MCP servers provide tools, resources, prompts LLMs can use conversations.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/mcp_integration.html","id":"mcp-server-connection","dir":"Reference","previous_headings":"","what":"MCP Server Connection","title":"MCP (Model Context Protocol) Integration — mcp_integration","text":"mcp_server() creates connection MCP server using programmatic configuration. stdio servers, server spawned subprocess communicates via JSON-RPC stdin/stdout. HTTP servers, communication happens via HTTP requests specified endpoint.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/mcp_integration.html","id":"tool-discovery","dir":"Reference","previous_headings":"","what":"Tool Discovery","title":"MCP (Model Context Protocol) Integration — mcp_integration","text":"mcp_tools() retrieves tool definitions MCP server converts argent's tool format. Tools can passed chat() alongside client-defined provider server tools.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/mcp_integration.html","id":"resources-and-prompts","dir":"Reference","previous_headings":"","what":"Resources and Prompts","title":"MCP (Model Context Protocol) Integration — mcp_integration","text":"mcp_resources() retrieves file-like structured data LLMs can access. mcp_prompts() retrieves predefined templates interactions.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/mcp_integration.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"MCP (Model Context Protocol) Integration — mcp_integration","text":"","code":"mcp_server(   name,   type = \"stdio\",   command = NULL,   args = NULL,   env = NULL,   url = NULL,   headers = NULL )  mcp_tools(server, tools = NULL)  mcp_resources(server, resources = NULL)  mcp_prompts(server, prompts = NULL)"},{"path":"https://ma-riviere.github.io/argent/reference/mcp_integration.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"MCP (Model Context Protocol) Integration — mcp_integration","text":"name Character. Name identifier MCP server type Character. Server type: \"stdio\" command-line \"http\" HTTP endpoint (default: \"stdio\") command Character. Command execute (stdio servers, e.g., \"docker\", \"npx\") args Character vector. Arguments pass command (stdio servers) env Named list. Environment variables server process (stdio servers) url Character. HTTP endpoint URL (http servers, e.g., \"https://api.githubcopilot.com/mcp\") headers Named list. HTTP headers (http servers, e.g., list(Authorization = \"Bearer token\")) server MCP server object created mcp_server() tools Character vector. Specific tool names retrieve. NULL, retrieves tools resources Character vector. Specific resource URIs retrieve. NULL, retrieves prompts Character vector. Specific prompt names retrieve. NULL, retrieves ","code":""},{"path":"https://ma-riviere.github.io/argent/reference/mcp_integration.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"MCP (Model Context Protocol) Integration — mcp_integration","text":"mcp_server(): MCP server connection object (list class \"mcp_server\") mcp_tools(): List tool definitions argent format mcp_resources(): List resource definitions mcp_prompts(): List prompt definitions","code":""},{"path":"https://ma-riviere.github.io/argent/reference/mcp_integration.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"MCP (Model Context Protocol) Integration — mcp_integration","text":"","code":"if (FALSE) { # \\dontrun{ # HTTP server configuration (GitHub Copilot MCP) github <- mcp_server(   name = \"github\",   type = \"http\",   url = \"https://api.githubcopilot.com/mcp\",   headers = list(     Authorization = paste(\"Bearer\", Sys.getenv(\"GITHUB_PAT\"))   ) )  # Stdio server configuration (Docker) filesystem <- mcp_server(   name = \"filesystem\",   type = \"stdio\",   command = \"docker\",   args = c(\"run\", \"-i\", \"--rm\", \"mcp/filesystem\"),   env = list() )  # Get all tools from server github_tools <- mcp_tools(github)  # Get specific tools only issue_tools <- mcp_tools(github, tools = c(\"search_issues\", \"create_issue\"))  # Use in chat with other tools google <- Google$new() google$chat(   \"Create an issue for the bug I found\",   tools = list(     github_tools,     as_tool(my_custom_function)   ) )  # Get resources github_resources <- mcp_resources(github) } # }"},{"path":[]},{"path":"https://ma-riviere.github.io/argent/reference/tool_definitions.html","id":"annotation-based-approach","dir":"Reference","previous_headings":"","what":"Annotation-based approach","title":"Generate tools and schemas definitions from functions annotations, or direct specification — tool_definitions","text":"as_tool() parses annotations function converts generic tool definition args_schema field. standardized format can converted provider-specific formats internally. Annotations use roxygen2-style #' comments inside function body (outside like regular roxygen2 documentation). annotation syntax follows plumber2 conventions type specifications. package automatically enables source preservation loaded. defined functions loading package, simply redefine loading argent.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/tool_definitions.html","id":"direct-specification-approach","dir":"Reference","previous_headings":"","what":"Direct specification approach","title":"Generate tools and schemas definitions from functions annotations, or direct specification — tool_definitions","text":"tool() creates tool definition directly specifying parameters, alternative using function annotations as_tool(). approach useful complex nested structures defining tools without corresponding R functions. schema() similar tool() designed structured output schemas. includes additional fields (strict additionalProperties) required LLM providers structured outputs. Parameters specified named arguments. parameter value can : string: \"type[*] [description]\" (e.g., \"string* user's name\") list: nested objects type field nested properties","code":""},{"path":"https://ma-riviere.github.io/argent/reference/tool_definitions.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Generate tools and schemas definitions from functions annotations, or direct specification — tool_definitions","text":"","code":"as_tool(fn)  tool(name, description, ..., fn = NULL)  schema(name, description, ..., strict = TRUE, additional_properties = FALSE)"},{"path":"https://ma-riviere.github.io/argent/reference/tool_definitions.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Generate tools and schemas definitions from functions annotations, or direct specification — tool_definitions","text":"fn Function. tool() . Optional function implementation store tool definition. provided, function (closure) called LLM invokes tool, supporting locally-defined functions access local variables. NULL (default), function looked name global environment. name Character. tool schema name description Character. tool schema represents ... Named parameter specifications. See Details. strict Logical. schema() . Whether use strict mode (defaults TRUE). Added root level schema definition. additional_properties Logical. schema() . Whether allow additional properties schema (defaults FALSE). Added args_schema.","code":""},{"path":"https://ma-riviere.github.io/argent/reference/tool_definitions.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Generate tools and schemas definitions from functions annotations, or direct specification — tool_definitions","text":"as_tool(): list : name: Tool name (character) description: Tool description (character) args_schema: JSON Schema object type, properties, required fields .fn: original function (closure) execution tool(): list : name: Tool name (character) description: Tool description (character) args_schema: JSON Schema object type, properties, required fields .fn: Optional function implementation (fn parameter provided) schema(): list : name: Schema name (character) description: Schema description (character) args_schema: JSON Schema object type, properties, required fields strict: Logical (root level) args_schema$additionalProperties: Logical (inside args_schema)","code":""},{"path":[]},{"path":"https://ma-riviere.github.io/argent/reference/tool_definitions.html","id":"type-specifications-for-tool-","dir":"Reference","previous_headings":"","what":"Type Specifications (for tool())","title":"Generate tools and schemas definitions from functions annotations, or direct specification — tool_definitions","text":"Primitive types: string, integer, number, boolean, date, date-time Arrays: Use [type] syntax (e.g., \"[string]\", \"[integer]\") Required marker: Add * type (e.g., \"string*\") Descriptions: Add text type (e.g., \"string* user's name\") Nested objects: Use list type = \"object\" type = \"object*\":   Arrays objects: Use type = \"[object]\":","code":"address = list(   type = \"object*\",   description = \"Mailing address\",   street = \"string* Street address\",   city = \"string* City name\" ) users = list(   type = \"[object]*\",   description = \"List of users\",   name = \"string*\",   email = \"string*\" )"},{"path":"https://ma-riviere.github.io/argent/reference/tool_definitions.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Generate tools and schemas definitions from functions annotations, or direct specification — tool_definitions","text":"","code":"if (FALSE) { # \\dontrun{ # Annotation-based approach options(keep.source = TRUE)  my_fn <- function(x, y = 3L) {     #' @description Add two numbers     #' @param x:number* First number     #' @param y:integer Second number (optional, has default)     x + y }  as_tool(my_fn)  # Direct specification - tool() search_tool <- tool(   name = \"search_db\",   description = \"Search the database\",   query = \"string* Search query\",   limit = \"integer Maximum results to return\" )  # Direct specification - schema() output_schema <- schema(   name = \"flight_search\",   description = \"Flight search results\",   destination = \"string* Destination city\",   departure_date = \"string* Departure date\",   passengers = \"integer* Number of passengers\",   strict = TRUE,   additional_properties = FALSE )  # Nested object create_user_tool <- tool(   name = \"create_user\",   description = \"Create a new user\",   name = \"string* User's full name\",   address = list(     type = \"object*\",     description = \"User's mailing address\",     street = \"string* Street address\",     city = \"string* City name\",     zip = \"string Postal code\"   ) )  # Using closures with local state create_counter_tool <- function() {   count <- 0    increment <- function() {     count <<- count + 1     count   }    as_tool(increment) }  counter_tool <- create_counter_tool() # The LLM can now call this tool and it maintains state via closure  # Using MCP tools alongside custom tools github_server <- mcp_server(   name = \"github\",   type = \"http\",   url = \"https://api.githubcopilot.com/mcp\",   headers = list(     Authorization = paste(\"Bearer\", Sys.getenv(\"GITHUB_PAT\"))   ) )  github_tools <- mcp_tools(github_server)  # Combine MCP tools with custom tools in chat google <- Google$new() google$chat(   \"Create an issue about the bug I found\",   tools = list(     github_tools,          # MCP tools from server     as_tool(my_fn),        # Custom R function     search_tool            # Direct specification   ) ) } # }"}]
