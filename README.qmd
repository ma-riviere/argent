---
format: gfm

engine: knitr
execute:
  echo: true
---
<!-- markdownlint-disable -->
<!-- README.md is generated from README.qmd. Please edit that file instead. -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "100%"
)
```

# argent: LLM Agents in R

<!-- badges: start -->
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Lifecycle: superseded](https://img.shields.io/badge/lifecycle-superseded-blue.svg)](https://lifecycle.r-lib.org/articles/stages.html#superseded)
<!-- badges: end -->

**argent** provides a unified R6-based interface for interacting with Large Language Models (LLMs) from multiple providers, specialized for creating AI agents with tool calling, multimodal inputs, and structured outputs.

::: {.callout-important appearance="simple"}

In most cases, you'd be better off using [ellmer](https://github.com/tidyverse/ellmer).

I started working on this back in 2023 when none of the existing packages supported tool calling or structured outputs, but I never took the time to put everything into a proper package.

However, now there are several packages available, including [ellmer](https://github.com/tidyverse/ellmer) by the Tidyverse team, which I recommend using instead.

I'm putting `argent` out there in case it supports some edge cases that other packages don't, and because I didn't want to let all that work go to waste.
I will be migrating my projects to `ellmer`, and I am not sure I'll maintain `argent` in the long run.

:::

## In a Nutshell

`argent` provides a unified interface to build AI agents with conversation history management, server or client-side tool calling, multimodal inputs, and universal structured outputs.

It supports most **server-side tools** (code execution, web search, file search, etc.) and allows to easily define **client-side tools** using plumber2-style annotations within R functions.
It allows sending **multimodal inputs** (i.e. mixing text, images, PDFs, data files, URLs, remote files, and R objects) in a single request, and it supports **structured outputs** for _**any**_ model supporting tool calling, whatever other tools/functions are used.

## Supported Providers & Features

| Feature | Google | Anthropic | OpenAI Chat | OpenAI Responses | OpenAI Assistants | OpenRouter | Local LLM |
|---------|--------|-----------|-------------|------------------|-------------------|------------|-----------|
| **Tool calling** | ✅ | ✅ | ✅ | ✅ | ✅ | ⚠️[^1] | ⚠️[^1] |
| **Structured outputs** | ✅ | ✅ | ✅ | ✅ | ✅ | ✅[^2] | ✅[^2] |
| **Multimodal inputs** | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ⚠️[^3] |
| **Server-side tools** | ✅[^4] | ✅[^5] | ⚠️[^6] | ✅[^7] | ✅[^8] | ⚠️[^9] | ❌ |
| **Code execution** | ✅ | ✅ | ❌ | ✅ | ✅ | ❌ | ❌ |
| **File upload** | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ❌ |
| **Server-side RAG** | ✅ | ❌ | ❌ | ✅ | ✅ | ❌ | ❌ |
| **Reasoning / thinking** | ✅ | ✅ | ⚠️ | ✅ | ❌ | ⚠️[^10] | ⚠️[^10] |
| **Server-side state** | ❌ | ❌ | ❌ | ✅[^11] | ✅ | ❌ | ❌ |
| **Prompt caching** | ✅ | ✅ | ✅[^12] | ✅[^12] | ✅[^12] | ⚠️[^13] | ❌ |
| **Status** | Active | Active | Active | Active | **Deprecated**[^14] | Active | Active |

[^1]: Depends on model capabilities. Not all models support tool calling.
[^2]: Works on any model supporting tool calling. Works even if you provide other tools, client-side or server-side, to the model at the same time.
[^3]: Depends on model capabilities
[^4]: google_search, google_maps, url_context, code_execution, file_search
[^5]: web_search, web_fetch, code_execution
[^6]: Only web search via specialized models (gpt-4o-mini-search-preview)
[^7]: web_search, file_search, code_interpreter
[^8]: file_search, code_interpreter
[^9]: web_search available on some models
[^10]: Depends on model, and on server configuration (e.g., `--reasoning-format` for llama.cpp)
[^11]: Optional via `previous_response_id` (30-day retention)
[^12]: Automatic prompt caching by OpenAI
[^13]: Depends on the underlying provider being used
[^14]: Shuts down August 26, 2026. Use Responses API instead.

## Installation

```{r}
#| eval: false

remotes::install_github("ma-riviere/argent")
```

```{r}
#| echo: false
#| output: false

devtools::load_all()

options(keep.source = TRUE)
```

## Setup

Set the API keys for the providers you want to use in your `.Renviron`:

```r
GEMINI_API_KEY="your-google-gemini-key"
ANTHROPIC_API_KEY="your-anthropic-key"
OPENAI_API_KEY="your-openai-key"
```

## Quick Start

Here is a quick example using Google Gemini:

```{r}
gemini <- Google$new(api_key = Sys.getenv("GEMINI_API_KEY"))
```

You can customize the rate limit when initializing with the `rate_limit` parameter.

### Basic Completion

```{r}
gemini$chat(
    "What is the R programming language? Answer in two sentences.",
    model = "gemini-2.5-flash"
)
```

`argent` will maintain a conversation history in the provider object, meaning that when using `$chat()` a second time, the model will have access to the previous exchanges:

```{r}
gemini$chat(
    "Tell me more about its statistical modeling capabilities.",
    model = "gemini-2.5-flash"
)
```

The chat history can be visualized by printing the provider object:

```{r gemini-print}
print(gemini)
```

::: {.callout-note appearance="simple"}
You can also check the JSON files containing the chat history that are created automatically in the `data/history/{provider}/` directory (default).
:::

The chat history can be reset with `reset_history()`:

```{r}
gemini$reset_history()
```


### Tool Calling + Structured Output

First, let's define a mock function for the LLM:

```{r}
get_user_info <- function(user_name) {
    #' @description Provides information about the user, like their favorite programming language
    #' @param user_name:string* The name of the user
    
    switch(
        user_name,
        "Marc" = list(favorite_language = "R", favorite_framework = "Shiny"),
        "Alice" = list(favorite_language = "Python", favorite_framework = "Flask"),
        "Bob" = list(favorite_language = "JavaScript", favorite_framework = "React"),
        .default = list(favorite_language = "unknown", favorite_framework = "unknown")
    )
}
```

We can then call `as_tool()` on the function to convert it to a tool using the annotations added inside the function's body (plumber2-style annotations):

```{r}
#| eval: false

as_tool(get_user_info)
```

```{yaml}
name: get_user_info
description: Provides information about the user, like their favorite programming
  language
args_schema:
  type: object
  properties:
    user_name:
      type: string
      description: The name of the user
  required:
  - user_name
```

Then, let's define the schema for the structured output using `schema()`:

```{r user-info-schema}
user_info_schema <- schema(
    name = "user_info",
    description = "Information about the user",
    user_name = "string* The name of the user",
    favorite_language = "string* The user's favorite programming language",
    favorite_framework = "string* The user's favorite framework"
)
```

Run the agent:

```{r}
#| eval: false

gemini$chat(
    "The user's name is Marc. Give me the information about the user.",
    model = "gemini-2.5-flash",
    thinking_budget = 512,
    include_thoughts = TRUE, # Google-specific parameter
    tools = list(as_tool(get_user_info)),
    output_schema = user_info_schema
)
```

```{verbatim}
$user_name
[1] "Marc"

$favorite_language
[1] "R"

$favorite_framework
[1] "Shiny"
```

The model will use the tools provided repeatedly until it has enough information to answer the question, and return structured JSON output.

::: {.callout-tip appearance="simple"}
By default, `argent` will print the tool calls the model makes in the console:

ℹ [Google] Calling: get_user_info(user_name = "Marc")

:::

To see more, we can print the provider object with `show_tools = TRUE` to show to tool definitions, calls, and results:

```{r}
print(gemini, show_tools = TRUE)
```

### Server-side Tools

Providers like Google, Anthropic, and OpenAI have server-side tools. Those are tools you can call without having to them define yourself.

For example, Google Gemini has a server-side `google_search` which combines searching & fetching web pages:

```{r}
#| eval: false

gemini$chat(
    "When was the first release of the R 'ellmer' package on GitHub?",
    model = "gemini-2.5-pro",
    tools = list("google_search"),
    thinking_budget = -1, # Unlimited thinking budget
    include_thoughts = TRUE, # Google-specific parameter
    output_schema = schema(
        name = "package_info",
        description = "Information about an R package release",
        release_version = "string* The release version of the package",
        release_date = "string* The release date of the `release_version`"
    )
)
```

```{verbatim}
$release_version
[1] "0.1.1"

$release_date
[1] "2025-02-25"
```

```{r}
#| echo: false
#| output: false

gemini$reset_history()
```

### Multimodal Input

All providers support multimodal inputs (to some degree). 
You can pass text, images, PDFs, data files, URLs, remote files, and R objects to the model in a single request.

#### Passing Files or URLs

Example with an URL to a PDF file:

```{r}
#| eval: false

bsg04_cast_image_url <- "https://upload.wikimedia.org/wikipedia/en/1/1a/Battlestar_Galactica_%282004%29_cast.jpg"

gemini$chat(
    "Who are the characters in this image, and what show is it from?",
    bsg04_cast_image_url,
    model = "gemini-2.5-flash"
)
```

```{verbatim}
This image features the cast of **Battlestar Galactica (2004 TV series)**.

The characters shown are:
*   **Edward James Olmos** as Admiral William Adama
*   **Mary McDonnell** as President Laura Roslin
*   **Jamie Bamber** as Captain Lee "Apollo" Adama
*   **Katee Sackhoff** as Captain Kara "Starbuck" Thrace
*   **Tricia Helfer** as Number Six
*   **James Callis** as Dr. Gaius Baltar
*   **Grace Park** as Lieutenant Sharon "Boomer" Valerii / Number Eight
```

*So say we all!*

::: {.callout-note appearance="simple"}
Here, the URL was automatically detected, downloaded in a temporary file, and converted to base64, before being passed to the model.

Other providers may have different behavior. For example, Anthropic supports passing images & PDFs URLs directly.

Helper functions like `as_text_content()` are available to force some behaviors (like extracting the contents of the PDF instead of passing base64).
:::

::: {.callout-tip appearance="simple"}
We can also pass any R object to `chat()` as is. They will be captured automatically and converted to JSON (or text if JSON conversion fails), with some added information like the name of the object and its classes.
:::

```{r}
#| echo: false
#| output: false

gemini$reset_history()
```

#### Passing Uploaded Files

Finally, major providers support uploading files to their servers, and passing them as references to the model, to use them in the conversation, either on their own, or as part of a vector store / RAG system (see [RAG Applications](articles/usecase-rag.html)). 

```{r}
#| eval: false

file_metadata <- gemini$upload_file("https://ma-riviere.com/res/cv.pdf")
#> ✔ [Google] File uploaded: files/7xulp36j9jq1
```

```{r}
#| eval: false

multipart_prompt <- list(
    "What is my favorite programming language?",
    as_file_content(file_metadata$name)
)

gemini$chat(!!!multipart_prompt, model = "gemini-2.5-flash")
```

```{verbatim}
Based on your resume, your favorite programming language appears to be **R**.
```

*Damn right!*

```{r}
#| eval: false

gemini$delete_file(file_metadata$name)
#> ✔ [Google] File deleted: files/7xulp36j9jq1
```

::: {.callout-note appearance="simple"}
Here, using `as_file_content()` signals to the model that this is a remote file reference, rather than just some text content.
:::

::: {.callout-tip appearance="simple"}
You can use `$list_files()` to list all uploaded files and their metadata.
:::


## Documentation

### Provider Guides

Detailed guides for each provider:

- [Google Gemini](articles/google-gemini.html)
- [Anthropic Claude](articles/anthropic.html)
- [OpenRouter](articles/openrouter.html)
- [Local LLMs](articles/local-llm.html)

#### OpenAI APIs

Guides for OpenAI's three different APIs:

- [Chat Completions API](articles/openai-completions.html) - Standard OpenAI chat interface
- [Responses API](articles/openai-responses.html) - Newest API combining the functionalities of the Chat and Assistants
- [Assistants API](articles/openai-assistants.html) - Deprecated

#### Other Providers

- [Using Other Compatible APIs](articles/other-providers.html) - Use argent classes with compatible services (e.g., Minimax instead of Claude)

### Advanced Topics

- [RAG Applications](articles/usecase-rag.html) - Retrieval-Augmented Generation patterns

## Contributing

You should probably contribute to [ellmer](https://github.com/tidyverse/ellmer/) instead.

## License

MIT License
