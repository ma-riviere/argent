---
format: gfm
execute:
  echo: true
  eval: false
---
<!-- markdownlint-disable -->
<!-- README.md is generated from README.qmd. Please edit that file instead. -->

```{r, include = FALSE}
knitr::opts_chunk$set(
    collapse = TRUE,
    comment = "#>",
    out.width = "100%"
)
```

# argent <a href="https://ma-riviere.github.io/argent/index.html"><img src="man/figures/logo.png" align="right" height="138" alt="argent documentation website" /></a>

<!-- badges: start -->
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Lifecycle: questioning](https://img.shields.io/badge/lifecycle-questioning-blue.svg)](https://lifecycle.r-lib.org/articles/stages.html#questioning)
[![R-CMD-check](https://github.com/ma-riviere/argent/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/ma-riviere/argent/actions/workflows/R-CMD-check.yaml)
<!-- badges: end -->

**argent** provides a unified interface for interacting with Large Language Models (LLMs) from multiple providers, specialized for creating AI agents with tool calling, multimodal inputs, and universal structured outputs.

::: {.callout-important appearance="simple"}

Why **questioning** ? In most cases, you'd be better off using [ellmer](https://github.com/tidyverse/ellmer).

I started working on this back in 2023 when none of the existing LLM packages supported tool calling or structured outputs, but I never took the time to put everything into a proper package until now. However, there are now several similar packages, including [ellmer](https://github.com/tidyverse/ellmer) by the Tidyverse team.

I'm putting `argent` out there in case it supports some edge cases that other packages don't, and because I didn't want to let all that work go to waste.
But I will be progressively migrating my projects to `ellmer`, so I'm not sure how long I'll maintain `argent`.

:::

## In a Nutshell

`argent` provides a unified interface to build AI agents with conversation history management, local function & MCP tools, server-side (built-in) tools, multimodal inputs, and universal structured outputs.

It supports most **server-side tools** (i.e. built-in tools like code execution, web search, file search, etc.), **MCP servers' tools** (both http & stdio), and allows to easily define **client-side tools** using plumber2-style annotations within R functions.
It allows sending **multimodal inputs** (i.e. mixing text, images, PDFs, data files, URLs, remote files, and R objects) in a single request, and it supports **structured outputs** for _**any**_ model supporting tool calling, whatever other tools/functions are used.

## Supported Providers & Features

| Feature | Google | Anthropic | OpenAI Chat | OpenAI Responses | OpenAI Assistants | OpenRouter | Local LLM |
|---------|--------|-----------|-------------|------------------|-------------------|------------|-----------|
| **Function & MCP tools** | ✅ | ✅ | ✅ | ✅ | ✅ | ⚠️[^1] | ⚠️[^1] |
| **Structured outputs** | ✅ | ✅ | ✅ | ✅ | ✅ | ✅[^2] | ✅[^2] |
| **Multimodal inputs** | ✅ | ✅ | ✅ | ✅ | ✅ | ✅ | ⚠️[^3] |
| **Server-side tools** | ✅[^4] | ✅[^5] | ⚠️[^6] | ✅[^7] | ✅[^8] | ⚠️[^9] | ❌ |
| **Code execution** | ✅ | ✅ | ❌ | ✅ | ✅ | ❌ | ❌ |
| **File upload** | ✅ | ✅ | ✅ | ✅ | ✅ | ❌ | ❌ |
| **Server-side RAG** | ✅ | ❌ | ❌ | ✅ | ✅ | ❌ | ❌ |
| **Reasoning / thinking** | ✅ | ✅ | ⚠️ | ✅ | ❌ | ⚠️[^10] | ⚠️[^10] |
| **Server-side state** | ❌ | ❌ | ❌ | ✅[^11] | ✅ | ❌ | ❌ |
| **Prompt caching** | ✅ | ✅ | ✅[^12] | ✅[^12] | ✅[^12] | ⚠️[^13] | ❌ |
| **Embeddings** | ✅ | ❌ | ✅ | ✅ | ✅ | ✅ | ⚠️[^3] |
| **Status** | Active | Active | Active | Active | **Deprecated**[^14] | Active | Active |

[^1]: Depends on model capabilities. Not all models support tool calling.
[^2]: Works on any model supporting tool calling. Works even if you provide other tools, client-side or server-side, to the model at the same time.
[^3]: Depends on model capabilities
[^4]: google_search, google_maps, url_context, code_execution, file_search
[^5]: web_search, web_fetch, code_execution
[^6]: Only web search via specialized models (gpt-4o-mini-search-preview)
[^7]: web_search, file_search, code_interpreter
[^8]: file_search, code_interpreter
[^9]: web_search available on some models
[^10]: Depends on model, and on server configuration (e.g., `--reasoning-format` for llama.cpp)
[^11]: Optional via `previous_response_id` (30-day retention)
[^12]: Automatic prompt caching by OpenAI
[^13]: Depends on the underlying provider being used
[^14]: Shuts down August 26, 2026. Use Responses API instead.

## Installation

```{r}
remotes::install_github("ma-riviere/argent")
```

## Setup

Set the API keys for the providers you want to use in your `.Renviron`:

```r
GEMINI_API_KEY="your-google-gemini-key"
ANTHROPIC_API_KEY="your-anthropic-key"
OPENAI_API_KEY="your-openai-key"
```

## Quick Start

Here is a quick example using Google Gemini:

```{r}
gemini <- Google$new(api_key = Sys.getenv("GEMINI_API_KEY"))
```

You can customize the rate limit when initializing with the `rate_limit` parameter, and the default model with the `default_model` parameter ('gemini-2.5-flash' for Google).

### Basic Completion

```{r}
gemini$chat(
    "What is the R programming language? Answer in two sentences.",
    model = "gemini-2.5-flash" # Not necessary, it's the default model for Google
)
```

`argent` will maintain a conversation history in the provider object, meaning that when using `$chat()` a second time, the model will have access to the previous exchanges:

```{r}
gemini$chat("Tell me more about its statistical modeling capabilities.")
```

The chat history can be visualized by printing the provider object:

```{r gemini-print}
print(gemini)
```

```{verbatim}
── [ <Google> turns: 4 | Current context: 419 | Cumulated tokens: 676 ] ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────


── user [159 / 257] ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

What is the R programming language? Answer in two sentences.

── System ──

You are a helpful AI assistant. Use your knowledge, the files you have access to, and the tools at your disposal to answer the user's query. You can use your tools multiple times, but use them sparingly. Make parallel tool calls if relevant to the user's query. Answer the user's query as soon as you have the information necessary to answer. Self-reflect and double-check your answer before responding. If you don't know the answer even after using your tools, say 'I don't know'. If you do not have all the information necessary to use a provided tool, use NA for required arguments. Today's date is 2025-11-15

── assistant [257 / 257] ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

R is a programming language and free software environment primarily used for statistical computing and graphics. It provides a wide variety of statistical (linear and nonlinear modeling, classical statistical tests, time-series analysis, classification, clustering, etc.) and graphical techniques, and is highly extensible.


── user [224 / 676] ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

Tell me more about its statistical modeling capabilities.

── System ──

You are a helpful AI assistant. Use your knowledge, the files you have access to, and the tools at your disposal to answer the user's query. You can use your tools multiple times, but use them sparingly. Make parallel tool calls if relevant to the user's query. Answer the user's query as soon as you have the information necessary to answer. Self-reflect and double-check your answer before responding. If you don't know the answer even after using your tools, say 'I don't know'. If you do not have all the information necessary to use a provided tool, use NA for required arguments. Today's date is 2025-11-15

── assistant [419 / 676] ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

R offers extensive statistical modeling capabilities, encompassing a broad range of techniques. It supports linear and generalized linear models (GLMs) for various response types, as well as nonlinear regression. Users can perform classical statistical tests like t-tests, ANOVA, and chi-squared tests. Furthermore, R is widely used for time-series analysis, including ARIMA models and state-space models, classification algorithms such as logistic regression, decision trees, and support vector machines, and clustering methods like k-means and hierarchical clustering. Its package ecosystem continuously expands these capabilities with cutting-edge statistical methodologies.
```

::: {.callout-note appearance="simple"}
You can also check the JSON files containing the chat history that are created automatically in the `data/history/{provider}/` directory (default).
:::

The chat history can be reset with `reset_history()`:

```{r}
gemini$reset_history()
```


### Tool Calling + Structured Output

First, let's define a mock function for the LLM, that returns some information about the user for a given name:

```{r}
get_user_info <- function(user_name) {
    #' @description Provides information about the user, like their favorite programming language
    #' @param user_name:string* The name of the user

    switch(user_name,
        "Marc" = list(favorite_language = "R", favorite_framework = "Shiny"),
        "Alice" = list(favorite_language = "Python", favorite_framework = "Flask"),
        "Bob" = list(favorite_language = "JavaScript", favorite_framework = "React"),
        .default = list(favorite_language = "unknown", favorite_framework = "unknown")
    )
}
```

We can then call `as_tool()` on the function to convert it to a tool using the annotations added inside the function's body (plumber2-style annotations):

```{r}
as_tool(get_user_info)
```

```{yaml}
name: get_user_info
description: Provides information about the user, like their favorite programming
  language
args_schema:
  type: object
  properties:
    user_name:
      type: string
      description: The name of the user
  required:
  - user_name
```

Then, let's define the schema for the structured output using `schema()`:

```{r user-info-schema}
user_info_schema <- schema(
    name = "user_info",
    description = "Information about the user",
    user_name = "string* The name of the user",
    favorite_language = "string* The user's favorite programming language",
    favorite_framework = "string* The user's favorite framework"
)
```

```{yaml}
name: user_info
description: Information about the user
strict: yes
args_schema:
  type: object
  properties:
    user_name:
      type: string
      description: The name of the user
    favorite_language:
      type: string
      description: The user's favorite programming language
    favorite_framework:
      type: string
      description: The user's favorite framework
  required:
  - user_name
  - favorite_language
  - favorite_framework
  additionalProperties: no
```

Run the agent:

```{r}
gemini$chat(
    "The user's name is Marc. Give me the information about the user.",
    model = "gemini-2.5-flash",
    thinking_budget = 512,
    include_thoughts = TRUE, # Google-specific parameter
    tools = list(as_tool(get_user_info)),
    output_schema = user_info_schema
)
```

```{verbatim}
$user_name
[1] "Marc"

$favorite_language
[1] "R"

$favorite_framework
[1] "Shiny"
```

The model will use the tools provided repeatedly until it has enough information to answer the question, and return structured JSON output.

::: {.callout-tip appearance="simple"}
By default, `argent` will print the tool calls the model makes in the console:

ℹ [Google] Calling: get_user_info(user_name = "Marc")

:::

To see more, we can print the provider object with `show_tools = TRUE` to show to tool definitions, calls, and results:

```{r}
print(gemini, show_tools = TRUE)
```

```{verbatim}	
── [ <Google> turns: 6 | Current context: 677 | Cumulated tokens: 1449 ] ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────


── user [216 / 289] ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

The user's name is Marc. Give me the information about the user.

── System ──

You are a helpful AI assistant. Use your knowledge, the files you have access to, and the tools at your disposal to answer the user's query. You can use your tools multiple times, but use them sparingly. Make parallel tool calls if relevant to the user's query. Answer the user's query as soon as you have the information necessary to answer. Self-reflect and double-check your answer before responding. If you don't know the answer even after using your tools, say 'I don't know'. If you do not have all the information necessary to use a provided tool, use NA for required arguments. Today's date is 2025-11-19

── Tool Definitions ──

• get_user_info(user_name): Provides information about the user, like their favorite programming language

── assistant [289 / 289] ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

── Thinking ──

**Processing a User Query**

Okay, so someone's asking about a user named Marc.  My gut says the `get_user_info` tool is the way to go. It's purpose-built for retrieving user data, and the `user_name` parameter is exactly what I need.  It's pretty straightforward, actually.  I'll just call the tool and pass 'Marc' as the `user_name` value. Should be a quick and efficient way to get the information.  Let's see what comes back.


── Tool Calls ──

• get_user_info(user_name = "Marc")

── tool [417 / 772] ────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

• Result from get_user_info:

name: get_user_info
arguments:
  user_name: Marc
result:
  favorite_language: R
  favorite_framework: Shiny


── assistant [483 / 772] ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

── Thinking ──

**Analyzing the Marc Inquiry**

Okay, so the user needed some quick background on "Marc." That's easy enough.  I leveraged the `get_user_info` tool, feeding it `user_name='Marc'`, and the results are pretty straightforward.  It seems Marc's a bit of an R aficionado, which isn't entirely surprising - there's a strong R community out there.  The Shiny framework mention is interesting, though. It suggests he's not just into statistical computing, but also interested in building interactive web applications or dashboards.  I can already start to picture the types of projects he might be involved in.  This information is pretty valuable context.



Marc's favorite programming language is R, and his favorite framework is Shiny.

── user [537 / 1449] ───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

Please format your previous response according to the requested schema.

── System ──

You are a helpful AI assistant. Use your knowledge, the files you have access to, and the tools at your disposal to answer the user's query. You can use your tools multiple times, but use them sparingly. Make parallel tool calls if relevant to the user's query. Answer the user's query as soon as you have the information necessary to answer. Self-reflect and double-check your answer before responding. If you don't know the answer even after using your tools, say 'I don't know'. If you do not have all the information necessary to use a provided tool, use NA for required arguments. Today's date is 2025-11-19

── Output Schema ──

type: object
properties:
  user_name:
    type: string
    description: The name of the user
  favorite_language:
    type: string
    description: The user's favorite programming language
  favorite_framework:
    type: string
    description: The user's favorite framework
required:
- user_name
- favorite_language
- favorite_framework

── assistant [677 / 1449] ──────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

{"user_name": "Marc", "favorite_language": "R", "favorite_framework": "Shiny"}
```

:::{.callout-note appearance="simple"}
You may have noticed an extra user input at the end of the conversation history. This is because Google does not support structured outputs when tools are used, so, behind the scenes, `argent` makes a final call to the model to format the previous response according to the requested schema, without any tools.

For other providers that do not support structured outputs at all, `argent` will turn the output schema into a tool and make a final call to the model with just that tool, intercept the tool call and return it as the result. This 'trick' allows to support structured outputs on any model supporting tool calling.
:::

### Server-side Tools

Providers like Google, Anthropic, and OpenAI have server-side tools. 
Those are tools you can call without having to define them yourself. They will be run on the provider's server.

For example, Google Gemini has a server-side `google_search` which combines searching & fetching web pages:

```{r}
gemini$chat(
    "When was the first release of the R 'ellmer' package on GitHub?",
    model = "gemini-2.5-pro",
    tools = list("google_search"),
    thinking_budget = -1, # Unlimited thinking budget
    include_thoughts = TRUE, # Google-specific parameter
    output_schema = schema(
        name = "package_info",
        description = "Information about an R package release",
        release_version = "string* The release version of the package",
        release_date = "string* The release date of the `release_version`"
    )
)
```

```{verbatim}
$release_version
[1] "0.1.0"

$release_date
[1] "2025-01-09"
```

### Multimodal Input

All providers support multimodal inputs (to some degree). 
You can pass text, images, PDFs, data files, URLs, remote files, and R objects to the model in a single request.

#### Passing Files or URLs

Example with an URL to a PDF file:

```{r}
bsg04_cast_image_url <- "https://upload.wikimedia.org/wikipedia/en/1/1a/Battlestar_Galactica_%282004%29_cast.jpg"

gemini$chat(
    "Who are the characters in this image, and what show is it from?",
    bsg04_cast_image_url,
    model = "gemini-2.5-flash"
)
```

```{verbatim}
This image features the cast of **Battlestar Galactica (2004 TV series)**.

The characters shown are:
*   **Edward James Olmos** as Admiral William Adama
*   **Mary McDonnell** as President Laura Roslin
*   **Jamie Bamber** as Captain Lee "Apollo" Adama
*   **Katee Sackhoff** as Captain Kara "Starbuck" Thrace
*   **Tricia Helfer** as Number Six
*   **James Callis** as Dr. Gaius Baltar
*   **Grace Park** as Lieutenant Sharon "Boomer" Valerii / Number Eight
```

*So say we all!*

::: {.callout-note appearance="simple"}
Here, the URL was automatically detected, downloaded in a temporary file, and converted to base64, before being passed to the model.

Other providers may have different behavior. For example, the image or PDF's URL will be passed as-is to Anthropic's API.

Helper functions like `as_text_content()` are available to force some behaviors (like extracting the contents of the PDF instead of passing base64).
:::

::: {.callout-tip appearance="simple"}
We can also pass any R object to `chat()` as is. They will be captured automatically and converted to JSON (or text if JSON conversion fails), with some added information like the name of the object and its classes.
:::

#### Passing Uploaded Files

Finally, major providers support uploading files to their servers, and passing them as references to the model, to use them in the conversation, either on their own, or as part of a vector store / RAG system (see [server-side RAG](/articles/google-gemini.html#server-side-rag)). 

```{r}
file_metadata <- gemini$upload_file("https://ma-riviere.com/res/cv.pdf")
#> ✔ [Google] File uploaded: files/7xulp36j9jq1
```

```{r}
multipart_prompt <- list(
    "What is my favorite programming language?",
    as_file_content(file_metadata$name)
)

gemini$chat(!!!multipart_prompt, model = "gemini-2.5-flash")
```

```{verbatim}
Based on your resume, your favorite programming language appears to be **R**.
```

*Damn right!*

```{r}
gemini$delete_file(file_metadata$name)
#> ✔ [Google] File deleted: files/7xulp36j9jq1
```

::: {.callout-note appearance="simple"}
Here, using `as_file_content()` signals to the model that this is a remote file reference, rather than just some text content.
:::

::: {.callout-tip appearance="simple"}
You can use `$list_files()` to list all uploaded files and their metadata.
:::


## Documentation

### Provider Guides

Detailed guides for each provider:

- [Google Gemini](https://ma-riviere.github.io/argent/articles/google-gemini.html)
- [Anthropic Claude](https://ma-riviere.github.io/argent/articles/anthropic.html)
- [OpenRouter](https://ma-riviere.github.io/argent/articles/openrouter.html)
- [Local LLMs](https://ma-riviere.github.io/argent/articles/local-llm.html)

#### OpenAI APIs

Guides for OpenAI's three different APIs:

- [Chat Completions API](https://ma-riviere.github.io/argent/articles/openai-completions.html) - Standard OpenAI chat interface
- [Responses API](https://ma-riviere.github.io/argent/articles/openai-responses.html) - Newest API combining the functionalities of the Chat and Assistants
- [Assistants API](https://ma-riviere.github.io/argent/articles/openai-assistants.html) - Deprecated

#### Other Providers

- [Using Other Compatible APIs](https://ma-riviere.github.io/argent/articles/other-providers.html) - Use argent with providers offering compatible APIs (e.g., Minimax, Qwen, ...)

### Advanced Topics

- [RAG](https://ma-riviere.github.io/argent/articles/advanced-rag.html) - How to use `argent` & `ragnar` for RAG
- [MCP Servers & Tools](https://ma-riviere.github.io/argent/articles/advanced-mcp.html) - How to use MCP server tools with `argent`

## Contributing

You should probably contribute to [ellmer](https://github.com/tidyverse/ellmer/) instead.

## License

MIT License
